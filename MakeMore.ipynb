{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "035afa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MakeMore -  Step by Step Language Modelling (Character Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f71cbf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt').read().splitlines() #reading text file and split data and store as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f6cf147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbffd3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6904ec4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f39809e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(w) for w  in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "953ec46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Bigram i.e always work with 2 characters at a time...i.e given a character, will predict next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c258765c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e m\n",
      "m m\n",
      "m a\n"
     ]
    }
   ],
   "source": [
    "for w in words[:1]: #lets examine 1st word i.e emma in the list\n",
    "    for ch1, ch2 in zip(w, w[1:]): #zipping each word with word without 1st letter to get 2 consequtive characters\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88567eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice how 2 consecutive characters were extracted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff9f2785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'emma'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d13be0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mma'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd0dd3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#when any one of word size dont match, it will stop...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e0c6366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets add special char to notice start and end of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05595364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e m\n",
      "m m\n",
      "m a\n"
     ]
    }
   ],
   "source": [
    "for w in words[:1]: #lets examine 1st word i.e emma in the list\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(w, w[1:]): #zipping each word with word without 1st letter to get 2 consequtive characters\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b1deeaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', 'm', 'm', 'a']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c318124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice list of w, extracted each characters in the word as individual elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36512d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a <E>\n"
     ]
    }
   ],
   "source": [
    "#therefore we will iterate from chs instead of w's\n",
    "for w in words[:1]: #lets examine 1st word i.e emma in the list\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]): #zipping each word with word without 1st letter to get 2 consequtive characters\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22ca8aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a <E>\n",
      "<S> o\n",
      "o l\n",
      "l i\n",
      "i v\n",
      "v i\n",
      "i a\n",
      "a <E>\n"
     ]
    }
   ],
   "source": [
    "for w in words[:2]:\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]): #zipping each word with word without 1st letter to get 2 consequtive characters\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e99f5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice how we identify start and end of each words using special symbols added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8562d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basically to identify which char comes next for given char, we find statistics of finding frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0294c90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a <E>\n",
      "<S> o\n",
      "o l\n",
      "l i\n",
      "i v\n",
      "v i\n",
      "i a\n",
      "a <E>\n"
     ]
    }
   ],
   "source": [
    "#therefor we create a dictionary to keep counter for each of these bigrams\n",
    "b = {}\n",
    "for w in words[:2]: #lets examine 1st word i.e emma in the list\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]): #zipping each word with word without 1st letter to get 2 consequtive characters\n",
    "        bigram = (ch1, ch2)\n",
    "        b[bigram] = b.get(bigram, 0) + 1# check if the bigram exists in dict, if not return 0; if present inc counter by 1\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9601c65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<S>', 'e'): 1,\n",
       " ('e', 'm'): 1,\n",
       " ('m', 'm'): 1,\n",
       " ('m', 'a'): 1,\n",
       " ('a', '<E>'): 2,\n",
       " ('<S>', 'o'): 1,\n",
       " ('o', 'l'): 1,\n",
       " ('l', 'i'): 1,\n",
       " ('i', 'v'): 1,\n",
       " ('v', 'i'): 1,\n",
       " ('i', 'a'): 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b21ccbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets do for all the words\n",
    "b = {}\n",
    "for w in words: #lets examine 1st word i.e emma in the list\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]): #zipping each word with word without 1st letter to get 2 consequtive characters\n",
    "        bigram = (ch1, ch2)\n",
    "        b[bigram] = b.get(bigram, 0) + 1# check if the bigram exists in dict, if not return 0; if present inc counter by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d719d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<S>', 'e'): 1531,\n",
       " ('e', 'm'): 769,\n",
       " ('m', 'm'): 168,\n",
       " ('m', 'a'): 2590,\n",
       " ('a', '<E>'): 6640,\n",
       " ('<S>', 'o'): 394,\n",
       " ('o', 'l'): 619,\n",
       " ('l', 'i'): 2480,\n",
       " ('i', 'v'): 269,\n",
       " ('v', 'i'): 911,\n",
       " ('i', 'a'): 2445,\n",
       " ('<S>', 'a'): 4410,\n",
       " ('a', 'v'): 834,\n",
       " ('v', 'a'): 642,\n",
       " ('<S>', 'i'): 591,\n",
       " ('i', 's'): 1316,\n",
       " ('s', 'a'): 1201,\n",
       " ('a', 'b'): 541,\n",
       " ('b', 'e'): 655,\n",
       " ('e', 'l'): 3248,\n",
       " ('l', 'l'): 1345,\n",
       " ('l', 'a'): 2623,\n",
       " ('<S>', 's'): 2055,\n",
       " ('s', 'o'): 531,\n",
       " ('o', 'p'): 95,\n",
       " ('p', 'h'): 204,\n",
       " ('h', 'i'): 729,\n",
       " ('<S>', 'c'): 1542,\n",
       " ('c', 'h'): 664,\n",
       " ('h', 'a'): 2244,\n",
       " ('a', 'r'): 3264,\n",
       " ('r', 'l'): 413,\n",
       " ('l', 'o'): 692,\n",
       " ('o', 't'): 118,\n",
       " ('t', 't'): 374,\n",
       " ('t', 'e'): 716,\n",
       " ('e', '<E>'): 3983,\n",
       " ('<S>', 'm'): 2538,\n",
       " ('m', 'i'): 1256,\n",
       " ('a', 'm'): 1634,\n",
       " ('m', 'e'): 818,\n",
       " ('<S>', 'h'): 874,\n",
       " ('r', 'p'): 14,\n",
       " ('p', 'e'): 197,\n",
       " ('e', 'r'): 1958,\n",
       " ('r', '<E>'): 1377,\n",
       " ('e', 'v'): 463,\n",
       " ('v', 'e'): 568,\n",
       " ('l', 'y'): 1588,\n",
       " ('y', 'n'): 1826,\n",
       " ('n', '<E>'): 6763,\n",
       " ('b', 'i'): 217,\n",
       " ('i', 'g'): 428,\n",
       " ('g', 'a'): 330,\n",
       " ('a', 'i'): 1650,\n",
       " ('i', 'l'): 1345,\n",
       " ('l', '<E>'): 1314,\n",
       " ('y', '<E>'): 2007,\n",
       " ('i', 'z'): 277,\n",
       " ('z', 'a'): 860,\n",
       " ('e', 't'): 580,\n",
       " ('t', 'h'): 647,\n",
       " ('h', '<E>'): 2409,\n",
       " ('r', 'y'): 773,\n",
       " ('o', 'f'): 34,\n",
       " ('f', 'i'): 160,\n",
       " ('c', 'a'): 815,\n",
       " ('r', 'i'): 3033,\n",
       " ('s', 'c'): 60,\n",
       " ('l', 'e'): 2921,\n",
       " ('t', '<E>'): 483,\n",
       " ('<S>', 'v'): 376,\n",
       " ('i', 'c'): 509,\n",
       " ('c', 't'): 35,\n",
       " ('t', 'o'): 667,\n",
       " ('o', 'r'): 1059,\n",
       " ('a', 'd'): 1042,\n",
       " ('d', 'i'): 674,\n",
       " ('o', 'n'): 2411,\n",
       " ('<S>', 'l'): 1572,\n",
       " ('l', 'u'): 324,\n",
       " ('u', 'n'): 275,\n",
       " ('n', 'a'): 2977,\n",
       " ('<S>', 'g'): 669,\n",
       " ('g', 'r'): 201,\n",
       " ('r', 'a'): 2356,\n",
       " ('a', 'c'): 470,\n",
       " ('c', 'e'): 551,\n",
       " ('h', 'l'): 185,\n",
       " ('o', 'e'): 132,\n",
       " ('<S>', 'p'): 515,\n",
       " ('e', 'n'): 2675,\n",
       " ('n', 'e'): 1359,\n",
       " ('a', 'y'): 2050,\n",
       " ('y', 'l'): 1104,\n",
       " ('<S>', 'r'): 1639,\n",
       " ('e', 'y'): 1070,\n",
       " ('<S>', 'z'): 929,\n",
       " ('z', 'o'): 110,\n",
       " ('<S>', 'n'): 1146,\n",
       " ('n', 'o'): 496,\n",
       " ('e', 'a'): 679,\n",
       " ('a', 'n'): 5438,\n",
       " ('n', 'n'): 1906,\n",
       " ('a', 'h'): 2332,\n",
       " ('d', 'd'): 149,\n",
       " ('a', 'u'): 381,\n",
       " ('u', 'b'): 103,\n",
       " ('b', 'r'): 842,\n",
       " ('r', 'e'): 1697,\n",
       " ('i', 'e'): 1653,\n",
       " ('s', 't'): 765,\n",
       " ('a', 't'): 687,\n",
       " ('t', 'a'): 1027,\n",
       " ('a', 'l'): 2528,\n",
       " ('a', 'z'): 435,\n",
       " ('z', 'e'): 373,\n",
       " ('i', 'o'): 588,\n",
       " ('u', 'r'): 414,\n",
       " ('r', 'o'): 869,\n",
       " ('u', 'd'): 136,\n",
       " ('d', 'r'): 424,\n",
       " ('<S>', 'b'): 1306,\n",
       " ('o', 'o'): 115,\n",
       " ('o', 'k'): 68,\n",
       " ('k', 'l'): 139,\n",
       " ('c', 'l'): 116,\n",
       " ('i', 'r'): 849,\n",
       " ('s', 'k'): 82,\n",
       " ('k', 'y'): 379,\n",
       " ('u', 'c'): 103,\n",
       " ('c', 'y'): 104,\n",
       " ('p', 'a'): 209,\n",
       " ('s', 'l'): 279,\n",
       " ('i', 'n'): 2126,\n",
       " ('o', 'v'): 176,\n",
       " ('g', 'e'): 334,\n",
       " ('e', 's'): 861,\n",
       " ('s', 'i'): 684,\n",
       " ('s', '<E>'): 1169,\n",
       " ('<S>', 'k'): 2963,\n",
       " ('k', 'e'): 895,\n",
       " ('e', 'd'): 384,\n",
       " ('d', 'y'): 317,\n",
       " ('n', 't'): 443,\n",
       " ('y', 'a'): 2143,\n",
       " ('<S>', 'w'): 307,\n",
       " ('w', 'i'): 148,\n",
       " ('o', 'w'): 114,\n",
       " ('w', '<E>'): 51,\n",
       " ('k', 'i'): 509,\n",
       " ('n', 's'): 278,\n",
       " ('a', 'o'): 63,\n",
       " ('o', 'm'): 261,\n",
       " ('i', '<E>'): 2489,\n",
       " ('a', 'a'): 556,\n",
       " ('i', 'y'): 779,\n",
       " ('d', 'e'): 1283,\n",
       " ('c', 'o'): 380,\n",
       " ('r', 'u'): 252,\n",
       " ('b', 'y'): 83,\n",
       " ('s', 'e'): 884,\n",
       " ('n', 'i'): 1725,\n",
       " ('i', 't'): 541,\n",
       " ('t', 'y'): 341,\n",
       " ('u', 't'): 82,\n",
       " ('t', 'u'): 78,\n",
       " ('u', 'm'): 154,\n",
       " ('m', 'n'): 20,\n",
       " ('g', 'i'): 190,\n",
       " ('t', 'i'): 532,\n",
       " ('<S>', 'q'): 92,\n",
       " ('q', 'u'): 206,\n",
       " ('u', 'i'): 121,\n",
       " ('a', 'e'): 692,\n",
       " ('e', 'h'): 152,\n",
       " ('v', 'y'): 121,\n",
       " ('p', 'i'): 61,\n",
       " ('i', 'p'): 53,\n",
       " ('y', 'd'): 272,\n",
       " ('e', 'x'): 132,\n",
       " ('x', 'a'): 103,\n",
       " ('<S>', 'j'): 2422,\n",
       " ('j', 'o'): 479,\n",
       " ('o', 's'): 504,\n",
       " ('e', 'p'): 83,\n",
       " ('j', 'u'): 202,\n",
       " ('u', 'l'): 301,\n",
       " ('<S>', 'd'): 1690,\n",
       " ('k', 'a'): 1731,\n",
       " ('e', 'e'): 1271,\n",
       " ('y', 't'): 104,\n",
       " ('d', 'l'): 60,\n",
       " ('c', 'k'): 316,\n",
       " ('n', 'z'): 145,\n",
       " ('z', 'i'): 364,\n",
       " ('a', 'g'): 168,\n",
       " ('d', 'a'): 1303,\n",
       " ('j', 'a'): 1473,\n",
       " ('h', 'e'): 674,\n",
       " ('<S>', 'x'): 134,\n",
       " ('x', 'i'): 102,\n",
       " ('i', 'm'): 427,\n",
       " ('e', 'i'): 818,\n",
       " ('<S>', 't'): 1308,\n",
       " ('<S>', 'f'): 417,\n",
       " ('f', 'a'): 242,\n",
       " ('n', 'd'): 704,\n",
       " ('r', 'g'): 76,\n",
       " ('a', 's'): 1118,\n",
       " ('s', 'h'): 1285,\n",
       " ('b', 'a'): 321,\n",
       " ('k', 'h'): 307,\n",
       " ('s', 'm'): 90,\n",
       " ('o', 'd'): 190,\n",
       " ('r', 's'): 190,\n",
       " ('g', 'h'): 360,\n",
       " ('s', 'y'): 215,\n",
       " ('y', 's'): 401,\n",
       " ('s', 's'): 461,\n",
       " ('e', 'c'): 153,\n",
       " ('c', 'i'): 271,\n",
       " ('m', 'o'): 452,\n",
       " ('r', 'k'): 90,\n",
       " ('n', 'l'): 195,\n",
       " ('d', 'n'): 31,\n",
       " ('r', 'd'): 187,\n",
       " ('o', 'i'): 69,\n",
       " ('t', 'r'): 352,\n",
       " ('m', 'b'): 112,\n",
       " ('r', 'm'): 162,\n",
       " ('n', 'y'): 465,\n",
       " ('d', 'o'): 378,\n",
       " ('o', 'a'): 149,\n",
       " ('o', 'c'): 114,\n",
       " ('m', 'y'): 287,\n",
       " ('s', 'u'): 185,\n",
       " ('m', 'c'): 51,\n",
       " ('p', 'r'): 151,\n",
       " ('o', 'u'): 275,\n",
       " ('r', 'n'): 140,\n",
       " ('w', 'a'): 280,\n",
       " ('e', 'b'): 121,\n",
       " ('c', 'c'): 42,\n",
       " ('a', 'w'): 161,\n",
       " ('w', 'y'): 73,\n",
       " ('y', 'e'): 301,\n",
       " ('e', 'o'): 269,\n",
       " ('a', 'k'): 568,\n",
       " ('n', 'g'): 273,\n",
       " ('k', 'o'): 344,\n",
       " ('b', 'l'): 103,\n",
       " ('h', 'o'): 287,\n",
       " ('e', 'g'): 125,\n",
       " ('f', 'r'): 114,\n",
       " ('s', 'p'): 51,\n",
       " ('l', 's'): 94,\n",
       " ('y', 'z'): 78,\n",
       " ('g', 'g'): 25,\n",
       " ('z', 'u'): 73,\n",
       " ('i', 'd'): 440,\n",
       " ('m', '<E>'): 516,\n",
       " ('o', 'g'): 44,\n",
       " ('j', 'e'): 440,\n",
       " ('g', 'n'): 27,\n",
       " ('y', 'r'): 291,\n",
       " ('c', '<E>'): 97,\n",
       " ('c', 'q'): 11,\n",
       " ('u', 'e'): 169,\n",
       " ('i', 'f'): 101,\n",
       " ('f', 'e'): 123,\n",
       " ('i', 'x'): 89,\n",
       " ('x', '<E>'): 164,\n",
       " ('o', 'y'): 103,\n",
       " ('g', 'o'): 83,\n",
       " ('g', 't'): 31,\n",
       " ('l', 't'): 77,\n",
       " ('g', 'w'): 26,\n",
       " ('w', 'e'): 149,\n",
       " ('l', 'd'): 138,\n",
       " ('a', 'p'): 82,\n",
       " ('h', 'n'): 138,\n",
       " ('t', 'l'): 134,\n",
       " ('m', 'r'): 97,\n",
       " ('n', 'c'): 213,\n",
       " ('l', 'b'): 52,\n",
       " ('i', 'k'): 445,\n",
       " ('<S>', 'y'): 535,\n",
       " ('t', 'z'): 105,\n",
       " ('h', 'r'): 204,\n",
       " ('j', 'i'): 119,\n",
       " ('h', 't'): 71,\n",
       " ('r', 'r'): 425,\n",
       " ('z', 'l'): 123,\n",
       " ('w', 'r'): 22,\n",
       " ('b', 'b'): 38,\n",
       " ('r', 't'): 208,\n",
       " ('l', 'v'): 72,\n",
       " ('e', 'j'): 55,\n",
       " ('o', 'h'): 171,\n",
       " ('u', 's'): 474,\n",
       " ('i', 'b'): 110,\n",
       " ('g', 'l'): 32,\n",
       " ('h', 'y'): 213,\n",
       " ('p', 'o'): 59,\n",
       " ('p', 'p'): 39,\n",
       " ('p', 'y'): 12,\n",
       " ('n', 'r'): 44,\n",
       " ('z', 'm'): 35,\n",
       " ('v', 'o'): 153,\n",
       " ('l', 'm'): 60,\n",
       " ('o', 'x'): 45,\n",
       " ('d', '<E>'): 516,\n",
       " ('i', 'u'): 109,\n",
       " ('v', '<E>'): 88,\n",
       " ('f', 'f'): 44,\n",
       " ('b', 'o'): 105,\n",
       " ('e', 'k'): 178,\n",
       " ('c', 'r'): 76,\n",
       " ('d', 'g'): 25,\n",
       " ('r', 'c'): 99,\n",
       " ('r', 'h'): 121,\n",
       " ('n', 'k'): 58,\n",
       " ('h', 'u'): 166,\n",
       " ('d', 's'): 29,\n",
       " ('a', 'x'): 182,\n",
       " ('y', 'c'): 115,\n",
       " ('e', 'w'): 50,\n",
       " ('v', 'k'): 3,\n",
       " ('z', 'h'): 43,\n",
       " ('w', 'h'): 23,\n",
       " ('t', 'n'): 22,\n",
       " ('x', 'l'): 39,\n",
       " ('g', 'u'): 85,\n",
       " ('u', 'a'): 163,\n",
       " ('u', 'p'): 16,\n",
       " ('u', 'g'): 47,\n",
       " ('d', 'u'): 92,\n",
       " ('l', 'c'): 25,\n",
       " ('r', 'b'): 41,\n",
       " ('a', 'q'): 60,\n",
       " ('b', '<E>'): 114,\n",
       " ('g', 'y'): 31,\n",
       " ('y', 'p'): 15,\n",
       " ('p', 't'): 17,\n",
       " ('e', 'z'): 181,\n",
       " ('z', 'r'): 32,\n",
       " ('f', 'l'): 20,\n",
       " ('o', '<E>'): 855,\n",
       " ('o', 'b'): 140,\n",
       " ('u', 'z'): 45,\n",
       " ('z', '<E>'): 160,\n",
       " ('i', 'q'): 52,\n",
       " ('y', 'v'): 106,\n",
       " ('n', 'v'): 55,\n",
       " ('d', 'h'): 118,\n",
       " ('g', 'd'): 19,\n",
       " ('t', 's'): 35,\n",
       " ('n', 'h'): 26,\n",
       " ('y', 'j'): 23,\n",
       " ('k', 'r'): 109,\n",
       " ('z', 'b'): 4,\n",
       " ('g', '<E>'): 108,\n",
       " ('a', 'j'): 175,\n",
       " ('r', 'j'): 25,\n",
       " ('m', 'p'): 38,\n",
       " ('p', 'b'): 2,\n",
       " ('y', 'o'): 271,\n",
       " ('z', 'y'): 147,\n",
       " ('p', 'l'): 16,\n",
       " ('l', 'k'): 24,\n",
       " ('i', 'j'): 76,\n",
       " ('x', 'e'): 36,\n",
       " ('y', 'u'): 141,\n",
       " ('l', 'n'): 14,\n",
       " ('u', 'x'): 34,\n",
       " ('i', 'h'): 95,\n",
       " ('w', 's'): 20,\n",
       " ('k', 's'): 95,\n",
       " ('m', 'u'): 139,\n",
       " ('y', 'k'): 86,\n",
       " ('e', 'f'): 82,\n",
       " ('k', '<E>'): 363,\n",
       " ('y', 'm'): 148,\n",
       " ('z', 'z'): 45,\n",
       " ('m', 'd'): 24,\n",
       " ('s', 'r'): 55,\n",
       " ('e', 'u'): 69,\n",
       " ('l', 'h'): 19,\n",
       " ('a', 'f'): 134,\n",
       " ('r', 'w'): 21,\n",
       " ('n', 'u'): 96,\n",
       " ('v', 'r'): 48,\n",
       " ('m', 's'): 35,\n",
       " ('<S>', 'u'): 78,\n",
       " ('f', 's'): 6,\n",
       " ('y', 'b'): 27,\n",
       " ('x', 'o'): 41,\n",
       " ('g', 's'): 30,\n",
       " ('x', 'y'): 30,\n",
       " ('w', 'n'): 58,\n",
       " ('j', 'h'): 45,\n",
       " ('f', 'n'): 4,\n",
       " ('n', 'j'): 44,\n",
       " ('r', 'v'): 80,\n",
       " ('n', 'm'): 19,\n",
       " ('t', 'c'): 17,\n",
       " ('s', 'w'): 24,\n",
       " ('k', 't'): 17,\n",
       " ('f', 't'): 18,\n",
       " ('x', 't'): 70,\n",
       " ('u', 'v'): 37,\n",
       " ('k', 'k'): 20,\n",
       " ('s', 'n'): 24,\n",
       " ('u', '<E>'): 155,\n",
       " ('j', 'r'): 11,\n",
       " ('y', 'x'): 28,\n",
       " ('h', 'm'): 117,\n",
       " ('e', 'q'): 14,\n",
       " ('u', 'o'): 10,\n",
       " ('f', '<E>'): 80,\n",
       " ('h', 'z'): 20,\n",
       " ('h', 'k'): 29,\n",
       " ('y', 'g'): 30,\n",
       " ('q', 'r'): 1,\n",
       " ('v', 'n'): 8,\n",
       " ('s', 'd'): 9,\n",
       " ('y', 'i'): 192,\n",
       " ('n', 'w'): 11,\n",
       " ('d', 'v'): 17,\n",
       " ('h', 'v'): 39,\n",
       " ('x', 'w'): 3,\n",
       " ('o', 'z'): 54,\n",
       " ('k', 'u'): 50,\n",
       " ('u', 'h'): 58,\n",
       " ('k', 'n'): 26,\n",
       " ('s', 'b'): 21,\n",
       " ('i', 'i'): 82,\n",
       " ('y', 'y'): 23,\n",
       " ('r', 'z'): 23,\n",
       " ('l', 'g'): 6,\n",
       " ('l', 'p'): 15,\n",
       " ('p', '<E>'): 33,\n",
       " ('b', 'u'): 45,\n",
       " ('f', 'u'): 10,\n",
       " ('b', 'h'): 41,\n",
       " ('f', 'y'): 14,\n",
       " ('u', 'w'): 86,\n",
       " ('x', 'u'): 5,\n",
       " ('q', '<E>'): 28,\n",
       " ('l', 'r'): 18,\n",
       " ('m', 'h'): 5,\n",
       " ('l', 'w'): 16,\n",
       " ('j', '<E>'): 71,\n",
       " ('s', 'v'): 14,\n",
       " ('m', 'l'): 5,\n",
       " ('n', 'f'): 11,\n",
       " ('u', 'j'): 14,\n",
       " ('f', 'o'): 60,\n",
       " ('j', 'l'): 9,\n",
       " ('t', 'g'): 2,\n",
       " ('j', 'm'): 5,\n",
       " ('v', 'v'): 7,\n",
       " ('p', 's'): 16,\n",
       " ('t', 'w'): 11,\n",
       " ('x', 'c'): 4,\n",
       " ('u', 'k'): 93,\n",
       " ('v', 'l'): 14,\n",
       " ('h', 'd'): 24,\n",
       " ('l', 'z'): 10,\n",
       " ('k', 'w'): 34,\n",
       " ('n', 'b'): 8,\n",
       " ('q', 's'): 2,\n",
       " ('i', 'w'): 8,\n",
       " ('c', 's'): 5,\n",
       " ('h', 's'): 31,\n",
       " ('m', 't'): 4,\n",
       " ('h', 'w'): 10,\n",
       " ('x', 'x'): 38,\n",
       " ('t', 'x'): 2,\n",
       " ('d', 'z'): 1,\n",
       " ('x', 'z'): 19,\n",
       " ('t', 'm'): 4,\n",
       " ('t', 'j'): 3,\n",
       " ('u', 'q'): 10,\n",
       " ('q', 'a'): 13,\n",
       " ('f', 'k'): 2,\n",
       " ('z', 'n'): 4,\n",
       " ('l', 'j'): 6,\n",
       " ('j', 'w'): 6,\n",
       " ('v', 'u'): 7,\n",
       " ('c', 'j'): 3,\n",
       " ('h', 'b'): 8,\n",
       " ('z', 't'): 4,\n",
       " ('p', 'u'): 4,\n",
       " ('m', 'z'): 11,\n",
       " ('x', 's'): 31,\n",
       " ('b', 't'): 2,\n",
       " ('u', 'y'): 13,\n",
       " ('d', 'j'): 9,\n",
       " ('j', 's'): 7,\n",
       " ('w', 'u'): 25,\n",
       " ('o', 'j'): 16,\n",
       " ('b', 's'): 8,\n",
       " ('d', 'w'): 23,\n",
       " ('w', 'o'): 36,\n",
       " ('j', 'n'): 2,\n",
       " ('w', 't'): 8,\n",
       " ('l', 'f'): 22,\n",
       " ('d', 'm'): 30,\n",
       " ('p', 'j'): 1,\n",
       " ('j', 'y'): 10,\n",
       " ('y', 'f'): 12,\n",
       " ('q', 'i'): 13,\n",
       " ('j', 'v'): 5,\n",
       " ('q', 'l'): 1,\n",
       " ('s', 'z'): 10,\n",
       " ('k', 'm'): 9,\n",
       " ('w', 'l'): 13,\n",
       " ('p', 'f'): 1,\n",
       " ('q', 'w'): 3,\n",
       " ('n', 'x'): 6,\n",
       " ('k', 'c'): 2,\n",
       " ('t', 'v'): 15,\n",
       " ('c', 'u'): 35,\n",
       " ('z', 'k'): 2,\n",
       " ('c', 'z'): 4,\n",
       " ('y', 'q'): 6,\n",
       " ('y', 'h'): 22,\n",
       " ('r', 'f'): 9,\n",
       " ('s', 'j'): 2,\n",
       " ('h', 'j'): 9,\n",
       " ('g', 'b'): 3,\n",
       " ('u', 'f'): 19,\n",
       " ('s', 'f'): 2,\n",
       " ('q', 'e'): 1,\n",
       " ('b', 'c'): 1,\n",
       " ('c', 'd'): 1,\n",
       " ('z', 'j'): 2,\n",
       " ('n', 'q'): 2,\n",
       " ('m', 'f'): 1,\n",
       " ('p', 'n'): 1,\n",
       " ('f', 'z'): 2,\n",
       " ('b', 'n'): 4,\n",
       " ('w', 'd'): 8,\n",
       " ('w', 'b'): 1,\n",
       " ('b', 'd'): 65,\n",
       " ('z', 's'): 4,\n",
       " ('p', 'c'): 1,\n",
       " ('h', 'g'): 2,\n",
       " ('m', 'j'): 7,\n",
       " ('w', 'w'): 2,\n",
       " ('k', 'j'): 2,\n",
       " ('h', 'p'): 1,\n",
       " ('j', 'k'): 2,\n",
       " ('o', 'q'): 3,\n",
       " ('f', 'w'): 4,\n",
       " ('f', 'h'): 1,\n",
       " ('w', 'm'): 2,\n",
       " ('b', 'j'): 1,\n",
       " ('r', 'q'): 16,\n",
       " ('z', 'c'): 2,\n",
       " ('z', 'v'): 2,\n",
       " ('f', 'g'): 1,\n",
       " ('n', 'p'): 5,\n",
       " ('z', 'g'): 1,\n",
       " ('d', 't'): 4,\n",
       " ('w', 'f'): 2,\n",
       " ('d', 'f'): 5,\n",
       " ('w', 'k'): 6,\n",
       " ('q', 'm'): 2,\n",
       " ('k', 'z'): 2,\n",
       " ('j', 'j'): 2,\n",
       " ('c', 'p'): 1,\n",
       " ('p', 'k'): 1,\n",
       " ('p', 'm'): 1,\n",
       " ('j', 'd'): 4,\n",
       " ('r', 'x'): 3,\n",
       " ('x', 'n'): 1,\n",
       " ('d', 'c'): 3,\n",
       " ('g', 'j'): 3,\n",
       " ('x', 'f'): 3,\n",
       " ('j', 'c'): 4,\n",
       " ('s', 'q'): 1,\n",
       " ('k', 'f'): 1,\n",
       " ('z', 'p'): 2,\n",
       " ('j', 't'): 2,\n",
       " ('k', 'b'): 2,\n",
       " ('m', 'k'): 1,\n",
       " ('m', 'w'): 2,\n",
       " ('x', 'h'): 1,\n",
       " ('h', 'f'): 2,\n",
       " ('x', 'd'): 5,\n",
       " ('y', 'w'): 4,\n",
       " ('z', 'w'): 3,\n",
       " ('d', 'k'): 3,\n",
       " ('c', 'g'): 2,\n",
       " ('u', 'u'): 3,\n",
       " ('t', 'f'): 2,\n",
       " ('g', 'm'): 6,\n",
       " ('m', 'v'): 3,\n",
       " ('c', 'x'): 3,\n",
       " ('h', 'c'): 2,\n",
       " ('g', 'f'): 1,\n",
       " ('q', 'o'): 2,\n",
       " ('l', 'q'): 3,\n",
       " ('v', 'b'): 1,\n",
       " ('j', 'p'): 1,\n",
       " ('k', 'd'): 2,\n",
       " ('g', 'z'): 1,\n",
       " ('v', 'd'): 1,\n",
       " ('d', 'b'): 1,\n",
       " ('v', 'h'): 1,\n",
       " ('k', 'v'): 2,\n",
       " ('h', 'h'): 1,\n",
       " ('s', 'g'): 2,\n",
       " ('g', 'v'): 1,\n",
       " ('d', 'q'): 1,\n",
       " ('x', 'b'): 1,\n",
       " ('w', 'z'): 1,\n",
       " ('h', 'q'): 1,\n",
       " ('j', 'b'): 1,\n",
       " ('z', 'd'): 2,\n",
       " ('x', 'm'): 1,\n",
       " ('w', 'g'): 1,\n",
       " ('t', 'b'): 1,\n",
       " ('z', 'x'): 1}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37f07d41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(('<S>', 'e'), 1531), (('e', 'm'), 769), (('m', 'm'), 168), (('m', 'a'), 2590), (('a', '<E>'), 6640), (('<S>', 'o'), 394), (('o', 'l'), 619), (('l', 'i'), 2480), (('i', 'v'), 269), (('v', 'i'), 911), (('i', 'a'), 2445), (('<S>', 'a'), 4410), (('a', 'v'), 834), (('v', 'a'), 642), (('<S>', 'i'), 591), (('i', 's'), 1316), (('s', 'a'), 1201), (('a', 'b'), 541), (('b', 'e'), 655), (('e', 'l'), 3248), (('l', 'l'), 1345), (('l', 'a'), 2623), (('<S>', 's'), 2055), (('s', 'o'), 531), (('o', 'p'), 95), (('p', 'h'), 204), (('h', 'i'), 729), (('<S>', 'c'), 1542), (('c', 'h'), 664), (('h', 'a'), 2244), (('a', 'r'), 3264), (('r', 'l'), 413), (('l', 'o'), 692), (('o', 't'), 118), (('t', 't'), 374), (('t', 'e'), 716), (('e', '<E>'), 3983), (('<S>', 'm'), 2538), (('m', 'i'), 1256), (('a', 'm'), 1634), (('m', 'e'), 818), (('<S>', 'h'), 874), (('r', 'p'), 14), (('p', 'e'), 197), (('e', 'r'), 1958), (('r', '<E>'), 1377), (('e', 'v'), 463), (('v', 'e'), 568), (('l', 'y'), 1588), (('y', 'n'), 1826), (('n', '<E>'), 6763), (('b', 'i'), 217), (('i', 'g'), 428), (('g', 'a'), 330), (('a', 'i'), 1650), (('i', 'l'), 1345), (('l', '<E>'), 1314), (('y', '<E>'), 2007), (('i', 'z'), 277), (('z', 'a'), 860), (('e', 't'), 580), (('t', 'h'), 647), (('h', '<E>'), 2409), (('r', 'y'), 773), (('o', 'f'), 34), (('f', 'i'), 160), (('c', 'a'), 815), (('r', 'i'), 3033), (('s', 'c'), 60), (('l', 'e'), 2921), (('t', '<E>'), 483), (('<S>', 'v'), 376), (('i', 'c'), 509), (('c', 't'), 35), (('t', 'o'), 667), (('o', 'r'), 1059), (('a', 'd'), 1042), (('d', 'i'), 674), (('o', 'n'), 2411), (('<S>', 'l'), 1572), (('l', 'u'), 324), (('u', 'n'), 275), (('n', 'a'), 2977), (('<S>', 'g'), 669), (('g', 'r'), 201), (('r', 'a'), 2356), (('a', 'c'), 470), (('c', 'e'), 551), (('h', 'l'), 185), (('o', 'e'), 132), (('<S>', 'p'), 515), (('e', 'n'), 2675), (('n', 'e'), 1359), (('a', 'y'), 2050), (('y', 'l'), 1104), (('<S>', 'r'), 1639), (('e', 'y'), 1070), (('<S>', 'z'), 929), (('z', 'o'), 110), (('<S>', 'n'), 1146), (('n', 'o'), 496), (('e', 'a'), 679), (('a', 'n'), 5438), (('n', 'n'), 1906), (('a', 'h'), 2332), (('d', 'd'), 149), (('a', 'u'), 381), (('u', 'b'), 103), (('b', 'r'), 842), (('r', 'e'), 1697), (('i', 'e'), 1653), (('s', 't'), 765), (('a', 't'), 687), (('t', 'a'), 1027), (('a', 'l'), 2528), (('a', 'z'), 435), (('z', 'e'), 373), (('i', 'o'), 588), (('u', 'r'), 414), (('r', 'o'), 869), (('u', 'd'), 136), (('d', 'r'), 424), (('<S>', 'b'), 1306), (('o', 'o'), 115), (('o', 'k'), 68), (('k', 'l'), 139), (('c', 'l'), 116), (('i', 'r'), 849), (('s', 'k'), 82), (('k', 'y'), 379), (('u', 'c'), 103), (('c', 'y'), 104), (('p', 'a'), 209), (('s', 'l'), 279), (('i', 'n'), 2126), (('o', 'v'), 176), (('g', 'e'), 334), (('e', 's'), 861), (('s', 'i'), 684), (('s', '<E>'), 1169), (('<S>', 'k'), 2963), (('k', 'e'), 895), (('e', 'd'), 384), (('d', 'y'), 317), (('n', 't'), 443), (('y', 'a'), 2143), (('<S>', 'w'), 307), (('w', 'i'), 148), (('o', 'w'), 114), (('w', '<E>'), 51), (('k', 'i'), 509), (('n', 's'), 278), (('a', 'o'), 63), (('o', 'm'), 261), (('i', '<E>'), 2489), (('a', 'a'), 556), (('i', 'y'), 779), (('d', 'e'), 1283), (('c', 'o'), 380), (('r', 'u'), 252), (('b', 'y'), 83), (('s', 'e'), 884), (('n', 'i'), 1725), (('i', 't'), 541), (('t', 'y'), 341), (('u', 't'), 82), (('t', 'u'), 78), (('u', 'm'), 154), (('m', 'n'), 20), (('g', 'i'), 190), (('t', 'i'), 532), (('<S>', 'q'), 92), (('q', 'u'), 206), (('u', 'i'), 121), (('a', 'e'), 692), (('e', 'h'), 152), (('v', 'y'), 121), (('p', 'i'), 61), (('i', 'p'), 53), (('y', 'd'), 272), (('e', 'x'), 132), (('x', 'a'), 103), (('<S>', 'j'), 2422), (('j', 'o'), 479), (('o', 's'), 504), (('e', 'p'), 83), (('j', 'u'), 202), (('u', 'l'), 301), (('<S>', 'd'), 1690), (('k', 'a'), 1731), (('e', 'e'), 1271), (('y', 't'), 104), (('d', 'l'), 60), (('c', 'k'), 316), (('n', 'z'), 145), (('z', 'i'), 364), (('a', 'g'), 168), (('d', 'a'), 1303), (('j', 'a'), 1473), (('h', 'e'), 674), (('<S>', 'x'), 134), (('x', 'i'), 102), (('i', 'm'), 427), (('e', 'i'), 818), (('<S>', 't'), 1308), (('<S>', 'f'), 417), (('f', 'a'), 242), (('n', 'd'), 704), (('r', 'g'), 76), (('a', 's'), 1118), (('s', 'h'), 1285), (('b', 'a'), 321), (('k', 'h'), 307), (('s', 'm'), 90), (('o', 'd'), 190), (('r', 's'), 190), (('g', 'h'), 360), (('s', 'y'), 215), (('y', 's'), 401), (('s', 's'), 461), (('e', 'c'), 153), (('c', 'i'), 271), (('m', 'o'), 452), (('r', 'k'), 90), (('n', 'l'), 195), (('d', 'n'), 31), (('r', 'd'), 187), (('o', 'i'), 69), (('t', 'r'), 352), (('m', 'b'), 112), (('r', 'm'), 162), (('n', 'y'), 465), (('d', 'o'), 378), (('o', 'a'), 149), (('o', 'c'), 114), (('m', 'y'), 287), (('s', 'u'), 185), (('m', 'c'), 51), (('p', 'r'), 151), (('o', 'u'), 275), (('r', 'n'), 140), (('w', 'a'), 280), (('e', 'b'), 121), (('c', 'c'), 42), (('a', 'w'), 161), (('w', 'y'), 73), (('y', 'e'), 301), (('e', 'o'), 269), (('a', 'k'), 568), (('n', 'g'), 273), (('k', 'o'), 344), (('b', 'l'), 103), (('h', 'o'), 287), (('e', 'g'), 125), (('f', 'r'), 114), (('s', 'p'), 51), (('l', 's'), 94), (('y', 'z'), 78), (('g', 'g'), 25), (('z', 'u'), 73), (('i', 'd'), 440), (('m', '<E>'), 516), (('o', 'g'), 44), (('j', 'e'), 440), (('g', 'n'), 27), (('y', 'r'), 291), (('c', '<E>'), 97), (('c', 'q'), 11), (('u', 'e'), 169), (('i', 'f'), 101), (('f', 'e'), 123), (('i', 'x'), 89), (('x', '<E>'), 164), (('o', 'y'), 103), (('g', 'o'), 83), (('g', 't'), 31), (('l', 't'), 77), (('g', 'w'), 26), (('w', 'e'), 149), (('l', 'd'), 138), (('a', 'p'), 82), (('h', 'n'), 138), (('t', 'l'), 134), (('m', 'r'), 97), (('n', 'c'), 213), (('l', 'b'), 52), (('i', 'k'), 445), (('<S>', 'y'), 535), (('t', 'z'), 105), (('h', 'r'), 204), (('j', 'i'), 119), (('h', 't'), 71), (('r', 'r'), 425), (('z', 'l'), 123), (('w', 'r'), 22), (('b', 'b'), 38), (('r', 't'), 208), (('l', 'v'), 72), (('e', 'j'), 55), (('o', 'h'), 171), (('u', 's'), 474), (('i', 'b'), 110), (('g', 'l'), 32), (('h', 'y'), 213), (('p', 'o'), 59), (('p', 'p'), 39), (('p', 'y'), 12), (('n', 'r'), 44), (('z', 'm'), 35), (('v', 'o'), 153), (('l', 'm'), 60), (('o', 'x'), 45), (('d', '<E>'), 516), (('i', 'u'), 109), (('v', '<E>'), 88), (('f', 'f'), 44), (('b', 'o'), 105), (('e', 'k'), 178), (('c', 'r'), 76), (('d', 'g'), 25), (('r', 'c'), 99), (('r', 'h'), 121), (('n', 'k'), 58), (('h', 'u'), 166), (('d', 's'), 29), (('a', 'x'), 182), (('y', 'c'), 115), (('e', 'w'), 50), (('v', 'k'), 3), (('z', 'h'), 43), (('w', 'h'), 23), (('t', 'n'), 22), (('x', 'l'), 39), (('g', 'u'), 85), (('u', 'a'), 163), (('u', 'p'), 16), (('u', 'g'), 47), (('d', 'u'), 92), (('l', 'c'), 25), (('r', 'b'), 41), (('a', 'q'), 60), (('b', '<E>'), 114), (('g', 'y'), 31), (('y', 'p'), 15), (('p', 't'), 17), (('e', 'z'), 181), (('z', 'r'), 32), (('f', 'l'), 20), (('o', '<E>'), 855), (('o', 'b'), 140), (('u', 'z'), 45), (('z', '<E>'), 160), (('i', 'q'), 52), (('y', 'v'), 106), (('n', 'v'), 55), (('d', 'h'), 118), (('g', 'd'), 19), (('t', 's'), 35), (('n', 'h'), 26), (('y', 'j'), 23), (('k', 'r'), 109), (('z', 'b'), 4), (('g', '<E>'), 108), (('a', 'j'), 175), (('r', 'j'), 25), (('m', 'p'), 38), (('p', 'b'), 2), (('y', 'o'), 271), (('z', 'y'), 147), (('p', 'l'), 16), (('l', 'k'), 24), (('i', 'j'), 76), (('x', 'e'), 36), (('y', 'u'), 141), (('l', 'n'), 14), (('u', 'x'), 34), (('i', 'h'), 95), (('w', 's'), 20), (('k', 's'), 95), (('m', 'u'), 139), (('y', 'k'), 86), (('e', 'f'), 82), (('k', '<E>'), 363), (('y', 'm'), 148), (('z', 'z'), 45), (('m', 'd'), 24), (('s', 'r'), 55), (('e', 'u'), 69), (('l', 'h'), 19), (('a', 'f'), 134), (('r', 'w'), 21), (('n', 'u'), 96), (('v', 'r'), 48), (('m', 's'), 35), (('<S>', 'u'), 78), (('f', 's'), 6), (('y', 'b'), 27), (('x', 'o'), 41), (('g', 's'), 30), (('x', 'y'), 30), (('w', 'n'), 58), (('j', 'h'), 45), (('f', 'n'), 4), (('n', 'j'), 44), (('r', 'v'), 80), (('n', 'm'), 19), (('t', 'c'), 17), (('s', 'w'), 24), (('k', 't'), 17), (('f', 't'), 18), (('x', 't'), 70), (('u', 'v'), 37), (('k', 'k'), 20), (('s', 'n'), 24), (('u', '<E>'), 155), (('j', 'r'), 11), (('y', 'x'), 28), (('h', 'm'), 117), (('e', 'q'), 14), (('u', 'o'), 10), (('f', '<E>'), 80), (('h', 'z'), 20), (('h', 'k'), 29), (('y', 'g'), 30), (('q', 'r'), 1), (('v', 'n'), 8), (('s', 'd'), 9), (('y', 'i'), 192), (('n', 'w'), 11), (('d', 'v'), 17), (('h', 'v'), 39), (('x', 'w'), 3), (('o', 'z'), 54), (('k', 'u'), 50), (('u', 'h'), 58), (('k', 'n'), 26), (('s', 'b'), 21), (('i', 'i'), 82), (('y', 'y'), 23), (('r', 'z'), 23), (('l', 'g'), 6), (('l', 'p'), 15), (('p', '<E>'), 33), (('b', 'u'), 45), (('f', 'u'), 10), (('b', 'h'), 41), (('f', 'y'), 14), (('u', 'w'), 86), (('x', 'u'), 5), (('q', '<E>'), 28), (('l', 'r'), 18), (('m', 'h'), 5), (('l', 'w'), 16), (('j', '<E>'), 71), (('s', 'v'), 14), (('m', 'l'), 5), (('n', 'f'), 11), (('u', 'j'), 14), (('f', 'o'), 60), (('j', 'l'), 9), (('t', 'g'), 2), (('j', 'm'), 5), (('v', 'v'), 7), (('p', 's'), 16), (('t', 'w'), 11), (('x', 'c'), 4), (('u', 'k'), 93), (('v', 'l'), 14), (('h', 'd'), 24), (('l', 'z'), 10), (('k', 'w'), 34), (('n', 'b'), 8), (('q', 's'), 2), (('i', 'w'), 8), (('c', 's'), 5), (('h', 's'), 31), (('m', 't'), 4), (('h', 'w'), 10), (('x', 'x'), 38), (('t', 'x'), 2), (('d', 'z'), 1), (('x', 'z'), 19), (('t', 'm'), 4), (('t', 'j'), 3), (('u', 'q'), 10), (('q', 'a'), 13), (('f', 'k'), 2), (('z', 'n'), 4), (('l', 'j'), 6), (('j', 'w'), 6), (('v', 'u'), 7), (('c', 'j'), 3), (('h', 'b'), 8), (('z', 't'), 4), (('p', 'u'), 4), (('m', 'z'), 11), (('x', 's'), 31), (('b', 't'), 2), (('u', 'y'), 13), (('d', 'j'), 9), (('j', 's'), 7), (('w', 'u'), 25), (('o', 'j'), 16), (('b', 's'), 8), (('d', 'w'), 23), (('w', 'o'), 36), (('j', 'n'), 2), (('w', 't'), 8), (('l', 'f'), 22), (('d', 'm'), 30), (('p', 'j'), 1), (('j', 'y'), 10), (('y', 'f'), 12), (('q', 'i'), 13), (('j', 'v'), 5), (('q', 'l'), 1), (('s', 'z'), 10), (('k', 'm'), 9), (('w', 'l'), 13), (('p', 'f'), 1), (('q', 'w'), 3), (('n', 'x'), 6), (('k', 'c'), 2), (('t', 'v'), 15), (('c', 'u'), 35), (('z', 'k'), 2), (('c', 'z'), 4), (('y', 'q'), 6), (('y', 'h'), 22), (('r', 'f'), 9), (('s', 'j'), 2), (('h', 'j'), 9), (('g', 'b'), 3), (('u', 'f'), 19), (('s', 'f'), 2), (('q', 'e'), 1), (('b', 'c'), 1), (('c', 'd'), 1), (('z', 'j'), 2), (('n', 'q'), 2), (('m', 'f'), 1), (('p', 'n'), 1), (('f', 'z'), 2), (('b', 'n'), 4), (('w', 'd'), 8), (('w', 'b'), 1), (('b', 'd'), 65), (('z', 's'), 4), (('p', 'c'), 1), (('h', 'g'), 2), (('m', 'j'), 7), (('w', 'w'), 2), (('k', 'j'), 2), (('h', 'p'), 1), (('j', 'k'), 2), (('o', 'q'), 3), (('f', 'w'), 4), (('f', 'h'), 1), (('w', 'm'), 2), (('b', 'j'), 1), (('r', 'q'), 16), (('z', 'c'), 2), (('z', 'v'), 2), (('f', 'g'), 1), (('n', 'p'), 5), (('z', 'g'), 1), (('d', 't'), 4), (('w', 'f'), 2), (('d', 'f'), 5), (('w', 'k'), 6), (('q', 'm'), 2), (('k', 'z'), 2), (('j', 'j'), 2), (('c', 'p'), 1), (('p', 'k'), 1), (('p', 'm'), 1), (('j', 'd'), 4), (('r', 'x'), 3), (('x', 'n'), 1), (('d', 'c'), 3), (('g', 'j'), 3), (('x', 'f'), 3), (('j', 'c'), 4), (('s', 'q'), 1), (('k', 'f'), 1), (('z', 'p'), 2), (('j', 't'), 2), (('k', 'b'), 2), (('m', 'k'), 1), (('m', 'w'), 2), (('x', 'h'), 1), (('h', 'f'), 2), (('x', 'd'), 5), (('y', 'w'), 4), (('z', 'w'), 3), (('d', 'k'), 3), (('c', 'g'), 2), (('u', 'u'), 3), (('t', 'f'), 2), (('g', 'm'), 6), (('m', 'v'), 3), (('c', 'x'), 3), (('h', 'c'), 2), (('g', 'f'), 1), (('q', 'o'), 2), (('l', 'q'), 3), (('v', 'b'), 1), (('j', 'p'), 1), (('k', 'd'), 2), (('g', 'z'), 1), (('v', 'd'), 1), (('d', 'b'), 1), (('v', 'h'), 1), (('k', 'v'), 2), (('h', 'h'), 1), (('s', 'g'), 2), (('g', 'v'), 1), (('d', 'q'), 1), (('x', 'b'), 1), (('w', 'z'), 1), (('h', 'q'), 1), (('j', 'b'), 1), (('z', 'd'), 2), (('x', 'm'), 1), (('w', 'g'), 1), (('t', 'b'), 1), (('z', 'x'), 1)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5aec83a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('q', 'r'), 1),\n",
       " (('d', 'z'), 1),\n",
       " (('p', 'j'), 1),\n",
       " (('q', 'l'), 1),\n",
       " (('p', 'f'), 1),\n",
       " (('q', 'e'), 1),\n",
       " (('b', 'c'), 1),\n",
       " (('c', 'd'), 1),\n",
       " (('m', 'f'), 1),\n",
       " (('p', 'n'), 1),\n",
       " (('w', 'b'), 1),\n",
       " (('p', 'c'), 1),\n",
       " (('h', 'p'), 1),\n",
       " (('f', 'h'), 1),\n",
       " (('b', 'j'), 1),\n",
       " (('f', 'g'), 1),\n",
       " (('z', 'g'), 1),\n",
       " (('c', 'p'), 1),\n",
       " (('p', 'k'), 1),\n",
       " (('p', 'm'), 1),\n",
       " (('x', 'n'), 1),\n",
       " (('s', 'q'), 1),\n",
       " (('k', 'f'), 1),\n",
       " (('m', 'k'), 1),\n",
       " (('x', 'h'), 1),\n",
       " (('g', 'f'), 1),\n",
       " (('v', 'b'), 1),\n",
       " (('j', 'p'), 1),\n",
       " (('g', 'z'), 1),\n",
       " (('v', 'd'), 1),\n",
       " (('d', 'b'), 1),\n",
       " (('v', 'h'), 1),\n",
       " (('h', 'h'), 1),\n",
       " (('g', 'v'), 1),\n",
       " (('d', 'q'), 1),\n",
       " (('x', 'b'), 1),\n",
       " (('w', 'z'), 1),\n",
       " (('h', 'q'), 1),\n",
       " (('j', 'b'), 1),\n",
       " (('x', 'm'), 1),\n",
       " (('w', 'g'), 1),\n",
       " (('t', 'b'), 1),\n",
       " (('z', 'x'), 1),\n",
       " (('p', 'b'), 2),\n",
       " (('t', 'g'), 2),\n",
       " (('q', 's'), 2),\n",
       " (('t', 'x'), 2),\n",
       " (('f', 'k'), 2),\n",
       " (('b', 't'), 2),\n",
       " (('j', 'n'), 2),\n",
       " (('k', 'c'), 2),\n",
       " (('z', 'k'), 2),\n",
       " (('s', 'j'), 2),\n",
       " (('s', 'f'), 2),\n",
       " (('z', 'j'), 2),\n",
       " (('n', 'q'), 2),\n",
       " (('f', 'z'), 2),\n",
       " (('h', 'g'), 2),\n",
       " (('w', 'w'), 2),\n",
       " (('k', 'j'), 2),\n",
       " (('j', 'k'), 2),\n",
       " (('w', 'm'), 2),\n",
       " (('z', 'c'), 2),\n",
       " (('z', 'v'), 2),\n",
       " (('w', 'f'), 2),\n",
       " (('q', 'm'), 2),\n",
       " (('k', 'z'), 2),\n",
       " (('j', 'j'), 2),\n",
       " (('z', 'p'), 2),\n",
       " (('j', 't'), 2),\n",
       " (('k', 'b'), 2),\n",
       " (('m', 'w'), 2),\n",
       " (('h', 'f'), 2),\n",
       " (('c', 'g'), 2),\n",
       " (('t', 'f'), 2),\n",
       " (('h', 'c'), 2),\n",
       " (('q', 'o'), 2),\n",
       " (('k', 'd'), 2),\n",
       " (('k', 'v'), 2),\n",
       " (('s', 'g'), 2),\n",
       " (('z', 'd'), 2),\n",
       " (('v', 'k'), 3),\n",
       " (('x', 'w'), 3),\n",
       " (('t', 'j'), 3),\n",
       " (('c', 'j'), 3),\n",
       " (('q', 'w'), 3),\n",
       " (('g', 'b'), 3),\n",
       " (('o', 'q'), 3),\n",
       " (('r', 'x'), 3),\n",
       " (('d', 'c'), 3),\n",
       " (('g', 'j'), 3),\n",
       " (('x', 'f'), 3),\n",
       " (('z', 'w'), 3),\n",
       " (('d', 'k'), 3),\n",
       " (('u', 'u'), 3),\n",
       " (('m', 'v'), 3),\n",
       " (('c', 'x'), 3),\n",
       " (('l', 'q'), 3),\n",
       " (('z', 'b'), 4),\n",
       " (('f', 'n'), 4),\n",
       " (('x', 'c'), 4),\n",
       " (('m', 't'), 4),\n",
       " (('t', 'm'), 4),\n",
       " (('z', 'n'), 4),\n",
       " (('z', 't'), 4),\n",
       " (('p', 'u'), 4),\n",
       " (('c', 'z'), 4),\n",
       " (('b', 'n'), 4),\n",
       " (('z', 's'), 4),\n",
       " (('f', 'w'), 4),\n",
       " (('d', 't'), 4),\n",
       " (('j', 'd'), 4),\n",
       " (('j', 'c'), 4),\n",
       " (('y', 'w'), 4),\n",
       " (('x', 'u'), 5),\n",
       " (('m', 'h'), 5),\n",
       " (('m', 'l'), 5),\n",
       " (('j', 'm'), 5),\n",
       " (('c', 's'), 5),\n",
       " (('j', 'v'), 5),\n",
       " (('n', 'p'), 5),\n",
       " (('d', 'f'), 5),\n",
       " (('x', 'd'), 5),\n",
       " (('f', 's'), 6),\n",
       " (('l', 'g'), 6),\n",
       " (('l', 'j'), 6),\n",
       " (('j', 'w'), 6),\n",
       " (('n', 'x'), 6),\n",
       " (('y', 'q'), 6),\n",
       " (('w', 'k'), 6),\n",
       " (('g', 'm'), 6),\n",
       " (('v', 'v'), 7),\n",
       " (('v', 'u'), 7),\n",
       " (('j', 's'), 7),\n",
       " (('m', 'j'), 7),\n",
       " (('v', 'n'), 8),\n",
       " (('n', 'b'), 8),\n",
       " (('i', 'w'), 8),\n",
       " (('h', 'b'), 8),\n",
       " (('b', 's'), 8),\n",
       " (('w', 't'), 8),\n",
       " (('w', 'd'), 8),\n",
       " (('s', 'd'), 9),\n",
       " (('j', 'l'), 9),\n",
       " (('d', 'j'), 9),\n",
       " (('k', 'm'), 9),\n",
       " (('r', 'f'), 9),\n",
       " (('h', 'j'), 9),\n",
       " (('u', 'o'), 10),\n",
       " (('f', 'u'), 10),\n",
       " (('l', 'z'), 10),\n",
       " (('h', 'w'), 10),\n",
       " (('u', 'q'), 10),\n",
       " (('j', 'y'), 10),\n",
       " (('s', 'z'), 10),\n",
       " (('c', 'q'), 11),\n",
       " (('j', 'r'), 11),\n",
       " (('n', 'w'), 11),\n",
       " (('n', 'f'), 11),\n",
       " (('t', 'w'), 11),\n",
       " (('m', 'z'), 11),\n",
       " (('p', 'y'), 12),\n",
       " (('y', 'f'), 12),\n",
       " (('q', 'a'), 13),\n",
       " (('u', 'y'), 13),\n",
       " (('q', 'i'), 13),\n",
       " (('w', 'l'), 13),\n",
       " (('r', 'p'), 14),\n",
       " (('l', 'n'), 14),\n",
       " (('e', 'q'), 14),\n",
       " (('f', 'y'), 14),\n",
       " (('s', 'v'), 14),\n",
       " (('u', 'j'), 14),\n",
       " (('v', 'l'), 14),\n",
       " (('y', 'p'), 15),\n",
       " (('l', 'p'), 15),\n",
       " (('t', 'v'), 15),\n",
       " (('u', 'p'), 16),\n",
       " (('p', 'l'), 16),\n",
       " (('l', 'w'), 16),\n",
       " (('p', 's'), 16),\n",
       " (('o', 'j'), 16),\n",
       " (('r', 'q'), 16),\n",
       " (('p', 't'), 17),\n",
       " (('t', 'c'), 17),\n",
       " (('k', 't'), 17),\n",
       " (('d', 'v'), 17),\n",
       " (('f', 't'), 18),\n",
       " (('l', 'r'), 18),\n",
       " (('g', 'd'), 19),\n",
       " (('l', 'h'), 19),\n",
       " (('n', 'm'), 19),\n",
       " (('x', 'z'), 19),\n",
       " (('u', 'f'), 19),\n",
       " (('m', 'n'), 20),\n",
       " (('f', 'l'), 20),\n",
       " (('w', 's'), 20),\n",
       " (('k', 'k'), 20),\n",
       " (('h', 'z'), 20),\n",
       " (('r', 'w'), 21),\n",
       " (('s', 'b'), 21),\n",
       " (('w', 'r'), 22),\n",
       " (('t', 'n'), 22),\n",
       " (('l', 'f'), 22),\n",
       " (('y', 'h'), 22),\n",
       " (('w', 'h'), 23),\n",
       " (('y', 'j'), 23),\n",
       " (('y', 'y'), 23),\n",
       " (('r', 'z'), 23),\n",
       " (('d', 'w'), 23),\n",
       " (('l', 'k'), 24),\n",
       " (('m', 'd'), 24),\n",
       " (('s', 'w'), 24),\n",
       " (('s', 'n'), 24),\n",
       " (('h', 'd'), 24),\n",
       " (('g', 'g'), 25),\n",
       " (('d', 'g'), 25),\n",
       " (('l', 'c'), 25),\n",
       " (('r', 'j'), 25),\n",
       " (('w', 'u'), 25),\n",
       " (('g', 'w'), 26),\n",
       " (('n', 'h'), 26),\n",
       " (('k', 'n'), 26),\n",
       " (('g', 'n'), 27),\n",
       " (('y', 'b'), 27),\n",
       " (('y', 'x'), 28),\n",
       " (('q', '<E>'), 28),\n",
       " (('d', 's'), 29),\n",
       " (('h', 'k'), 29),\n",
       " (('g', 's'), 30),\n",
       " (('x', 'y'), 30),\n",
       " (('y', 'g'), 30),\n",
       " (('d', 'm'), 30),\n",
       " (('d', 'n'), 31),\n",
       " (('g', 't'), 31),\n",
       " (('g', 'y'), 31),\n",
       " (('h', 's'), 31),\n",
       " (('x', 's'), 31),\n",
       " (('g', 'l'), 32),\n",
       " (('z', 'r'), 32),\n",
       " (('p', '<E>'), 33),\n",
       " (('o', 'f'), 34),\n",
       " (('u', 'x'), 34),\n",
       " (('k', 'w'), 34),\n",
       " (('c', 't'), 35),\n",
       " (('z', 'm'), 35),\n",
       " (('t', 's'), 35),\n",
       " (('m', 's'), 35),\n",
       " (('c', 'u'), 35),\n",
       " (('x', 'e'), 36),\n",
       " (('w', 'o'), 36),\n",
       " (('u', 'v'), 37),\n",
       " (('b', 'b'), 38),\n",
       " (('m', 'p'), 38),\n",
       " (('x', 'x'), 38),\n",
       " (('p', 'p'), 39),\n",
       " (('x', 'l'), 39),\n",
       " (('h', 'v'), 39),\n",
       " (('r', 'b'), 41),\n",
       " (('x', 'o'), 41),\n",
       " (('b', 'h'), 41),\n",
       " (('c', 'c'), 42),\n",
       " (('z', 'h'), 43),\n",
       " (('o', 'g'), 44),\n",
       " (('n', 'r'), 44),\n",
       " (('f', 'f'), 44),\n",
       " (('n', 'j'), 44),\n",
       " (('o', 'x'), 45),\n",
       " (('u', 'z'), 45),\n",
       " (('z', 'z'), 45),\n",
       " (('j', 'h'), 45),\n",
       " (('b', 'u'), 45),\n",
       " (('u', 'g'), 47),\n",
       " (('v', 'r'), 48),\n",
       " (('e', 'w'), 50),\n",
       " (('k', 'u'), 50),\n",
       " (('w', '<E>'), 51),\n",
       " (('m', 'c'), 51),\n",
       " (('s', 'p'), 51),\n",
       " (('l', 'b'), 52),\n",
       " (('i', 'q'), 52),\n",
       " (('i', 'p'), 53),\n",
       " (('o', 'z'), 54),\n",
       " (('e', 'j'), 55),\n",
       " (('n', 'v'), 55),\n",
       " (('s', 'r'), 55),\n",
       " (('n', 'k'), 58),\n",
       " (('w', 'n'), 58),\n",
       " (('u', 'h'), 58),\n",
       " (('p', 'o'), 59),\n",
       " (('s', 'c'), 60),\n",
       " (('d', 'l'), 60),\n",
       " (('l', 'm'), 60),\n",
       " (('a', 'q'), 60),\n",
       " (('f', 'o'), 60),\n",
       " (('p', 'i'), 61),\n",
       " (('a', 'o'), 63),\n",
       " (('b', 'd'), 65),\n",
       " (('o', 'k'), 68),\n",
       " (('o', 'i'), 69),\n",
       " (('e', 'u'), 69),\n",
       " (('x', 't'), 70),\n",
       " (('h', 't'), 71),\n",
       " (('j', '<E>'), 71),\n",
       " (('l', 'v'), 72),\n",
       " (('w', 'y'), 73),\n",
       " (('z', 'u'), 73),\n",
       " (('r', 'g'), 76),\n",
       " (('c', 'r'), 76),\n",
       " (('i', 'j'), 76),\n",
       " (('l', 't'), 77),\n",
       " (('t', 'u'), 78),\n",
       " (('y', 'z'), 78),\n",
       " (('<S>', 'u'), 78),\n",
       " (('r', 'v'), 80),\n",
       " (('f', '<E>'), 80),\n",
       " (('s', 'k'), 82),\n",
       " (('u', 't'), 82),\n",
       " (('a', 'p'), 82),\n",
       " (('e', 'f'), 82),\n",
       " (('i', 'i'), 82),\n",
       " (('b', 'y'), 83),\n",
       " (('e', 'p'), 83),\n",
       " (('g', 'o'), 83),\n",
       " (('g', 'u'), 85),\n",
       " (('y', 'k'), 86),\n",
       " (('u', 'w'), 86),\n",
       " (('v', '<E>'), 88),\n",
       " (('i', 'x'), 89),\n",
       " (('s', 'm'), 90),\n",
       " (('r', 'k'), 90),\n",
       " (('<S>', 'q'), 92),\n",
       " (('d', 'u'), 92),\n",
       " (('u', 'k'), 93),\n",
       " (('l', 's'), 94),\n",
       " (('o', 'p'), 95),\n",
       " (('i', 'h'), 95),\n",
       " (('k', 's'), 95),\n",
       " (('n', 'u'), 96),\n",
       " (('c', '<E>'), 97),\n",
       " (('m', 'r'), 97),\n",
       " (('r', 'c'), 99),\n",
       " (('i', 'f'), 101),\n",
       " (('x', 'i'), 102),\n",
       " (('u', 'b'), 103),\n",
       " (('u', 'c'), 103),\n",
       " (('x', 'a'), 103),\n",
       " (('b', 'l'), 103),\n",
       " (('o', 'y'), 103),\n",
       " (('c', 'y'), 104),\n",
       " (('y', 't'), 104),\n",
       " (('t', 'z'), 105),\n",
       " (('b', 'o'), 105),\n",
       " (('y', 'v'), 106),\n",
       " (('g', '<E>'), 108),\n",
       " (('i', 'u'), 109),\n",
       " (('k', 'r'), 109),\n",
       " (('z', 'o'), 110),\n",
       " (('i', 'b'), 110),\n",
       " (('m', 'b'), 112),\n",
       " (('o', 'w'), 114),\n",
       " (('o', 'c'), 114),\n",
       " (('f', 'r'), 114),\n",
       " (('b', '<E>'), 114),\n",
       " (('o', 'o'), 115),\n",
       " (('y', 'c'), 115),\n",
       " (('c', 'l'), 116),\n",
       " (('h', 'm'), 117),\n",
       " (('o', 't'), 118),\n",
       " (('d', 'h'), 118),\n",
       " (('j', 'i'), 119),\n",
       " (('u', 'i'), 121),\n",
       " (('v', 'y'), 121),\n",
       " (('e', 'b'), 121),\n",
       " (('r', 'h'), 121),\n",
       " (('f', 'e'), 123),\n",
       " (('z', 'l'), 123),\n",
       " (('e', 'g'), 125),\n",
       " (('o', 'e'), 132),\n",
       " (('e', 'x'), 132),\n",
       " (('<S>', 'x'), 134),\n",
       " (('t', 'l'), 134),\n",
       " (('a', 'f'), 134),\n",
       " (('u', 'd'), 136),\n",
       " (('l', 'd'), 138),\n",
       " (('h', 'n'), 138),\n",
       " (('k', 'l'), 139),\n",
       " (('m', 'u'), 139),\n",
       " (('r', 'n'), 140),\n",
       " (('o', 'b'), 140),\n",
       " (('y', 'u'), 141),\n",
       " (('n', 'z'), 145),\n",
       " (('z', 'y'), 147),\n",
       " (('w', 'i'), 148),\n",
       " (('y', 'm'), 148),\n",
       " (('d', 'd'), 149),\n",
       " (('o', 'a'), 149),\n",
       " (('w', 'e'), 149),\n",
       " (('p', 'r'), 151),\n",
       " (('e', 'h'), 152),\n",
       " (('e', 'c'), 153),\n",
       " (('v', 'o'), 153),\n",
       " (('u', 'm'), 154),\n",
       " (('u', '<E>'), 155),\n",
       " (('f', 'i'), 160),\n",
       " (('z', '<E>'), 160),\n",
       " (('a', 'w'), 161),\n",
       " (('r', 'm'), 162),\n",
       " (('u', 'a'), 163),\n",
       " (('x', '<E>'), 164),\n",
       " (('h', 'u'), 166),\n",
       " (('m', 'm'), 168),\n",
       " (('a', 'g'), 168),\n",
       " (('u', 'e'), 169),\n",
       " (('o', 'h'), 171),\n",
       " (('a', 'j'), 175),\n",
       " (('o', 'v'), 176),\n",
       " (('e', 'k'), 178),\n",
       " (('e', 'z'), 181),\n",
       " (('a', 'x'), 182),\n",
       " (('h', 'l'), 185),\n",
       " (('s', 'u'), 185),\n",
       " (('r', 'd'), 187),\n",
       " (('g', 'i'), 190),\n",
       " (('o', 'd'), 190),\n",
       " (('r', 's'), 190),\n",
       " (('y', 'i'), 192),\n",
       " (('n', 'l'), 195),\n",
       " (('p', 'e'), 197),\n",
       " (('g', 'r'), 201),\n",
       " (('j', 'u'), 202),\n",
       " (('p', 'h'), 204),\n",
       " (('h', 'r'), 204),\n",
       " (('q', 'u'), 206),\n",
       " (('r', 't'), 208),\n",
       " (('p', 'a'), 209),\n",
       " (('n', 'c'), 213),\n",
       " (('h', 'y'), 213),\n",
       " (('s', 'y'), 215),\n",
       " (('b', 'i'), 217),\n",
       " (('f', 'a'), 242),\n",
       " (('r', 'u'), 252),\n",
       " (('o', 'm'), 261),\n",
       " (('i', 'v'), 269),\n",
       " (('e', 'o'), 269),\n",
       " (('c', 'i'), 271),\n",
       " (('y', 'o'), 271),\n",
       " (('y', 'd'), 272),\n",
       " (('n', 'g'), 273),\n",
       " (('u', 'n'), 275),\n",
       " (('o', 'u'), 275),\n",
       " (('i', 'z'), 277),\n",
       " (('n', 's'), 278),\n",
       " (('s', 'l'), 279),\n",
       " (('w', 'a'), 280),\n",
       " (('m', 'y'), 287),\n",
       " (('h', 'o'), 287),\n",
       " (('y', 'r'), 291),\n",
       " (('u', 'l'), 301),\n",
       " (('y', 'e'), 301),\n",
       " (('<S>', 'w'), 307),\n",
       " (('k', 'h'), 307),\n",
       " (('c', 'k'), 316),\n",
       " (('d', 'y'), 317),\n",
       " (('b', 'a'), 321),\n",
       " (('l', 'u'), 324),\n",
       " (('g', 'a'), 330),\n",
       " (('g', 'e'), 334),\n",
       " (('t', 'y'), 341),\n",
       " (('k', 'o'), 344),\n",
       " (('t', 'r'), 352),\n",
       " (('g', 'h'), 360),\n",
       " (('k', '<E>'), 363),\n",
       " (('z', 'i'), 364),\n",
       " (('z', 'e'), 373),\n",
       " (('t', 't'), 374),\n",
       " (('<S>', 'v'), 376),\n",
       " (('d', 'o'), 378),\n",
       " (('k', 'y'), 379),\n",
       " (('c', 'o'), 380),\n",
       " (('a', 'u'), 381),\n",
       " (('e', 'd'), 384),\n",
       " (('<S>', 'o'), 394),\n",
       " (('y', 's'), 401),\n",
       " (('r', 'l'), 413),\n",
       " (('u', 'r'), 414),\n",
       " (('<S>', 'f'), 417),\n",
       " (('d', 'r'), 424),\n",
       " (('r', 'r'), 425),\n",
       " (('i', 'm'), 427),\n",
       " (('i', 'g'), 428),\n",
       " (('a', 'z'), 435),\n",
       " (('i', 'd'), 440),\n",
       " (('j', 'e'), 440),\n",
       " (('n', 't'), 443),\n",
       " (('i', 'k'), 445),\n",
       " (('m', 'o'), 452),\n",
       " (('s', 's'), 461),\n",
       " (('e', 'v'), 463),\n",
       " (('n', 'y'), 465),\n",
       " (('a', 'c'), 470),\n",
       " (('u', 's'), 474),\n",
       " (('j', 'o'), 479),\n",
       " (('t', '<E>'), 483),\n",
       " (('n', 'o'), 496),\n",
       " (('o', 's'), 504),\n",
       " (('i', 'c'), 509),\n",
       " (('k', 'i'), 509),\n",
       " (('<S>', 'p'), 515),\n",
       " (('m', '<E>'), 516),\n",
       " (('d', '<E>'), 516),\n",
       " (('s', 'o'), 531),\n",
       " (('t', 'i'), 532),\n",
       " (('<S>', 'y'), 535),\n",
       " (('a', 'b'), 541),\n",
       " (('i', 't'), 541),\n",
       " (('c', 'e'), 551),\n",
       " (('a', 'a'), 556),\n",
       " (('v', 'e'), 568),\n",
       " (('a', 'k'), 568),\n",
       " (('e', 't'), 580),\n",
       " (('i', 'o'), 588),\n",
       " (('<S>', 'i'), 591),\n",
       " (('o', 'l'), 619),\n",
       " (('v', 'a'), 642),\n",
       " (('t', 'h'), 647),\n",
       " (('b', 'e'), 655),\n",
       " (('c', 'h'), 664),\n",
       " (('t', 'o'), 667),\n",
       " (('<S>', 'g'), 669),\n",
       " (('d', 'i'), 674),\n",
       " (('h', 'e'), 674),\n",
       " (('e', 'a'), 679),\n",
       " (('s', 'i'), 684),\n",
       " (('a', 't'), 687),\n",
       " (('l', 'o'), 692),\n",
       " (('a', 'e'), 692),\n",
       " (('n', 'd'), 704),\n",
       " (('t', 'e'), 716),\n",
       " (('h', 'i'), 729),\n",
       " (('s', 't'), 765),\n",
       " (('e', 'm'), 769),\n",
       " (('r', 'y'), 773),\n",
       " (('i', 'y'), 779),\n",
       " (('c', 'a'), 815),\n",
       " (('m', 'e'), 818),\n",
       " (('e', 'i'), 818),\n",
       " (('a', 'v'), 834),\n",
       " (('b', 'r'), 842),\n",
       " (('i', 'r'), 849),\n",
       " (('o', '<E>'), 855),\n",
       " (('z', 'a'), 860),\n",
       " (('e', 's'), 861),\n",
       " (('r', 'o'), 869),\n",
       " (('<S>', 'h'), 874),\n",
       " (('s', 'e'), 884),\n",
       " (('k', 'e'), 895),\n",
       " (('v', 'i'), 911),\n",
       " (('<S>', 'z'), 929),\n",
       " (('t', 'a'), 1027),\n",
       " (('a', 'd'), 1042),\n",
       " (('o', 'r'), 1059),\n",
       " (('e', 'y'), 1070),\n",
       " (('y', 'l'), 1104),\n",
       " (('a', 's'), 1118),\n",
       " (('<S>', 'n'), 1146),\n",
       " (('s', '<E>'), 1169),\n",
       " (('s', 'a'), 1201),\n",
       " (('m', 'i'), 1256),\n",
       " (('e', 'e'), 1271),\n",
       " (('d', 'e'), 1283),\n",
       " (('s', 'h'), 1285),\n",
       " (('d', 'a'), 1303),\n",
       " (('<S>', 'b'), 1306),\n",
       " (('<S>', 't'), 1308),\n",
       " (('l', '<E>'), 1314),\n",
       " (('i', 's'), 1316),\n",
       " (('l', 'l'), 1345),\n",
       " (('i', 'l'), 1345),\n",
       " (('n', 'e'), 1359),\n",
       " (('r', '<E>'), 1377),\n",
       " (('j', 'a'), 1473),\n",
       " (('<S>', 'e'), 1531),\n",
       " (('<S>', 'c'), 1542),\n",
       " (('<S>', 'l'), 1572),\n",
       " (('l', 'y'), 1588),\n",
       " (('a', 'm'), 1634),\n",
       " (('<S>', 'r'), 1639),\n",
       " (('a', 'i'), 1650),\n",
       " (('i', 'e'), 1653),\n",
       " (('<S>', 'd'), 1690),\n",
       " (('r', 'e'), 1697),\n",
       " (('n', 'i'), 1725),\n",
       " (('k', 'a'), 1731),\n",
       " (('y', 'n'), 1826),\n",
       " (('n', 'n'), 1906),\n",
       " (('e', 'r'), 1958),\n",
       " (('y', '<E>'), 2007),\n",
       " (('a', 'y'), 2050),\n",
       " (('<S>', 's'), 2055),\n",
       " (('i', 'n'), 2126),\n",
       " (('y', 'a'), 2143),\n",
       " (('h', 'a'), 2244),\n",
       " (('a', 'h'), 2332),\n",
       " (('r', 'a'), 2356),\n",
       " (('h', '<E>'), 2409),\n",
       " (('o', 'n'), 2411),\n",
       " (('<S>', 'j'), 2422),\n",
       " (('i', 'a'), 2445),\n",
       " (('l', 'i'), 2480),\n",
       " (('i', '<E>'), 2489),\n",
       " (('a', 'l'), 2528),\n",
       " (('<S>', 'm'), 2538),\n",
       " (('m', 'a'), 2590),\n",
       " (('l', 'a'), 2623),\n",
       " (('e', 'n'), 2675),\n",
       " (('l', 'e'), 2921),\n",
       " (('<S>', 'k'), 2963),\n",
       " (('n', 'a'), 2977),\n",
       " (('r', 'i'), 3033),\n",
       " (('e', 'l'), 3248),\n",
       " (('a', 'r'), 3264),\n",
       " (('e', '<E>'), 3983),\n",
       " (('<S>', 'a'), 4410),\n",
       " (('a', 'n'), 5438),\n",
       " (('a', '<E>'), 6640),\n",
       " (('n', '<E>'), 6763)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(b.items(), key = lambda kv: kv[1]) # sort the dict based on count value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6dfd55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('n', '<E>'), 6763),\n",
       " (('a', '<E>'), 6640),\n",
       " (('a', 'n'), 5438),\n",
       " (('<S>', 'a'), 4410),\n",
       " (('e', '<E>'), 3983),\n",
       " (('a', 'r'), 3264),\n",
       " (('e', 'l'), 3248),\n",
       " (('r', 'i'), 3033),\n",
       " (('n', 'a'), 2977),\n",
       " (('<S>', 'k'), 2963),\n",
       " (('l', 'e'), 2921),\n",
       " (('e', 'n'), 2675),\n",
       " (('l', 'a'), 2623),\n",
       " (('m', 'a'), 2590),\n",
       " (('<S>', 'm'), 2538),\n",
       " (('a', 'l'), 2528),\n",
       " (('i', '<E>'), 2489),\n",
       " (('l', 'i'), 2480),\n",
       " (('i', 'a'), 2445),\n",
       " (('<S>', 'j'), 2422),\n",
       " (('o', 'n'), 2411),\n",
       " (('h', '<E>'), 2409),\n",
       " (('r', 'a'), 2356),\n",
       " (('a', 'h'), 2332),\n",
       " (('h', 'a'), 2244),\n",
       " (('y', 'a'), 2143),\n",
       " (('i', 'n'), 2126),\n",
       " (('<S>', 's'), 2055),\n",
       " (('a', 'y'), 2050),\n",
       " (('y', '<E>'), 2007),\n",
       " (('e', 'r'), 1958),\n",
       " (('n', 'n'), 1906),\n",
       " (('y', 'n'), 1826),\n",
       " (('k', 'a'), 1731),\n",
       " (('n', 'i'), 1725),\n",
       " (('r', 'e'), 1697),\n",
       " (('<S>', 'd'), 1690),\n",
       " (('i', 'e'), 1653),\n",
       " (('a', 'i'), 1650),\n",
       " (('<S>', 'r'), 1639),\n",
       " (('a', 'm'), 1634),\n",
       " (('l', 'y'), 1588),\n",
       " (('<S>', 'l'), 1572),\n",
       " (('<S>', 'c'), 1542),\n",
       " (('<S>', 'e'), 1531),\n",
       " (('j', 'a'), 1473),\n",
       " (('r', '<E>'), 1377),\n",
       " (('n', 'e'), 1359),\n",
       " (('l', 'l'), 1345),\n",
       " (('i', 'l'), 1345),\n",
       " (('i', 's'), 1316),\n",
       " (('l', '<E>'), 1314),\n",
       " (('<S>', 't'), 1308),\n",
       " (('<S>', 'b'), 1306),\n",
       " (('d', 'a'), 1303),\n",
       " (('s', 'h'), 1285),\n",
       " (('d', 'e'), 1283),\n",
       " (('e', 'e'), 1271),\n",
       " (('m', 'i'), 1256),\n",
       " (('s', 'a'), 1201),\n",
       " (('s', '<E>'), 1169),\n",
       " (('<S>', 'n'), 1146),\n",
       " (('a', 's'), 1118),\n",
       " (('y', 'l'), 1104),\n",
       " (('e', 'y'), 1070),\n",
       " (('o', 'r'), 1059),\n",
       " (('a', 'd'), 1042),\n",
       " (('t', 'a'), 1027),\n",
       " (('<S>', 'z'), 929),\n",
       " (('v', 'i'), 911),\n",
       " (('k', 'e'), 895),\n",
       " (('s', 'e'), 884),\n",
       " (('<S>', 'h'), 874),\n",
       " (('r', 'o'), 869),\n",
       " (('e', 's'), 861),\n",
       " (('z', 'a'), 860),\n",
       " (('o', '<E>'), 855),\n",
       " (('i', 'r'), 849),\n",
       " (('b', 'r'), 842),\n",
       " (('a', 'v'), 834),\n",
       " (('m', 'e'), 818),\n",
       " (('e', 'i'), 818),\n",
       " (('c', 'a'), 815),\n",
       " (('i', 'y'), 779),\n",
       " (('r', 'y'), 773),\n",
       " (('e', 'm'), 769),\n",
       " (('s', 't'), 765),\n",
       " (('h', 'i'), 729),\n",
       " (('t', 'e'), 716),\n",
       " (('n', 'd'), 704),\n",
       " (('l', 'o'), 692),\n",
       " (('a', 'e'), 692),\n",
       " (('a', 't'), 687),\n",
       " (('s', 'i'), 684),\n",
       " (('e', 'a'), 679),\n",
       " (('d', 'i'), 674),\n",
       " (('h', 'e'), 674),\n",
       " (('<S>', 'g'), 669),\n",
       " (('t', 'o'), 667),\n",
       " (('c', 'h'), 664),\n",
       " (('b', 'e'), 655),\n",
       " (('t', 'h'), 647),\n",
       " (('v', 'a'), 642),\n",
       " (('o', 'l'), 619),\n",
       " (('<S>', 'i'), 591),\n",
       " (('i', 'o'), 588),\n",
       " (('e', 't'), 580),\n",
       " (('v', 'e'), 568),\n",
       " (('a', 'k'), 568),\n",
       " (('a', 'a'), 556),\n",
       " (('c', 'e'), 551),\n",
       " (('a', 'b'), 541),\n",
       " (('i', 't'), 541),\n",
       " (('<S>', 'y'), 535),\n",
       " (('t', 'i'), 532),\n",
       " (('s', 'o'), 531),\n",
       " (('m', '<E>'), 516),\n",
       " (('d', '<E>'), 516),\n",
       " (('<S>', 'p'), 515),\n",
       " (('i', 'c'), 509),\n",
       " (('k', 'i'), 509),\n",
       " (('o', 's'), 504),\n",
       " (('n', 'o'), 496),\n",
       " (('t', '<E>'), 483),\n",
       " (('j', 'o'), 479),\n",
       " (('u', 's'), 474),\n",
       " (('a', 'c'), 470),\n",
       " (('n', 'y'), 465),\n",
       " (('e', 'v'), 463),\n",
       " (('s', 's'), 461),\n",
       " (('m', 'o'), 452),\n",
       " (('i', 'k'), 445),\n",
       " (('n', 't'), 443),\n",
       " (('i', 'd'), 440),\n",
       " (('j', 'e'), 440),\n",
       " (('a', 'z'), 435),\n",
       " (('i', 'g'), 428),\n",
       " (('i', 'm'), 427),\n",
       " (('r', 'r'), 425),\n",
       " (('d', 'r'), 424),\n",
       " (('<S>', 'f'), 417),\n",
       " (('u', 'r'), 414),\n",
       " (('r', 'l'), 413),\n",
       " (('y', 's'), 401),\n",
       " (('<S>', 'o'), 394),\n",
       " (('e', 'd'), 384),\n",
       " (('a', 'u'), 381),\n",
       " (('c', 'o'), 380),\n",
       " (('k', 'y'), 379),\n",
       " (('d', 'o'), 378),\n",
       " (('<S>', 'v'), 376),\n",
       " (('t', 't'), 374),\n",
       " (('z', 'e'), 373),\n",
       " (('z', 'i'), 364),\n",
       " (('k', '<E>'), 363),\n",
       " (('g', 'h'), 360),\n",
       " (('t', 'r'), 352),\n",
       " (('k', 'o'), 344),\n",
       " (('t', 'y'), 341),\n",
       " (('g', 'e'), 334),\n",
       " (('g', 'a'), 330),\n",
       " (('l', 'u'), 324),\n",
       " (('b', 'a'), 321),\n",
       " (('d', 'y'), 317),\n",
       " (('c', 'k'), 316),\n",
       " (('<S>', 'w'), 307),\n",
       " (('k', 'h'), 307),\n",
       " (('u', 'l'), 301),\n",
       " (('y', 'e'), 301),\n",
       " (('y', 'r'), 291),\n",
       " (('m', 'y'), 287),\n",
       " (('h', 'o'), 287),\n",
       " (('w', 'a'), 280),\n",
       " (('s', 'l'), 279),\n",
       " (('n', 's'), 278),\n",
       " (('i', 'z'), 277),\n",
       " (('u', 'n'), 275),\n",
       " (('o', 'u'), 275),\n",
       " (('n', 'g'), 273),\n",
       " (('y', 'd'), 272),\n",
       " (('c', 'i'), 271),\n",
       " (('y', 'o'), 271),\n",
       " (('i', 'v'), 269),\n",
       " (('e', 'o'), 269),\n",
       " (('o', 'm'), 261),\n",
       " (('r', 'u'), 252),\n",
       " (('f', 'a'), 242),\n",
       " (('b', 'i'), 217),\n",
       " (('s', 'y'), 215),\n",
       " (('n', 'c'), 213),\n",
       " (('h', 'y'), 213),\n",
       " (('p', 'a'), 209),\n",
       " (('r', 't'), 208),\n",
       " (('q', 'u'), 206),\n",
       " (('p', 'h'), 204),\n",
       " (('h', 'r'), 204),\n",
       " (('j', 'u'), 202),\n",
       " (('g', 'r'), 201),\n",
       " (('p', 'e'), 197),\n",
       " (('n', 'l'), 195),\n",
       " (('y', 'i'), 192),\n",
       " (('g', 'i'), 190),\n",
       " (('o', 'd'), 190),\n",
       " (('r', 's'), 190),\n",
       " (('r', 'd'), 187),\n",
       " (('h', 'l'), 185),\n",
       " (('s', 'u'), 185),\n",
       " (('a', 'x'), 182),\n",
       " (('e', 'z'), 181),\n",
       " (('e', 'k'), 178),\n",
       " (('o', 'v'), 176),\n",
       " (('a', 'j'), 175),\n",
       " (('o', 'h'), 171),\n",
       " (('u', 'e'), 169),\n",
       " (('m', 'm'), 168),\n",
       " (('a', 'g'), 168),\n",
       " (('h', 'u'), 166),\n",
       " (('x', '<E>'), 164),\n",
       " (('u', 'a'), 163),\n",
       " (('r', 'm'), 162),\n",
       " (('a', 'w'), 161),\n",
       " (('f', 'i'), 160),\n",
       " (('z', '<E>'), 160),\n",
       " (('u', '<E>'), 155),\n",
       " (('u', 'm'), 154),\n",
       " (('e', 'c'), 153),\n",
       " (('v', 'o'), 153),\n",
       " (('e', 'h'), 152),\n",
       " (('p', 'r'), 151),\n",
       " (('d', 'd'), 149),\n",
       " (('o', 'a'), 149),\n",
       " (('w', 'e'), 149),\n",
       " (('w', 'i'), 148),\n",
       " (('y', 'm'), 148),\n",
       " (('z', 'y'), 147),\n",
       " (('n', 'z'), 145),\n",
       " (('y', 'u'), 141),\n",
       " (('r', 'n'), 140),\n",
       " (('o', 'b'), 140),\n",
       " (('k', 'l'), 139),\n",
       " (('m', 'u'), 139),\n",
       " (('l', 'd'), 138),\n",
       " (('h', 'n'), 138),\n",
       " (('u', 'd'), 136),\n",
       " (('<S>', 'x'), 134),\n",
       " (('t', 'l'), 134),\n",
       " (('a', 'f'), 134),\n",
       " (('o', 'e'), 132),\n",
       " (('e', 'x'), 132),\n",
       " (('e', 'g'), 125),\n",
       " (('f', 'e'), 123),\n",
       " (('z', 'l'), 123),\n",
       " (('u', 'i'), 121),\n",
       " (('v', 'y'), 121),\n",
       " (('e', 'b'), 121),\n",
       " (('r', 'h'), 121),\n",
       " (('j', 'i'), 119),\n",
       " (('o', 't'), 118),\n",
       " (('d', 'h'), 118),\n",
       " (('h', 'm'), 117),\n",
       " (('c', 'l'), 116),\n",
       " (('o', 'o'), 115),\n",
       " (('y', 'c'), 115),\n",
       " (('o', 'w'), 114),\n",
       " (('o', 'c'), 114),\n",
       " (('f', 'r'), 114),\n",
       " (('b', '<E>'), 114),\n",
       " (('m', 'b'), 112),\n",
       " (('z', 'o'), 110),\n",
       " (('i', 'b'), 110),\n",
       " (('i', 'u'), 109),\n",
       " (('k', 'r'), 109),\n",
       " (('g', '<E>'), 108),\n",
       " (('y', 'v'), 106),\n",
       " (('t', 'z'), 105),\n",
       " (('b', 'o'), 105),\n",
       " (('c', 'y'), 104),\n",
       " (('y', 't'), 104),\n",
       " (('u', 'b'), 103),\n",
       " (('u', 'c'), 103),\n",
       " (('x', 'a'), 103),\n",
       " (('b', 'l'), 103),\n",
       " (('o', 'y'), 103),\n",
       " (('x', 'i'), 102),\n",
       " (('i', 'f'), 101),\n",
       " (('r', 'c'), 99),\n",
       " (('c', '<E>'), 97),\n",
       " (('m', 'r'), 97),\n",
       " (('n', 'u'), 96),\n",
       " (('o', 'p'), 95),\n",
       " (('i', 'h'), 95),\n",
       " (('k', 's'), 95),\n",
       " (('l', 's'), 94),\n",
       " (('u', 'k'), 93),\n",
       " (('<S>', 'q'), 92),\n",
       " (('d', 'u'), 92),\n",
       " (('s', 'm'), 90),\n",
       " (('r', 'k'), 90),\n",
       " (('i', 'x'), 89),\n",
       " (('v', '<E>'), 88),\n",
       " (('y', 'k'), 86),\n",
       " (('u', 'w'), 86),\n",
       " (('g', 'u'), 85),\n",
       " (('b', 'y'), 83),\n",
       " (('e', 'p'), 83),\n",
       " (('g', 'o'), 83),\n",
       " (('s', 'k'), 82),\n",
       " (('u', 't'), 82),\n",
       " (('a', 'p'), 82),\n",
       " (('e', 'f'), 82),\n",
       " (('i', 'i'), 82),\n",
       " (('r', 'v'), 80),\n",
       " (('f', '<E>'), 80),\n",
       " (('t', 'u'), 78),\n",
       " (('y', 'z'), 78),\n",
       " (('<S>', 'u'), 78),\n",
       " (('l', 't'), 77),\n",
       " (('r', 'g'), 76),\n",
       " (('c', 'r'), 76),\n",
       " (('i', 'j'), 76),\n",
       " (('w', 'y'), 73),\n",
       " (('z', 'u'), 73),\n",
       " (('l', 'v'), 72),\n",
       " (('h', 't'), 71),\n",
       " (('j', '<E>'), 71),\n",
       " (('x', 't'), 70),\n",
       " (('o', 'i'), 69),\n",
       " (('e', 'u'), 69),\n",
       " (('o', 'k'), 68),\n",
       " (('b', 'd'), 65),\n",
       " (('a', 'o'), 63),\n",
       " (('p', 'i'), 61),\n",
       " (('s', 'c'), 60),\n",
       " (('d', 'l'), 60),\n",
       " (('l', 'm'), 60),\n",
       " (('a', 'q'), 60),\n",
       " (('f', 'o'), 60),\n",
       " (('p', 'o'), 59),\n",
       " (('n', 'k'), 58),\n",
       " (('w', 'n'), 58),\n",
       " (('u', 'h'), 58),\n",
       " (('e', 'j'), 55),\n",
       " (('n', 'v'), 55),\n",
       " (('s', 'r'), 55),\n",
       " (('o', 'z'), 54),\n",
       " (('i', 'p'), 53),\n",
       " (('l', 'b'), 52),\n",
       " (('i', 'q'), 52),\n",
       " (('w', '<E>'), 51),\n",
       " (('m', 'c'), 51),\n",
       " (('s', 'p'), 51),\n",
       " (('e', 'w'), 50),\n",
       " (('k', 'u'), 50),\n",
       " (('v', 'r'), 48),\n",
       " (('u', 'g'), 47),\n",
       " (('o', 'x'), 45),\n",
       " (('u', 'z'), 45),\n",
       " (('z', 'z'), 45),\n",
       " (('j', 'h'), 45),\n",
       " (('b', 'u'), 45),\n",
       " (('o', 'g'), 44),\n",
       " (('n', 'r'), 44),\n",
       " (('f', 'f'), 44),\n",
       " (('n', 'j'), 44),\n",
       " (('z', 'h'), 43),\n",
       " (('c', 'c'), 42),\n",
       " (('r', 'b'), 41),\n",
       " (('x', 'o'), 41),\n",
       " (('b', 'h'), 41),\n",
       " (('p', 'p'), 39),\n",
       " (('x', 'l'), 39),\n",
       " (('h', 'v'), 39),\n",
       " (('b', 'b'), 38),\n",
       " (('m', 'p'), 38),\n",
       " (('x', 'x'), 38),\n",
       " (('u', 'v'), 37),\n",
       " (('x', 'e'), 36),\n",
       " (('w', 'o'), 36),\n",
       " (('c', 't'), 35),\n",
       " (('z', 'm'), 35),\n",
       " (('t', 's'), 35),\n",
       " (('m', 's'), 35),\n",
       " (('c', 'u'), 35),\n",
       " (('o', 'f'), 34),\n",
       " (('u', 'x'), 34),\n",
       " (('k', 'w'), 34),\n",
       " (('p', '<E>'), 33),\n",
       " (('g', 'l'), 32),\n",
       " (('z', 'r'), 32),\n",
       " (('d', 'n'), 31),\n",
       " (('g', 't'), 31),\n",
       " (('g', 'y'), 31),\n",
       " (('h', 's'), 31),\n",
       " (('x', 's'), 31),\n",
       " (('g', 's'), 30),\n",
       " (('x', 'y'), 30),\n",
       " (('y', 'g'), 30),\n",
       " (('d', 'm'), 30),\n",
       " (('d', 's'), 29),\n",
       " (('h', 'k'), 29),\n",
       " (('y', 'x'), 28),\n",
       " (('q', '<E>'), 28),\n",
       " (('g', 'n'), 27),\n",
       " (('y', 'b'), 27),\n",
       " (('g', 'w'), 26),\n",
       " (('n', 'h'), 26),\n",
       " (('k', 'n'), 26),\n",
       " (('g', 'g'), 25),\n",
       " (('d', 'g'), 25),\n",
       " (('l', 'c'), 25),\n",
       " (('r', 'j'), 25),\n",
       " (('w', 'u'), 25),\n",
       " (('l', 'k'), 24),\n",
       " (('m', 'd'), 24),\n",
       " (('s', 'w'), 24),\n",
       " (('s', 'n'), 24),\n",
       " (('h', 'd'), 24),\n",
       " (('w', 'h'), 23),\n",
       " (('y', 'j'), 23),\n",
       " (('y', 'y'), 23),\n",
       " (('r', 'z'), 23),\n",
       " (('d', 'w'), 23),\n",
       " (('w', 'r'), 22),\n",
       " (('t', 'n'), 22),\n",
       " (('l', 'f'), 22),\n",
       " (('y', 'h'), 22),\n",
       " (('r', 'w'), 21),\n",
       " (('s', 'b'), 21),\n",
       " (('m', 'n'), 20),\n",
       " (('f', 'l'), 20),\n",
       " (('w', 's'), 20),\n",
       " (('k', 'k'), 20),\n",
       " (('h', 'z'), 20),\n",
       " (('g', 'd'), 19),\n",
       " (('l', 'h'), 19),\n",
       " (('n', 'm'), 19),\n",
       " (('x', 'z'), 19),\n",
       " (('u', 'f'), 19),\n",
       " (('f', 't'), 18),\n",
       " (('l', 'r'), 18),\n",
       " (('p', 't'), 17),\n",
       " (('t', 'c'), 17),\n",
       " (('k', 't'), 17),\n",
       " (('d', 'v'), 17),\n",
       " (('u', 'p'), 16),\n",
       " (('p', 'l'), 16),\n",
       " (('l', 'w'), 16),\n",
       " (('p', 's'), 16),\n",
       " (('o', 'j'), 16),\n",
       " (('r', 'q'), 16),\n",
       " (('y', 'p'), 15),\n",
       " (('l', 'p'), 15),\n",
       " (('t', 'v'), 15),\n",
       " (('r', 'p'), 14),\n",
       " (('l', 'n'), 14),\n",
       " (('e', 'q'), 14),\n",
       " (('f', 'y'), 14),\n",
       " (('s', 'v'), 14),\n",
       " (('u', 'j'), 14),\n",
       " (('v', 'l'), 14),\n",
       " (('q', 'a'), 13),\n",
       " (('u', 'y'), 13),\n",
       " (('q', 'i'), 13),\n",
       " (('w', 'l'), 13),\n",
       " (('p', 'y'), 12),\n",
       " (('y', 'f'), 12),\n",
       " (('c', 'q'), 11),\n",
       " (('j', 'r'), 11),\n",
       " (('n', 'w'), 11),\n",
       " (('n', 'f'), 11),\n",
       " (('t', 'w'), 11),\n",
       " (('m', 'z'), 11),\n",
       " (('u', 'o'), 10),\n",
       " (('f', 'u'), 10),\n",
       " (('l', 'z'), 10),\n",
       " (('h', 'w'), 10),\n",
       " (('u', 'q'), 10),\n",
       " (('j', 'y'), 10),\n",
       " (('s', 'z'), 10),\n",
       " (('s', 'd'), 9),\n",
       " (('j', 'l'), 9),\n",
       " (('d', 'j'), 9),\n",
       " (('k', 'm'), 9),\n",
       " (('r', 'f'), 9),\n",
       " (('h', 'j'), 9),\n",
       " (('v', 'n'), 8),\n",
       " (('n', 'b'), 8),\n",
       " (('i', 'w'), 8),\n",
       " (('h', 'b'), 8),\n",
       " (('b', 's'), 8),\n",
       " (('w', 't'), 8),\n",
       " (('w', 'd'), 8),\n",
       " (('v', 'v'), 7),\n",
       " (('v', 'u'), 7),\n",
       " (('j', 's'), 7),\n",
       " (('m', 'j'), 7),\n",
       " (('f', 's'), 6),\n",
       " (('l', 'g'), 6),\n",
       " (('l', 'j'), 6),\n",
       " (('j', 'w'), 6),\n",
       " (('n', 'x'), 6),\n",
       " (('y', 'q'), 6),\n",
       " (('w', 'k'), 6),\n",
       " (('g', 'm'), 6),\n",
       " (('x', 'u'), 5),\n",
       " (('m', 'h'), 5),\n",
       " (('m', 'l'), 5),\n",
       " (('j', 'm'), 5),\n",
       " (('c', 's'), 5),\n",
       " (('j', 'v'), 5),\n",
       " (('n', 'p'), 5),\n",
       " (('d', 'f'), 5),\n",
       " (('x', 'd'), 5),\n",
       " (('z', 'b'), 4),\n",
       " (('f', 'n'), 4),\n",
       " (('x', 'c'), 4),\n",
       " (('m', 't'), 4),\n",
       " (('t', 'm'), 4),\n",
       " (('z', 'n'), 4),\n",
       " (('z', 't'), 4),\n",
       " (('p', 'u'), 4),\n",
       " (('c', 'z'), 4),\n",
       " (('b', 'n'), 4),\n",
       " (('z', 's'), 4),\n",
       " (('f', 'w'), 4),\n",
       " (('d', 't'), 4),\n",
       " (('j', 'd'), 4),\n",
       " (('j', 'c'), 4),\n",
       " (('y', 'w'), 4),\n",
       " (('v', 'k'), 3),\n",
       " (('x', 'w'), 3),\n",
       " (('t', 'j'), 3),\n",
       " (('c', 'j'), 3),\n",
       " (('q', 'w'), 3),\n",
       " (('g', 'b'), 3),\n",
       " (('o', 'q'), 3),\n",
       " (('r', 'x'), 3),\n",
       " (('d', 'c'), 3),\n",
       " (('g', 'j'), 3),\n",
       " (('x', 'f'), 3),\n",
       " (('z', 'w'), 3),\n",
       " (('d', 'k'), 3),\n",
       " (('u', 'u'), 3),\n",
       " (('m', 'v'), 3),\n",
       " (('c', 'x'), 3),\n",
       " (('l', 'q'), 3),\n",
       " (('p', 'b'), 2),\n",
       " (('t', 'g'), 2),\n",
       " (('q', 's'), 2),\n",
       " (('t', 'x'), 2),\n",
       " (('f', 'k'), 2),\n",
       " (('b', 't'), 2),\n",
       " (('j', 'n'), 2),\n",
       " (('k', 'c'), 2),\n",
       " (('z', 'k'), 2),\n",
       " (('s', 'j'), 2),\n",
       " (('s', 'f'), 2),\n",
       " (('z', 'j'), 2),\n",
       " (('n', 'q'), 2),\n",
       " (('f', 'z'), 2),\n",
       " (('h', 'g'), 2),\n",
       " (('w', 'w'), 2),\n",
       " (('k', 'j'), 2),\n",
       " (('j', 'k'), 2),\n",
       " (('w', 'm'), 2),\n",
       " (('z', 'c'), 2),\n",
       " (('z', 'v'), 2),\n",
       " (('w', 'f'), 2),\n",
       " (('q', 'm'), 2),\n",
       " (('k', 'z'), 2),\n",
       " (('j', 'j'), 2),\n",
       " (('z', 'p'), 2),\n",
       " (('j', 't'), 2),\n",
       " (('k', 'b'), 2),\n",
       " (('m', 'w'), 2),\n",
       " (('h', 'f'), 2),\n",
       " (('c', 'g'), 2),\n",
       " (('t', 'f'), 2),\n",
       " (('h', 'c'), 2),\n",
       " (('q', 'o'), 2),\n",
       " (('k', 'd'), 2),\n",
       " (('k', 'v'), 2),\n",
       " (('s', 'g'), 2),\n",
       " (('z', 'd'), 2),\n",
       " (('q', 'r'), 1),\n",
       " (('d', 'z'), 1),\n",
       " (('p', 'j'), 1),\n",
       " (('q', 'l'), 1),\n",
       " (('p', 'f'), 1),\n",
       " (('q', 'e'), 1),\n",
       " (('b', 'c'), 1),\n",
       " (('c', 'd'), 1),\n",
       " (('m', 'f'), 1),\n",
       " (('p', 'n'), 1),\n",
       " (('w', 'b'), 1),\n",
       " (('p', 'c'), 1),\n",
       " (('h', 'p'), 1),\n",
       " (('f', 'h'), 1),\n",
       " (('b', 'j'), 1),\n",
       " (('f', 'g'), 1),\n",
       " (('z', 'g'), 1),\n",
       " (('c', 'p'), 1),\n",
       " (('p', 'k'), 1),\n",
       " (('p', 'm'), 1),\n",
       " (('x', 'n'), 1),\n",
       " (('s', 'q'), 1),\n",
       " (('k', 'f'), 1),\n",
       " (('m', 'k'), 1),\n",
       " (('x', 'h'), 1),\n",
       " (('g', 'f'), 1),\n",
       " (('v', 'b'), 1),\n",
       " (('j', 'p'), 1),\n",
       " (('g', 'z'), 1),\n",
       " (('v', 'd'), 1),\n",
       " (('d', 'b'), 1),\n",
       " (('v', 'h'), 1),\n",
       " (('h', 'h'), 1),\n",
       " (('g', 'v'), 1),\n",
       " (('d', 'q'), 1),\n",
       " (('x', 'b'), 1),\n",
       " (('w', 'z'), 1),\n",
       " (('h', 'q'), 1),\n",
       " (('j', 'b'), 1),\n",
       " (('x', 'm'), 1),\n",
       " (('w', 'g'), 1),\n",
       " (('t', 'b'), 1),\n",
       " (('z', 'x'), 1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(b.items(), key = lambda kv: -kv[1]) # sort the dict based on count value; descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b9363d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we need to keep above info into significantly convinient format insted of dictionary\n",
    "#store the info into 2D format\n",
    "#1st char in row & 2nd char in column\n",
    "#this will tell how freq 2nd follow 1st char based on its intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a43b4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c2600d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cae587d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb1fc2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0af3657",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros((3, 5), dtype=torch.int32) #as count is integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16168a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "976b5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1,3] = 1 #indexing the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7c272d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ac867d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#26 chars, 2 special chars...therefor 28 * 28 array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c34ae767",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((28, 28), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce187ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'b': 1,\n",
       " 'c': 2,\n",
       " 'd': 3,\n",
       " 'e': 4,\n",
       " 'f': 5,\n",
       " 'g': 6,\n",
       " 'h': 7,\n",
       " 'i': 8,\n",
       " 'j': 9,\n",
       " 'k': 10,\n",
       " 'l': 11,\n",
       " 'm': 12,\n",
       " 'n': 13,\n",
       " 'o': 14,\n",
       " 'p': 15,\n",
       " 'q': 16,\n",
       " 'r': 17,\n",
       " 's': 18,\n",
       " 't': 19,\n",
       " 'u': 20,\n",
       " 'v': 21,\n",
       " 'w': 22,\n",
       " 'x': 23,\n",
       " 'y': 24,\n",
       " 'z': 25}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = sorted(list(set(''.join(words)))) #get all distinct char for lookup\n",
    "stoi ={s:i for i,s in enumerate(char)}\n",
    "stoi #notice we index characters using index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b07f0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi['<S>'] = 26 #adding special chars\n",
    "stoi['<E>'] = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "687c8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1] #get the index of character1\n",
    "        ix2 = stoi[ch2] #get the index of character2\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef78239c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568, 2528,\n",
       "         1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,  182,\n",
       "         2050,  435,    0, 6640],\n",
       "        [ 321,   38,    1,   65,  655,    0,    0,   41,  217,    1,    0,  103,\n",
       "            0,    4,  105,    0,    0,  842,    8,    2,   45,    0,    0,    0,\n",
       "           83,    0,    0,  114],\n",
       "        [ 815,    0,   42,    1,  551,    0,    2,  664,  271,    3,  316,  116,\n",
       "            0,    0,  380,    1,   11,   76,    5,   35,   35,    0,    0,    3,\n",
       "          104,    4,    0,   97],\n",
       "        [1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9,    3,   60,\n",
       "           30,   31,  378,    0,    1,  424,   29,    4,   92,   17,   23,    0,\n",
       "          317,    1,    0,  516],\n",
       "        [ 679,  121,  153,  384, 1271,   82,  125,  152,  818,   55,  178, 3248,\n",
       "          769, 2675,  269,   83,   14, 1958,  861,  580,   69,  463,   50,  132,\n",
       "         1070,  181,    0, 3983],\n",
       "        [ 242,    0,    0,    0,  123,   44,    1,    1,  160,    0,    2,   20,\n",
       "            0,    4,   60,    0,    0,  114,    6,   18,   10,    0,    4,    0,\n",
       "           14,    2,    0,   80],\n",
       "        [ 330,    3,    0,   19,  334,    1,   25,  360,  190,    3,    0,   32,\n",
       "            6,   27,   83,    0,    0,  201,   30,   31,   85,    1,   26,    0,\n",
       "           31,    1,    0,  108],\n",
       "        [2244,    8,    2,   24,  674,    2,    2,    1,  729,    9,   29,  185,\n",
       "          117,  138,  287,    1,    1,  204,   31,   71,  166,   39,   10,    0,\n",
       "          213,   20,    0, 2409],\n",
       "        [2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76,  445, 1345,\n",
       "          427, 2126,  588,   53,   52,  849, 1316,  541,  109,  269,    8,   89,\n",
       "          779,  277,    0, 2489],\n",
       "        [1473,    1,    4,    4,  440,    0,    0,   45,  119,    2,    2,    9,\n",
       "            5,    2,  479,    1,    0,   11,    7,    2,  202,    5,    6,    0,\n",
       "           10,    0,    0,   71],\n",
       "        [1731,    2,    2,    2,  895,    1,    0,  307,  509,    2,   20,  139,\n",
       "            9,   26,  344,    0,    0,  109,   95,   17,   50,    2,   34,    0,\n",
       "          379,    2,    0,  363],\n",
       "        [2623,   52,   25,  138, 2921,   22,    6,   19, 2480,    6,   24, 1345,\n",
       "           60,   14,  692,   15,    3,   18,   94,   77,  324,   72,   16,    0,\n",
       "         1588,   10,    0, 1314],\n",
       "        [2590,  112,   51,   24,  818,    1,    0,    5, 1256,    7,    1,    5,\n",
       "          168,   20,  452,   38,    0,   97,   35,    4,  139,    3,    2,    0,\n",
       "          287,   11,    0,  516],\n",
       "        [2977,    8,  213,  704, 1359,   11,  273,   26, 1725,   44,   58,  195,\n",
       "           19, 1906,  496,    5,    2,   44,  278,  443,   96,   55,   11,    6,\n",
       "          465,  145,    0, 6763],\n",
       "        [ 149,  140,  114,  190,  132,   34,   44,  171,   69,   16,   68,  619,\n",
       "          261, 2411,  115,   95,    3, 1059,  504,  118,  275,  176,  114,   45,\n",
       "          103,   54,    0,  855],\n",
       "        [ 209,    2,    1,    0,  197,    1,    0,  204,   61,    1,    1,   16,\n",
       "            1,    1,   59,   39,    0,  151,   16,   17,    4,    0,    0,    0,\n",
       "           12,    0,    0,   33],\n",
       "        [  13,    0,    0,    0,    1,    0,    0,    0,   13,    0,    0,    1,\n",
       "            2,    0,    2,    0,    0,    1,    2,    0,  206,    0,    3,    0,\n",
       "            0,    0,    0,   28],\n",
       "        [2356,   41,   99,  187, 1697,    9,   76,  121, 3033,   25,   90,  413,\n",
       "          162,  140,  869,   14,   16,  425,  190,  208,  252,   80,   21,    3,\n",
       "          773,   23,    0, 1377],\n",
       "        [1201,   21,   60,    9,  884,    2,    2, 1285,  684,    2,   82,  279,\n",
       "           90,   24,  531,   51,    1,   55,  461,  765,  185,   14,   24,    0,\n",
       "          215,   10,    0, 1169],\n",
       "        [1027,    1,   17,    0,  716,    2,    2,  647,  532,    3,    0,  134,\n",
       "            4,   22,  667,    0,    0,  352,   35,  374,   78,   15,   11,    2,\n",
       "          341,  105,    0,  483],\n",
       "        [ 163,  103,  103,  136,  169,   19,   47,   58,  121,   14,   93,  301,\n",
       "          154,  275,   10,   16,   10,  414,  474,   82,    3,   37,   86,   34,\n",
       "           13,   45,    0,  155],\n",
       "        [ 642,    1,    0,    1,  568,    0,    0,    1,  911,    0,    3,   14,\n",
       "            0,    8,  153,    0,    0,   48,    0,    0,    7,    7,    0,    0,\n",
       "          121,    0,    0,   88],\n",
       "        [ 280,    1,    0,    8,  149,    2,    1,   23,  148,    0,    6,   13,\n",
       "            2,   58,   36,    0,    0,   22,   20,    8,   25,    0,    2,    0,\n",
       "           73,    1,    0,   51],\n",
       "        [ 103,    1,    4,    5,   36,    3,    0,    1,  102,    0,    0,   39,\n",
       "            1,    1,   41,    0,    0,    0,   31,   70,    5,    0,    3,   38,\n",
       "           30,   19,    0,  164],\n",
       "        [2143,   27,  115,  272,  301,   12,   30,   22,  192,   23,   86, 1104,\n",
       "          148, 1826,  271,   15,    6,  291,  401,  104,  141,  106,    4,   28,\n",
       "           23,   78,    0, 2007],\n",
       "        [ 860,    4,    2,    2,  373,    0,    1,   43,  364,    2,    2,  123,\n",
       "           35,    4,  110,    2,    0,   32,    4,    4,   73,    2,    3,    1,\n",
       "          147,   45,    0,  160],\n",
       "        [4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963, 1572,\n",
       "         2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,  134,\n",
       "          535,  929,    0,    0],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6577cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2eb8a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b470d94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568, 2528,\n",
       "         1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,  182,\n",
       "         2050,  435,    0, 6640],\n",
       "        [ 321,   38,    1,   65,  655,    0,    0,   41,  217,    1,    0,  103,\n",
       "            0,    4,  105,    0,    0,  842,    8,    2,   45,    0,    0,    0,\n",
       "           83,    0,    0,  114],\n",
       "        [ 815,    0,   42,    1,  551,    0,    2,  664,  271,    3,  316,  116,\n",
       "            0,    0,  380,    1,   11,   76,    5,   35,   35,    0,    0,    3,\n",
       "          104,    4,    0,   97],\n",
       "        [1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9,    3,   60,\n",
       "           30,   31,  378,    0,    1,  424,   29,    4,   92,   17,   23,    0,\n",
       "          317,    1,    0,  516],\n",
       "        [ 679,  121,  153,  384, 1271,   82,  125,  152,  818,   55,  178, 3248,\n",
       "          769, 2675,  269,   83,   14, 1958,  861,  580,   69,  463,   50,  132,\n",
       "         1070,  181,    0, 3983],\n",
       "        [ 242,    0,    0,    0,  123,   44,    1,    1,  160,    0,    2,   20,\n",
       "            0,    4,   60,    0,    0,  114,    6,   18,   10,    0,    4,    0,\n",
       "           14,    2,    0,   80],\n",
       "        [ 330,    3,    0,   19,  334,    1,   25,  360,  190,    3,    0,   32,\n",
       "            6,   27,   83,    0,    0,  201,   30,   31,   85,    1,   26,    0,\n",
       "           31,    1,    0,  108],\n",
       "        [2244,    8,    2,   24,  674,    2,    2,    1,  729,    9,   29,  185,\n",
       "          117,  138,  287,    1,    1,  204,   31,   71,  166,   39,   10,    0,\n",
       "          213,   20,    0, 2409],\n",
       "        [2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76,  445, 1345,\n",
       "          427, 2126,  588,   53,   52,  849, 1316,  541,  109,  269,    8,   89,\n",
       "          779,  277,    0, 2489],\n",
       "        [1473,    1,    4,    4,  440,    0,    0,   45,  119,    2,    2,    9,\n",
       "            5,    2,  479,    1,    0,   11,    7,    2,  202,    5,    6,    0,\n",
       "           10,    0,    0,   71],\n",
       "        [1731,    2,    2,    2,  895,    1,    0,  307,  509,    2,   20,  139,\n",
       "            9,   26,  344,    0,    0,  109,   95,   17,   50,    2,   34,    0,\n",
       "          379,    2,    0,  363],\n",
       "        [2623,   52,   25,  138, 2921,   22,    6,   19, 2480,    6,   24, 1345,\n",
       "           60,   14,  692,   15,    3,   18,   94,   77,  324,   72,   16,    0,\n",
       "         1588,   10,    0, 1314],\n",
       "        [2590,  112,   51,   24,  818,    1,    0,    5, 1256,    7,    1,    5,\n",
       "          168,   20,  452,   38,    0,   97,   35,    4,  139,    3,    2,    0,\n",
       "          287,   11,    0,  516],\n",
       "        [2977,    8,  213,  704, 1359,   11,  273,   26, 1725,   44,   58,  195,\n",
       "           19, 1906,  496,    5,    2,   44,  278,  443,   96,   55,   11,    6,\n",
       "          465,  145,    0, 6763],\n",
       "        [ 149,  140,  114,  190,  132,   34,   44,  171,   69,   16,   68,  619,\n",
       "          261, 2411,  115,   95,    3, 1059,  504,  118,  275,  176,  114,   45,\n",
       "          103,   54,    0,  855],\n",
       "        [ 209,    2,    1,    0,  197,    1,    0,  204,   61,    1,    1,   16,\n",
       "            1,    1,   59,   39,    0,  151,   16,   17,    4,    0,    0,    0,\n",
       "           12,    0,    0,   33],\n",
       "        [  13,    0,    0,    0,    1,    0,    0,    0,   13,    0,    0,    1,\n",
       "            2,    0,    2,    0,    0,    1,    2,    0,  206,    0,    3,    0,\n",
       "            0,    0,    0,   28],\n",
       "        [2356,   41,   99,  187, 1697,    9,   76,  121, 3033,   25,   90,  413,\n",
       "          162,  140,  869,   14,   16,  425,  190,  208,  252,   80,   21,    3,\n",
       "          773,   23,    0, 1377],\n",
       "        [1201,   21,   60,    9,  884,    2,    2, 1285,  684,    2,   82,  279,\n",
       "           90,   24,  531,   51,    1,   55,  461,  765,  185,   14,   24,    0,\n",
       "          215,   10,    0, 1169],\n",
       "        [1027,    1,   17,    0,  716,    2,    2,  647,  532,    3,    0,  134,\n",
       "            4,   22,  667,    0,    0,  352,   35,  374,   78,   15,   11,    2,\n",
       "          341,  105,    0,  483],\n",
       "        [ 163,  103,  103,  136,  169,   19,   47,   58,  121,   14,   93,  301,\n",
       "          154,  275,   10,   16,   10,  414,  474,   82,    3,   37,   86,   34,\n",
       "           13,   45,    0,  155],\n",
       "        [ 642,    1,    0,    1,  568,    0,    0,    1,  911,    0,    3,   14,\n",
       "            0,    8,  153,    0,    0,   48,    0,    0,    7,    7,    0,    0,\n",
       "          121,    0,    0,   88],\n",
       "        [ 280,    1,    0,    8,  149,    2,    1,   23,  148,    0,    6,   13,\n",
       "            2,   58,   36,    0,    0,   22,   20,    8,   25,    0,    2,    0,\n",
       "           73,    1,    0,   51],\n",
       "        [ 103,    1,    4,    5,   36,    3,    0,    1,  102,    0,    0,   39,\n",
       "            1,    1,   41,    0,    0,    0,   31,   70,    5,    0,    3,   38,\n",
       "           30,   19,    0,  164],\n",
       "        [2143,   27,  115,  272,  301,   12,   30,   22,  192,   23,   86, 1104,\n",
       "          148, 1826,  271,   15,    6,  291,  401,  104,  141,  106,    4,   28,\n",
       "           23,   78,    0, 2007],\n",
       "        [ 860,    4,    2,    2,  373,    0,    1,   43,  364,    2,    2,  123,\n",
       "           35,    4,  110,    2,    0,   32,    4,    4,   73,    2,    3,    1,\n",
       "          147,   45,    0,  160],\n",
       "        [4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963, 1572,\n",
       "         2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,  134,\n",
       "          535,  929,    0,    0],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8739acb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos ={i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ac0739",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "for i in range(28):\n",
    "    for j in range(28):\n",
    "        chstr = itos[i] + itos[j]\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "32d78cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kernal failing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b887a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#noticing the graph, we identified <E> will not be a start letter for any words...hence we can optimize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52eca739",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ecfe8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "char = sorted(list(set(''.join(words)))) #get all distinct char for lookup\n",
    "stoi ={s:i+1 for i,s in enumerate(char)}\n",
    "stoi['.'] = 0 #.represents special char\n",
    "itos ={i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7cc7652b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '.': 0}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ebd668dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1] #get the index of character1\n",
    "        ix2 = stoi[ch2] #get the index of character2\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bbc86c",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = itos[i] + itos[j]\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12d52744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "         134,  535,  929], dtype=torch.int32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#N[0, :]#0th row & all column values i.e. 1D Array\n",
    "#or\n",
    "N[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f5198022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N[0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3fd472c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,\n",
       "        0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,\n",
       "        0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we need to convert these values into probabilities\n",
    "p = N[0].float()\n",
    "p = p / p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "94e90edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.sum() #will be summing up to 1 since we normalized the data and converted into proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "afd9969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to sample these data, we use multinomial prob distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cf4e72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "p = torch.rand(3, generator=g)\n",
    "p = p / p.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "503ebb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6064, 0.3033, 0.0903])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4b15ff0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 2, 0, 0, 2, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multinomial(p, num_samples=20, replacement=True, generator=g)\n",
    "#60% will be 0 , 30% will be 1 & 10% will be 2...since probabilities of p is 60, 30 & 10%!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3e583211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,\n",
       "        0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,\n",
       "        0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets do for our bag of words\n",
    "#we need to convert these values into probabilities\n",
    "p = N[0].float()\n",
    "p = p / p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a3b321bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6a5dc588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'j'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fded596e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cony.\n",
      "a.\n",
      "nn.\n",
      "kohin.\n",
      "tolian.\n",
      "juee.\n",
      "ksahnaauranilevias.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        p = N[ix].float()\n",
    "        p = p / p.sum()\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break;\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1c143d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have generated 10 different sequence of words based on bigram probabilities\n",
    "#Seems these names look terrible, because of Bigram performance\n",
    "#can we fine tune  ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "59e886dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f24bb548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juwjdvdipkcqaz.\n",
      "p.\n",
      "cfqywocnzqfjiirltozcogsjgwzvudlhnpauyjbilevhajkdbduinrwibtlzsnjyievyvaftbzffvmumthyfodtumjrpfytszwjhrjagq.\n",
      "coreaysezocfkyjjabdywejfmoifmwyfinwagaasnhsvfihofszxhddgosfmptpagicz.\n",
      "rjpiufmthdt.\n",
      "rkrrsru.\n",
      "iyumuyfy.\n",
      "mjekujcbkhvupwyhvpvhvccragr.\n",
      "wdkhwfdztta.\n",
      "mplyisbxlyhuuiqzavmpocbzthqmimvyqwat.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        #p = N[ix].float()\n",
    "        #p = p / p.sum()\n",
    "        p = torch.ones(27) / 27.0 #kind of uniform distribution making everything equally likely\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break;\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a306030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is something model without training...\n",
    "#but bigram was something better..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "30931821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#currently we sample one row from N and finding its probability\n",
    "#instead we want to create matrix that contains all probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ec02fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = N.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e46519e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[32033.],\n",
       "        [33885.],\n",
       "        [ 2645.],\n",
       "        [ 3532.],\n",
       "        [ 5496.],\n",
       "        [20423.],\n",
       "        [  905.],\n",
       "        [ 1927.],\n",
       "        [ 7616.],\n",
       "        [17701.],\n",
       "        [ 2900.],\n",
       "        [ 5040.],\n",
       "        [13958.],\n",
       "        [ 6642.],\n",
       "        [18327.],\n",
       "        [ 7934.],\n",
       "        [ 1026.],\n",
       "        [  272.],\n",
       "        [12700.],\n",
       "        [ 8106.],\n",
       "        [ 5570.],\n",
       "        [ 3135.],\n",
       "        [ 2573.],\n",
       "        [  929.],\n",
       "        [  697.],\n",
       "        [ 9776.],\n",
       "        [ 2398.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(P.sum(1, keepdim=True)) #aggregating columns at row level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "86b6e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P = P / (P.sum(1, keepdim=True)) #broadcasting is done here.. P is matrix, below sum is single column\n",
    "P /= (P.sum(1, keepdim=True)) #much faster because of inmemory and not creating new variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aa5825bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27 27\n",
    "# 27 1\n",
    "#Broadcasting conditions met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a2bef714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a6d2cb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cony.\n",
      "a.\n",
      "nn.\n",
      "kohin.\n",
      "tolian.\n",
      "juee.\n",
      "ksahnaauranilevias.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        p = P[ix]\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break;\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8f7f8432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting exactly same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a69c5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How quality was the above model results ?\n",
    "#How to get the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1b1f1869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".e: 0.0622\n",
      "em: 0.0253\n",
      "mm: 0.0253\n",
      "ma: 0.0164\n",
      "a.: 0.0000\n",
      ".o: 0.0145\n",
      "ol: 0.0964\n",
      "li: 0.0046\n",
      "iv: 0.0027\n",
      "vi: 0.0046\n",
      "ia: 0.0164\n",
      "a.: 0.0000\n",
      ".a: 0.0164\n",
      "av: 0.0027\n",
      "va: 0.0164\n",
      "a.: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for w in words[:3]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1] #get the index of character1\n",
    "        ix2 = stoi[ch2] #get the index of character2\n",
    "        prob = P[ix2, ix2]\n",
    "        print(f\"{ch1}{ch2}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "72dc0ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.037037037037037035"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total 27 chars ; equaly prob = 1/27 ~ 4%\n",
    "1/27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f03c2810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice many bigram with above 4% which means we learned something useful from that bigram...\n",
    "#if a good model, these prob should be near equal to 1\n",
    "#how can we summarize these prob into a single number to arrive at loss value or that measures quality of this model?\n",
    "#Maximum Likelihood Estimate function\n",
    "#Product of all these probabilities and should be as high as possible\n",
    "#but usually this is very very small no\n",
    "#hence we go for Log Likelihood !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d09663a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".e: 0.0622 -2.7769\n",
      "em: 0.0253 -3.6772\n",
      "mm: 0.0253 -3.6772\n",
      "ma: 0.0164 -4.1100\n",
      "a.: 0.0000 -inf\n",
      ".o: 0.0145 -4.2340\n",
      "ol: 0.0964 -2.3397\n",
      "li: 0.0046 -5.3747\n",
      "iv: 0.0027 -5.9069\n",
      "vi: 0.0046 -5.3747\n",
      "ia: 0.0164 -4.1100\n",
      "a.: 0.0000 -inf\n",
      ".a: 0.0164 -4.1100\n",
      "av: 0.0027 -5.9069\n",
      "va: 0.0164 -4.1100\n",
      "a.: 0.0000 -inf\n"
     ]
    }
   ],
   "source": [
    "for w in words[:3]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1] #get the index of character1\n",
    "        ix2 = stoi[ch2] #get the index of character2\n",
    "        prob = P[ix2, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        print(f\"{ch1}{ch2}: {prob:.4f} {logprob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0c9ae81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log(a*b) = log(a) + log(b)\n",
    "#log will add instead of product. Hence summing of probabilities will be good no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f1b74e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".e: 0.0622 -2.7769\n",
      "em: 0.0253 -3.6772\n",
      "mm: 0.0253 -3.6772\n",
      "ma: 0.0164 -4.1100\n",
      "a.: 0.0000 -inf\n",
      ".o: 0.0145 -4.2340\n",
      "ol: 0.0964 -2.3397\n",
      "li: 0.0046 -5.3747\n",
      "iv: 0.0027 -5.9069\n",
      "vi: 0.0046 -5.3747\n",
      "ia: 0.0164 -4.1100\n",
      "a.: 0.0000 -inf\n",
      ".a: 0.0164 -4.1100\n",
      "av: 0.0027 -5.9069\n",
      "va: 0.0164 -4.1100\n",
      "a.: 0.0000 -inf\n",
      "tensor(-inf)\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "for w in words[:3]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1] #get the index of character1\n",
    "        ix2 = stoi[ch2] #get the index of character2\n",
    "        prob = P[ix2, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        print(f\"{ch1}{ch2}: {prob:.4f} {logprob:.4f}\")\n",
    "print(log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "31570396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#when all probabilities  = 1; Log(1) = 0\n",
    "#when all prob are lower , log value will be more and more lower\n",
    "#But definition of loss function says low is good...\n",
    "#with loglikelihood, it is reverse\n",
    "#hence we go for negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "73c3fe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".e: 0.0622 -2.7769\n",
      "em: 0.0253 -3.6772\n",
      "mm: 0.0253 -3.6772\n",
      "ma: 0.0164 -4.1100\n",
      "a.: 0.0000 -inf\n",
      ".o: 0.0145 -4.2340\n",
      "ol: 0.0964 -2.3397\n",
      "li: 0.0046 -5.3747\n",
      "iv: 0.0027 -5.9069\n",
      "vi: 0.0046 -5.3747\n",
      "ia: 0.0164 -4.1100\n",
      "a.: 0.0000 -inf\n",
      ".a: 0.0164 -4.1100\n",
      "av: 0.0027 -5.9069\n",
      "va: 0.0164 -4.1100\n",
      "a.: 0.0000 -inf\n",
      "Log Likelihood = -inf\n",
      "Negative Log Likelihood = inf\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "for w in words[:3]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1] #get the index of character1\n",
    "        ix2 = stoi[ch2] #get the index of character2\n",
    "        prob = P[ix2, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        print(f\"{ch1}{ch2}: {prob:.4f} {logprob:.4f}\")\n",
    "\n",
    "print(f'Log Likelihood = {log_likelihood}')\n",
    "nll = -log_likelihood\n",
    "print(f'Negative Log Likelihood = {nll}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "121114b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".e: 0.0622 -2.7769\n",
      "em: 0.0253 -3.6772\n",
      "mm: 0.0253 -3.6772\n",
      "ma: 0.0164 -4.1100\n",
      "a.: 0.0000 -inf\n",
      ".o: 0.0145 -4.2340\n",
      "ol: 0.0964 -2.3397\n",
      "li: 0.0046 -5.3747\n",
      "iv: 0.0027 -5.9069\n",
      "vi: 0.0046 -5.3747\n",
      "ia: 0.0164 -4.1100\n",
      "a.: 0.0000 -inf\n",
      ".a: 0.0164 -4.1100\n",
      "av: 0.0027 -5.9069\n",
      "va: 0.0164 -4.1100\n",
      "a.: 0.0000 -inf\n",
      "Log Likelihood = -inf\n",
      "Negative Log Likelihood = inf\n",
      "Avg neg log likelihood = inf\n"
     ]
    }
   ],
   "source": [
    "#normalize the loss function\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "for w in words[:3]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1] #get the index of character1\n",
    "        ix2 = stoi[ch2] #get the index of character2\n",
    "        prob = P[ix2, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        print(f\"{ch1}{ch2}: {prob:.4f} {logprob:.4f}\")\n",
    "\n",
    "print(f'Log Likelihood = {log_likelihood}')\n",
    "nll = -log_likelihood\n",
    "print(f'Negative Log Likelihood = {nll}')\n",
    "print(f'Avg neg log likelihood = {nll/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c8f8d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goal is to maximize likelihood of the data w.r.t model parameters\n",
    "#equivalent to maximise the log likelihood function\n",
    "#equivalent to maximize the negative log likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9e71f582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".a: 0.0164 -4.1100\n",
      "an: 0.1040 -2.2634\n",
      "nd: 0.0271 -3.6078\n",
      "dr: 0.0335 -3.3973\n",
      "re: 0.0622 -2.7769\n",
      "e.: 0.0000 -inf\n",
      "Log Likelihood = -inf\n",
      "Negative Log Likelihood = inf\n",
      "Avg neg log likelihood = inf\n"
     ]
    }
   ],
   "source": [
    "#normalize the loss function\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "for w in [\"andre\"]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1] #get the index of character1\n",
    "        ix2 = stoi[ch2] #get the index of character2\n",
    "        prob = P[ix2, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        print(f\"{ch1}{ch2}: {prob:.4f} {logprob:.4f}\")\n",
    "\n",
    "print(f'Log Likelihood = {log_likelihood}')\n",
    "nll = -log_likelihood\n",
    "print(f'Negative Log Likelihood = {nll}')\n",
    "print(f'Avg neg log likelihood = {nll/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4789d37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".b: 0.0144 -4.2428\n",
      "ba: 0.0164 -4.1100\n",
      "al: 0.0964 -2.3397\n",
      "la: 0.0164 -4.1100\n",
      "aj: 0.0007 -7.2793\n",
      "ji: 0.0046 -5.3747\n",
      "ij: 0.0007 -7.2793\n",
      "j.: 0.0000 -inf\n",
      "Log Likelihood = -inf\n",
      "Negative Log Likelihood = inf\n",
      "Avg neg log likelihood = inf\n"
     ]
    }
   ],
   "source": [
    "#normalize the loss function\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "for w in [\"balajij\"]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1] #get the index of character1\n",
    "        ix2 = stoi[ch2] #get the index of character2\n",
    "        prob = P[ix2, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        print(f\"{ch1}{ch2}: {prob:.4f} {logprob:.4f}\")\n",
    "\n",
    "print(f'Log Likelihood = {log_likelihood}')\n",
    "nll = -log_likelihood\n",
    "print(f'Negative Log Likelihood = {nll}')\n",
    "print(f'Avg neg log likelihood = {nll/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "61dc2c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log likelihood is going to infinity ???\n",
    "#because j. combination is not available in bigram model..hence prob goes to infinity\n",
    "#to avoid this, need to smooth out the model\n",
    "#we add 1 to values instead of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bbd6fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+1).float() #this will ensure no prob with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "de995030",
   "metadata": {},
   "outputs": [],
   "source": [
    "P /= P.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ef0ecc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cony.\n",
      "a.\n",
      "nn.\n",
      "kohin.\n",
      "tolian.\n",
      "juee.\n",
      "ksahnaauranilevias.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        p = P[ix]\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break;\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5139dd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".b: 0.0146 -4.2270\n",
      "ba: 0.0164 -4.1090\n",
      "al: 0.0962 -2.3408\n",
      "la: 0.0164 -4.1090\n",
      "aj: 0.0010 -6.8831\n",
      "ji: 0.0047 -5.3641\n",
      "ij: 0.0010 -6.8831\n",
      "j.: 0.0000 -10.3754\n",
      "Log Likelihood = -44.29145050048828\n",
      "Negative Log Likelihood = 44.29145050048828\n",
      "Avg neg log likelihood = 5.536431312561035\n"
     ]
    }
   ],
   "source": [
    "#normalize the loss function\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "for w in [\"balajij\"]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1] #get the index of character1\n",
    "        ix2 = stoi[ch2] #get the index of character2\n",
    "        prob = P[ix2, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        print(f\"{ch1}{ch2}: {prob:.4f} {logprob:.4f}\")\n",
    "\n",
    "print(f'Log Likelihood = {log_likelihood}')\n",
    "nll = -log_likelihood\n",
    "print(f'Negative Log Likelihood = {nll}')\n",
    "print(f'Avg neg log likelihood = {nll/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d7397a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#So now, our model will predict some loss values instead of infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "49a720ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Till now, we trained a respectable Bigram Model\n",
    "#we created list of bigrams, its co-occurance count\n",
    "#converted into probabilities & normalized\n",
    "#and sample character to generate new words\n",
    "#and found loss function to measure performance of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0db78c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets implement the same using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7b8cfced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7f508599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the training set of all the bigrams (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b0dab503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        print(ch1, ch2)\n",
    "        xs.append(ix1)# instead we add input into xs\n",
    "        ys.append(ix2)# equivalent next char output into ys\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2615f446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2ba82e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ae34685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#will create 1 - hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "98008325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3a030285",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, 27) # input, no of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "51c179d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ff62de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1974601f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc #notice how encoding is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "83c5a0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2f9b9337",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = xenc.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1ebd2983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.dtype#since tensor operate using float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0f5a654a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6009],\n",
       "        [ 0.4836],\n",
       "        [ 0.1940],\n",
       "        [ 1.9925],\n",
       "        [ 0.5382],\n",
       "        [-0.4097],\n",
       "        [ 0.0469],\n",
       "        [ 1.6689],\n",
       "        [ 0.6652],\n",
       "        [-0.2756],\n",
       "        [ 0.2623],\n",
       "        [-1.0403],\n",
       "        [-0.5606],\n",
       "        [-1.2323],\n",
       "        [-0.2055],\n",
       "        [ 1.1763],\n",
       "        [ 0.0605],\n",
       "        [-0.7068],\n",
       "        [-1.4584],\n",
       "        [-0.1220],\n",
       "        [ 0.1921],\n",
       "        [-0.5486],\n",
       "        [ 0.0241],\n",
       "        [-1.9292],\n",
       "        [ 0.8182],\n",
       "        [ 1.4553],\n",
       "        [ 0.2873]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn((27, 1))\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a7cd9f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6009],\n",
       "        [-0.4097],\n",
       "        [-1.2323],\n",
       "        [-1.2323],\n",
       "        [ 0.4836]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc @ W #@ refers matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd8d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5, 27) @ (27, 1) = (5, 1)\n",
    "#because 27 will multiply & add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e428639d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4127e+00, -1.0323e+00, -1.2416e+00, -1.3529e+00, -3.7065e-01,\n",
       "         -2.0843e+00,  4.8626e-01, -2.0800e-01,  2.0811e-01, -7.2852e-01,\n",
       "          3.1693e-04, -3.7791e-01,  3.5539e-02, -7.8472e-01, -1.7430e+00,\n",
       "         -2.1291e-01,  1.5303e+00,  3.2433e-01, -2.2134e-01,  1.9307e+00,\n",
       "          1.0623e-01,  4.0056e-01,  1.6609e+00,  1.4033e-01, -9.6486e-01,\n",
       "          8.6772e-02,  1.4405e+00],\n",
       "        [-4.0569e-01,  1.3693e+00,  1.3742e-01, -4.8969e-01,  1.5117e+00,\n",
       "          6.1550e-01, -4.2795e-01, -5.7637e-01,  6.9940e-01,  2.9226e-01,\n",
       "         -1.4475e+00, -2.6385e-01, -1.3356e+00,  1.1788e+00,  1.2694e+00,\n",
       "          1.2456e+00,  2.0707e+00, -7.2070e-01, -1.6873e+00, -1.0932e+00,\n",
       "          1.5943e+00,  2.2989e-01, -1.5005e+00, -5.6755e-01,  3.3543e-01,\n",
       "          1.7593e+00, -9.6089e-01],\n",
       "        [ 4.8009e-01,  2.3811e+00,  1.0540e+00,  7.2272e-01, -1.4951e+00,\n",
       "          7.2220e-01,  2.6590e-01,  1.8818e-01, -7.5627e-01, -1.0150e+00,\n",
       "          1.5757e+00,  8.3883e-02,  2.8497e-01, -1.4964e+00,  2.2132e-01,\n",
       "         -1.8556e-02,  2.2274e+00,  1.8857e+00, -2.9190e-01, -4.2206e-01,\n",
       "         -3.6937e-02, -8.4706e-01,  1.0704e+00, -1.8389e+00,  6.4891e-01,\n",
       "         -5.4371e-01,  1.5196e+00],\n",
       "        [-6.2716e-01, -6.1391e-01, -7.4965e-02, -2.7039e-01,  9.3165e-01,\n",
       "         -4.5886e-01, -3.7477e-01, -1.7202e+00,  8.5990e-01,  1.6502e-01,\n",
       "         -6.0678e-01, -1.0079e+00, -6.9107e-01, -7.2716e-01,  1.2012e+00,\n",
       "          4.1518e-01, -3.3342e-01,  1.9631e-01, -2.3515e-01,  4.4630e-01,\n",
       "         -1.2170e+00, -7.0118e-01, -9.3669e-01, -9.4692e-01, -7.2423e-02,\n",
       "          2.3088e+00, -3.7984e-02],\n",
       "        [ 1.5467e-02, -4.8899e-03,  4.9252e-01,  1.1975e+00, -1.3598e+00,\n",
       "          3.6840e-01,  4.8217e-01, -9.3115e-01, -2.0303e-01, -5.1851e-01,\n",
       "         -1.9637e-01,  7.2335e-01, -2.7738e-01,  2.5624e-01, -1.0937e-01,\n",
       "          4.0880e-01, -2.4953e+00,  1.7644e+00, -9.7892e-01,  1.4634e+00,\n",
       "          2.4907e-01,  1.0242e+00, -5.6300e-01,  1.7240e+00,  3.1699e-01,\n",
       "          1.2213e+00,  4.1096e-01],\n",
       "        [ 2.1297e+00, -1.4128e+00,  1.4160e+00,  1.2280e+00,  3.7320e-01,\n",
       "          1.4479e+00, -1.3197e+00,  4.3890e-01, -3.5884e-01, -8.8091e-02,\n",
       "         -7.5033e-01,  7.0202e-01, -9.0377e-01,  7.6061e-01,  2.1112e+00,\n",
       "         -2.0073e+00, -5.8846e-01,  1.3949e-01, -8.6333e-02,  4.7706e-01,\n",
       "         -1.1149e+00,  4.1814e-01,  4.8450e-01,  6.1192e-01, -9.1331e-01,\n",
       "         -5.2665e-01,  2.2808e-01],\n",
       "        [ 4.8478e-01, -2.8674e-01,  4.0726e-01, -7.6912e-01,  1.3994e+00,\n",
       "          1.8068e-01, -9.6377e-01,  1.2127e+00, -1.4354e-01,  3.8829e-03,\n",
       "          6.4911e-01, -3.1395e-01, -1.0286e-01,  1.5276e+00, -7.0489e-01,\n",
       "          4.0734e-01,  9.7103e-01,  7.4552e-01,  3.8901e-01,  4.7890e-01,\n",
       "          6.9837e-01,  7.4569e-01,  1.4865e+00,  2.3277e-01, -4.4181e-01,\n",
       "         -7.9688e-01,  7.4748e-01],\n",
       "        [-7.1347e-01,  2.6106e-01,  2.7030e-01,  9.7717e-01,  6.0175e-02,\n",
       "          3.1451e-01,  2.3523e-01,  7.4273e-01,  8.7603e-02, -1.5748e+00,\n",
       "         -3.0365e-01, -3.1163e-01, -1.4996e+00, -1.9202e+00,  1.5522e-01,\n",
       "          4.5874e-01,  8.8498e-01,  6.0234e-01, -1.3137e-01, -2.0283e+00,\n",
       "         -5.8190e-01,  3.7916e-02,  5.1081e-01, -9.4318e-01,  1.1434e+00,\n",
       "         -3.1391e-02,  5.4893e-01],\n",
       "        [ 7.3436e-01, -1.0978e+00,  2.9266e-01,  5.5366e-01,  9.2107e-01,\n",
       "         -8.1149e-01,  8.1396e-01,  6.0581e-01, -1.2354e-01,  3.8761e-01,\n",
       "         -7.5563e-02,  7.3744e-01, -9.8162e-01, -1.7109e+00, -1.4805e-02,\n",
       "          2.9400e+00,  1.3551e+00,  2.0024e+00, -2.4461e-01,  1.1014e+00,\n",
       "          4.8686e-01,  1.1295e+00,  7.0450e-02,  9.2008e-01, -1.2317e+00,\n",
       "         -1.0674e-01, -7.0312e-02],\n",
       "        [ 1.6944e+00, -5.1607e-01, -6.8159e-01, -9.0672e-01, -5.6362e-02,\n",
       "         -1.2698e+00, -4.9120e-01,  2.1421e-01,  2.3297e-02,  6.1178e-01,\n",
       "          6.3564e-01, -1.3404e+00, -1.2703e+00,  2.0047e-01,  1.2606e+00,\n",
       "          1.3592e-01,  4.5542e-01, -1.6886e+00, -3.3100e-01, -2.4484e-01,\n",
       "          1.8556e-01, -7.9177e-01,  1.5705e+00, -2.4859e-01, -1.7988e+00,\n",
       "          6.5898e-01, -4.4450e-01],\n",
       "        [-8.4904e-01,  6.3192e-02, -8.7016e-01,  2.0704e+00,  3.6340e-01,\n",
       "          3.9919e-01,  1.0838e+00, -1.4799e+00,  1.1557e+00,  2.1470e+00,\n",
       "          4.8168e-01, -1.2480e+00,  3.5159e-01, -1.1069e+00,  1.9731e-01,\n",
       "         -6.7837e-01,  1.6258e+00, -8.9895e-01, -9.3085e-01, -1.4221e+00,\n",
       "          7.0491e-01,  6.1243e-01, -5.2559e-01, -1.9458e+00,  6.1246e-01,\n",
       "         -1.0670e+00,  7.2860e-01],\n",
       "        [ 9.9257e-01,  3.9332e-02, -2.1546e-01, -5.1956e-01,  9.8435e-02,\n",
       "         -9.6427e-01,  9.6417e-01,  1.1384e+00,  2.8416e-01,  1.4249e+00,\n",
       "          8.6768e-01,  1.8289e-01, -5.6951e-01, -8.2822e-01,  1.0337e-01,\n",
       "          1.2544e+00, -1.1226e+00, -7.2556e-01, -2.5727e+00, -2.0650e-01,\n",
       "          2.2650e-01,  6.9441e-01, -1.3158e+00,  3.3795e-01, -6.0642e-01,\n",
       "          9.5405e-01, -1.8524e-01],\n",
       "        [ 3.1607e-01, -1.1022e+00,  2.1725e-01, -1.0267e+00,  2.0166e-01,\n",
       "         -9.2315e-01, -3.3625e-01, -7.0987e-01,  9.0464e-01,  3.6226e-01,\n",
       "          2.3118e-01, -5.2006e-02,  3.7121e-01, -3.6558e-01, -6.6273e-01,\n",
       "          1.9186e+00,  3.3106e-01,  1.2911e+00,  3.5366e-01, -5.8627e-01,\n",
       "         -5.8395e-02,  1.2980e+00, -7.4801e-01, -6.5491e-01, -1.2333e+00,\n",
       "          1.1364e+00,  1.5131e-01],\n",
       "        [-1.0154e+00,  1.1600e-01, -3.0322e-01, -9.9661e-01, -1.6538e+00,\n",
       "         -1.5214e+00, -6.4469e-01,  5.3138e-01,  2.8008e-01,  8.4764e-01,\n",
       "          1.4783e+00, -5.5993e-01, -3.4837e-01,  2.7378e-01, -6.3771e-02,\n",
       "         -7.6800e-01,  1.5143e+00,  5.2367e-01,  8.3598e-01,  7.9777e-01,\n",
       "          1.0553e+00, -6.2518e-01,  3.2494e-01,  3.2466e-01, -6.3312e-01,\n",
       "          1.5306e+00,  5.0261e-01],\n",
       "        [ 2.0618e+00, -1.9969e+00, -1.2915e+00,  5.0624e-01, -7.8746e-01,\n",
       "         -1.5642e+00,  3.0341e-01, -2.3988e-01, -2.6561e-01,  5.6631e-01,\n",
       "         -1.7610e+00,  6.8116e-01, -1.7384e+00,  1.2501e+00, -1.0023e+00,\n",
       "          3.3059e-01, -3.4693e-01, -4.1149e-01,  1.5586e-01, -1.0699e-01,\n",
       "         -4.8073e-02,  3.7444e-01, -1.4148e+00, -8.4492e-01, -1.0824e+00,\n",
       "          1.2696e+00,  4.6642e-01],\n",
       "        [ 1.4394e+00,  4.7354e-02,  3.1976e-01,  7.0432e-01, -2.5948e-01,\n",
       "         -6.1271e-02, -4.1415e-02, -7.5343e-01, -3.0566e-01, -1.2424e+00,\n",
       "         -4.1649e-01, -1.9859e-01, -1.0788e-01,  1.3645e-01,  1.3940e-01,\n",
       "          1.0999e+00, -1.0009e+00, -7.1688e-01,  3.8074e-01,  7.1952e-01,\n",
       "          2.3484e+00,  7.9798e-01, -3.0258e-01, -6.3662e-01,  1.4394e+00,\n",
       "         -9.9681e-01, -2.1083e+00],\n",
       "        [-1.3081e+00,  8.9221e-01,  1.8060e-01,  1.1269e+00, -2.5848e-02,\n",
       "          9.8553e-01,  8.6079e-01, -3.1396e+00,  2.9834e-01, -3.5808e-01,\n",
       "          2.0279e+00, -7.1738e-01,  8.1814e-01, -6.0959e-03,  3.8185e-01,\n",
       "         -5.8330e-01, -8.4554e-01, -2.0817e-01,  6.6208e-01,  4.9717e-01,\n",
       "         -5.3419e-01, -1.5661e+00,  8.6185e-01, -1.3685e+00,  2.8387e-01,\n",
       "         -5.0462e-01, -3.7185e-01],\n",
       "        [ 7.8443e-02,  4.6999e-01, -1.3092e+00, -1.0332e+00,  1.2974e+00,\n",
       "          1.0393e+00,  1.6012e+00,  2.2630e+00,  4.4141e-01, -8.4626e-01,\n",
       "          1.7126e+00,  1.0709e+00, -1.3748e+00,  4.2215e-01, -4.5804e-01,\n",
       "          2.0370e+00, -3.9349e-01, -3.2115e-01,  1.2607e+00,  1.1732e+00,\n",
       "         -1.1450e+00,  2.1268e+00, -6.0303e-01, -7.7185e-01, -3.4073e+00,\n",
       "         -4.0849e-01, -9.3905e-02],\n",
       "        [-2.1908e-01, -6.3269e-01, -3.5539e-01, -6.9860e-01, -1.3107e-01,\n",
       "          1.3802e+00, -8.0723e-01,  4.7209e-01,  1.0308e+00,  1.4583e+00,\n",
       "         -7.5599e-02,  2.2288e-01,  8.6434e-01, -3.9876e-01, -4.3343e-02,\n",
       "         -9.3767e-01, -7.5334e-02,  1.1124e+00, -1.1515e-01, -8.1047e-01,\n",
       "         -1.0263e+00,  6.2481e-01,  2.0874e+00,  8.2797e-01, -5.7098e-01,\n",
       "         -8.8949e-01,  9.5044e-01],\n",
       "        [-1.0223e+00, -4.1773e-01, -1.5859e+00, -8.0054e-01,  6.2511e-01,\n",
       "          1.3512e+00, -1.5071e+00,  7.3846e-01, -5.5403e-01, -2.6846e-01,\n",
       "          2.8269e-01,  1.2086e+00, -9.1497e-01,  4.1509e-01, -5.7653e-01,\n",
       "          8.0419e-01, -6.6636e-01,  1.0392e+00,  4.2937e-01,  1.5308e+00,\n",
       "         -4.7744e-01,  3.3885e-01, -2.4305e+00,  4.7367e-01, -8.4796e-02,\n",
       "         -5.1265e-01,  8.3089e-01],\n",
       "        [-1.2673e+00, -7.0977e-01, -7.4329e-01, -1.7354e+00,  4.7138e-01,\n",
       "         -3.9974e-01,  1.1246e+00,  1.4525e-01, -8.6449e-01,  1.1598e+00,\n",
       "         -5.6309e-01, -1.8742e-01, -6.5608e-01, -1.5301e+00, -1.3715e+00,\n",
       "          3.3135e-01,  9.5340e-01,  3.0537e-01,  2.4172e+00,  4.5086e-02,\n",
       "         -2.9980e-01, -9.9337e-01,  5.4936e-01,  1.3418e+00,  3.0059e-01,\n",
       "          2.0217e+00, -4.3326e-01],\n",
       "        [-9.9450e-01, -1.8393e+00,  8.7176e-01,  1.0446e-01,  6.1168e-01,\n",
       "         -3.7142e-01,  7.5545e-01, -1.4655e+00,  8.8255e-01, -4.4340e-01,\n",
       "          6.8800e-02,  1.4630e-02, -1.1275e+00,  1.1647e+00,  2.4602e-01,\n",
       "         -2.2683e-01, -1.6998e+00,  7.9790e-01, -4.5980e-02,  8.6146e-01,\n",
       "         -2.4224e-01,  2.7496e-01,  3.3320e-01, -2.0254e+00,  1.1105e-01,\n",
       "          9.9903e-01,  4.5895e-01],\n",
       "        [-7.8093e-01, -1.2195e+00,  5.9556e-01, -1.7089e+00,  1.4108e-01,\n",
       "         -2.6983e-01,  9.8126e-01, -2.0915e+00, -1.0527e+00, -8.3655e-01,\n",
       "          1.0566e+00, -8.6670e-01, -5.0686e-01,  8.6436e-01,  2.9915e+00,\n",
       "          1.4497e+00, -1.3450e+00,  1.7272e-01,  7.1021e-01,  6.1096e-02,\n",
       "          6.7845e-01, -1.0005e+00, -1.1676e-01,  1.1872e-01, -4.0262e-01,\n",
       "         -9.8584e-01, -4.9550e-01],\n",
       "        [ 1.0702e+00,  8.9390e-01,  2.0058e+00, -5.1853e-02, -3.7116e-01,\n",
       "          5.4768e-01,  5.5830e-01,  2.4470e-03,  2.3739e+00,  1.9322e-01,\n",
       "          1.5837e+00,  3.2307e-03, -7.0497e-01,  1.2940e+00,  6.5805e-02,\n",
       "          1.3344e+00,  1.9522e+00,  9.9551e-01,  2.0046e+00,  1.6866e+00,\n",
       "          5.7855e-01,  9.5560e-01,  1.9897e+00,  8.8272e-01,  2.5679e+00,\n",
       "         -2.3166e+00,  6.3940e-01],\n",
       "        [ 7.1183e-01, -3.6494e-01, -4.4992e-01, -3.9876e-01,  1.9443e+00,\n",
       "          1.2703e+00, -6.9484e-02, -6.8961e-01, -8.0300e-01, -2.2576e+00,\n",
       "          1.1885e+00, -1.2256e+00, -1.0602e-01,  8.3611e-01,  1.5064e+00,\n",
       "         -5.5711e-01,  1.7067e+00,  6.8167e-01, -1.0140e+00, -8.4412e-01,\n",
       "         -2.2034e-01,  1.1615e+00,  3.4076e-01, -1.0884e+00, -1.1593e+00,\n",
       "          1.9783e+00, -2.1558e+00],\n",
       "        [-1.5143e+00, -5.7225e-01, -1.1628e+00,  9.2484e-01, -2.1818e-01,\n",
       "          7.8980e-01, -4.5065e-01, -6.8696e-01,  8.0923e-01, -1.5201e+00,\n",
       "          3.7465e-01,  4.9603e-01,  1.7993e-01,  7.9531e-01, -1.6027e+00,\n",
       "          2.5921e-02,  7.7768e-01,  8.3662e-01,  1.2604e+00, -9.8714e-01,\n",
       "          1.3081e+00,  7.0008e-03,  3.6956e-01,  1.5473e+00,  2.0202e-01,\n",
       "         -1.9814e+00,  1.1015e+00],\n",
       "        [-2.9948e-01, -1.2314e+00, -2.6404e-01, -6.7715e-01, -1.3077e+00,\n",
       "         -5.1437e-01, -1.0184e+00, -7.9112e-01, -1.2289e+00, -5.1610e-01,\n",
       "          1.0574e+00, -1.7341e+00, -3.3013e-01,  5.0515e-01,  1.1291e+00,\n",
       "          1.6469e-01, -1.2299e+00, -7.6272e-01, -8.7927e-03,  1.5002e+00,\n",
       "         -5.0443e-01,  1.7833e+00, -7.1265e-01,  1.1916e+00,  1.7113e+00,\n",
       "         -3.2564e-01, -1.0531e-01]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn((27, 27))\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8834c53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4127e+00, -1.0323e+00, -1.2416e+00, -1.3529e+00, -3.7065e-01,\n",
       "         -2.0843e+00,  4.8626e-01, -2.0800e-01,  2.0811e-01, -7.2852e-01,\n",
       "          3.1693e-04, -3.7791e-01,  3.5539e-02, -7.8472e-01, -1.7430e+00,\n",
       "         -2.1291e-01,  1.5303e+00,  3.2433e-01, -2.2134e-01,  1.9307e+00,\n",
       "          1.0623e-01,  4.0056e-01,  1.6609e+00,  1.4033e-01, -9.6486e-01,\n",
       "          8.6772e-02,  1.4405e+00],\n",
       "        [ 2.1297e+00, -1.4128e+00,  1.4160e+00,  1.2280e+00,  3.7320e-01,\n",
       "          1.4479e+00, -1.3197e+00,  4.3890e-01, -3.5884e-01, -8.8091e-02,\n",
       "         -7.5033e-01,  7.0202e-01, -9.0377e-01,  7.6061e-01,  2.1112e+00,\n",
       "         -2.0073e+00, -5.8846e-01,  1.3949e-01, -8.6333e-02,  4.7706e-01,\n",
       "         -1.1149e+00,  4.1814e-01,  4.8450e-01,  6.1192e-01, -9.1331e-01,\n",
       "         -5.2665e-01,  2.2808e-01],\n",
       "        [-1.0154e+00,  1.1600e-01, -3.0322e-01, -9.9661e-01, -1.6538e+00,\n",
       "         -1.5214e+00, -6.4469e-01,  5.3138e-01,  2.8008e-01,  8.4764e-01,\n",
       "          1.4783e+00, -5.5993e-01, -3.4837e-01,  2.7378e-01, -6.3771e-02,\n",
       "         -7.6800e-01,  1.5143e+00,  5.2367e-01,  8.3598e-01,  7.9777e-01,\n",
       "          1.0553e+00, -6.2518e-01,  3.2494e-01,  3.2466e-01, -6.3312e-01,\n",
       "          1.5306e+00,  5.0261e-01],\n",
       "        [-1.0154e+00,  1.1600e-01, -3.0322e-01, -9.9661e-01, -1.6538e+00,\n",
       "         -1.5214e+00, -6.4469e-01,  5.3138e-01,  2.8008e-01,  8.4764e-01,\n",
       "          1.4783e+00, -5.5993e-01, -3.4837e-01,  2.7378e-01, -6.3771e-02,\n",
       "         -7.6800e-01,  1.5143e+00,  5.2367e-01,  8.3598e-01,  7.9777e-01,\n",
       "          1.0553e+00, -6.2518e-01,  3.2494e-01,  3.2466e-01, -6.3312e-01,\n",
       "          1.5306e+00,  5.0261e-01],\n",
       "        [-4.0569e-01,  1.3693e+00,  1.3742e-01, -4.8969e-01,  1.5117e+00,\n",
       "          6.1550e-01, -4.2795e-01, -5.7637e-01,  6.9940e-01,  2.9226e-01,\n",
       "         -1.4475e+00, -2.6385e-01, -1.3356e+00,  1.1788e+00,  1.2694e+00,\n",
       "          1.2456e+00,  2.0707e+00, -7.2070e-01, -1.6873e+00, -1.0932e+00,\n",
       "          1.5943e+00,  2.2989e-01, -1.5005e+00, -5.6755e-01,  3.3543e-01,\n",
       "          1.7593e+00, -9.6089e-01]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d594ea9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (5, 27) @ (27, 27) = (5, 27)\n",
    "(xenc @ W).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "65d8eb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2738)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What is every element in output matrxi (5, 27) telling us?\n",
    "#for every one of 27 neurons that created , what is the firing rate of those neurons on 5 inputs given\n",
    "(xenc @ W)[3, 13]\n",
    "#firing rate of 13 neuron looking at the 3rd input; we achieved by dot pdt between 3rd input & 13th column of W matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "980e15fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2a7fbb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7847,  1.1788, -1.4964, -0.7272,  0.2562,  0.7606,  1.5276, -1.9202,\n",
       "        -1.7109,  0.2005, -1.1069, -0.8282, -0.3656,  0.2738,  1.2501,  0.1364,\n",
       "        -0.0061,  0.4221, -0.3988,  0.4151, -1.5301,  1.1647,  0.8644,  1.2940,\n",
       "         0.8361,  0.7953,  0.5051])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[:, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "76780774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2738)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc[3] * W[:, 13]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a4ec3fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice the output is same as above\n",
    "#matrix multiplication is more efficient than doing individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "42001fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#till now we did only x*W ; not added bias & non-linearity function\n",
    "#Lets interprest these 27 neuron values !!!\n",
    "#Notice these no's are both +ve & -ve values...coming from randn function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "24f7fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#But what we want ?\n",
    "#Previously in Bigram, we have a matrix for each co-occurance of characters, we counted the freq of co-occurance\n",
    "#we Normalized the counts to get probabilities and wanted to have similar in pytorch model\n",
    "#count are usually +ve and sum to 1 & prob is always between 0 to 1\n",
    "#these prob represents the prob of next char given a char...\n",
    "#currently in pytorch, neuron matrix is some +ve & -ve no's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a8b22413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0063, 0.0092, 0.0075, 0.0067, 0.0178, 0.0032, 0.0420, 0.0210, 0.0318,\n",
       "         0.0125, 0.0259, 0.0177, 0.0268, 0.0118, 0.0045, 0.0209, 0.1194, 0.0358,\n",
       "         0.0207, 0.1782, 0.0287, 0.0386, 0.1361, 0.0297, 0.0098, 0.0282, 0.1092],\n",
       "        [0.1664, 0.0048, 0.0815, 0.0675, 0.0287, 0.0842, 0.0053, 0.0307, 0.0138,\n",
       "         0.0181, 0.0093, 0.0399, 0.0080, 0.0423, 0.1634, 0.0027, 0.0110, 0.0227,\n",
       "         0.0181, 0.0319, 0.0065, 0.0301, 0.0321, 0.0365, 0.0079, 0.0117, 0.0248],\n",
       "        [0.0088, 0.0274, 0.0180, 0.0090, 0.0047, 0.0053, 0.0128, 0.0415, 0.0323,\n",
       "         0.0569, 0.1069, 0.0139, 0.0172, 0.0321, 0.0229, 0.0113, 0.1109, 0.0412,\n",
       "         0.0563, 0.0541, 0.0701, 0.0131, 0.0337, 0.0337, 0.0129, 0.1127, 0.0403],\n",
       "        [0.0088, 0.0274, 0.0180, 0.0090, 0.0047, 0.0053, 0.0128, 0.0415, 0.0323,\n",
       "         0.0569, 0.1069, 0.0139, 0.0172, 0.0321, 0.0229, 0.0113, 0.1109, 0.0412,\n",
       "         0.0563, 0.0541, 0.0701, 0.0131, 0.0337, 0.0337, 0.0129, 0.1127, 0.0403],\n",
       "        [0.0127, 0.0751, 0.0219, 0.0117, 0.0866, 0.0353, 0.0124, 0.0107, 0.0384,\n",
       "         0.0256, 0.0045, 0.0147, 0.0050, 0.0621, 0.0680, 0.0664, 0.1515, 0.0093,\n",
       "         0.0035, 0.0064, 0.0941, 0.0240, 0.0043, 0.0108, 0.0267, 0.1109, 0.0073]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in pytorch matrix, these 27 no's are basically log counts\n",
    "#to get counts, we take log & exp\n",
    "logits = (xenc @ W) #we will consider as log-counts\n",
    "counts = logits.exp() #this will give positive no and interpret as freq of each co-occurance. this is equivalent to N Matrix\n",
    "#notice all becomes +ve because of exponential function\n",
    "probs = counts/counts.sum(1, keepdims=True) # normalize the rows to get probabilities\n",
    "probs\n",
    "#softmax function - taking logits and exponetiate and divide by total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c6a994d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "06c40038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice all rows count will sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "014a3806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b735948d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0063, 0.0092, 0.0075, 0.0067, 0.0178, 0.0032, 0.0420, 0.0210, 0.0318,\n",
       "        0.0125, 0.0259, 0.0177, 0.0268, 0.0118, 0.0045, 0.0209, 0.1194, 0.0358,\n",
       "        0.0207, 0.1782, 0.0287, 0.0386, 0.1361, 0.0297, 0.0098, 0.0282, 0.1092])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0]\n",
    "#1. basically we feeded \".\" into neural net using 1-hot encoder\n",
    "#2. Post transformations, we got below weights prob\n",
    "#3. we will tune this W - prob using backprop to predict next char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a70bcc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "47dc667a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "94bb5fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b267c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f3099e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass\n",
    "#xenc , logits, counts, prob all differentiable, hence we can back propagate\n",
    "xenc = F.one_hot(xs, num_classes=27).float()#input to the network using one hot encoder\n",
    "logits = xenc @ W #predict log counts\n",
    "counts = logits.exp() # counts, equivelant to N\n",
    "probs = counts/counts.sum(1, keepdims=True) #prob of next character\n",
    "#between: last 2 lines here are together called a softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5e4d7c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0c30f96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "bigram example 1: .e (indexes 0,5)\n",
      "input to the neural net:  0\n",
      "output probabilities from the neural net:  tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "label  (actual next character): 5\n",
      "probablity assigned by the net to the correct chracter: 0.01228625513613224\n",
      "log likelihood: -4.399273872375488\n",
      "negative log likelihood: 4.399273872375488\n",
      "--------------------\n",
      "bigram example 2: em (indexes 5,13)\n",
      "input to the neural net:  5\n",
      "output probabilities from the neural net:  tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
      "label  (actual next character): 13\n",
      "probablity assigned by the net to the correct chracter: 0.018050700426101685\n",
      "log likelihood: -4.014570713043213\n",
      "negative log likelihood: 4.014570713043213\n",
      "--------------------\n",
      "bigram example 3: mm (indexes 13,13)\n",
      "input to the neural net:  13\n",
      "output probabilities from the neural net:  tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label  (actual next character): 13\n",
      "probablity assigned by the net to the correct chracter: 0.026691533625125885\n",
      "log likelihood: -3.623408794403076\n",
      "negative log likelihood: 3.623408794403076\n",
      "--------------------\n",
      "bigram example 4: ma (indexes 13,1)\n",
      "input to the neural net:  13\n",
      "output probabilities from the neural net:  tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label  (actual next character): 1\n",
      "probablity assigned by the net to the correct chracter: 0.07367686182260513\n",
      "log likelihood: -2.6080665588378906\n",
      "negative log likelihood: 2.6080665588378906\n",
      "--------------------\n",
      "bigram example 5: a. (indexes 1,0)\n",
      "input to the neural net:  1\n",
      "output probabilities from the neural net:  tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "label  (actual next character): 0\n",
      "probablity assigned by the net to the correct chracter: 0.014977526850998402\n",
      "log likelihood: -4.201204299926758\n",
      "negative log likelihood: 4.201204299926758\n",
      "============\n",
      "average negative log likelihhood i.e. loss= 3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "#taking 5 examples\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    x = xs[i].item() #input character index\n",
    "    y = ys[i].item() #label character index\n",
    "    print('--------------------')\n",
    "    print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "    print('input to the neural net: ', x)\n",
    "    print('output probabilities from the neural net: ', probs[i])\n",
    "    print('label  (actual next character):', y)\n",
    "    p = probs[i, y]\n",
    "    print('probablity assigned by the net to the correct chracter:', p.item())\n",
    "    logp = torch.log(p)\n",
    "    print('log likelihood:', logp.item())\n",
    "    nll = -logp\n",
    "    print('negative log likelihood:', nll.item())\n",
    "    nlls[i] = nll\n",
    "\n",
    "print('============')\n",
    "print('average negative log likelihhood i.e. loss=', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ca8029d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#by changing seed...we seee different avg loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f6d337d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can minimize loss by tuning W using gradient based optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3128c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimization--------------------------------!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "32842fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "953842bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "9f2eada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g, requires_grad = True)#tell pytorch for gradients to backpropagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "294c157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float()#input to the network using one hot encoder\n",
    "logits = xenc @ W #predict log counts\n",
    "counts = logits.exp() # counts, equivelant to N\n",
    "probs = counts/counts.sum(1, keepdims=True) #prob of next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c0a625db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#refer from our Micrograd project, lets calculate loss and backpropate the gradients and update W parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c995d869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "23985196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0123, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0181, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0267, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0737, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0150, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0, 5], probs[1, 13], probs[2, 13], probs[3, 1], probs[4, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "138beec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#insted of manual picking, how to pass it dynamically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b9f45e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ccaa2d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3c23b0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0123, 0.0181, 0.0267, 0.0737, 0.0150], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[torch.arange(5), ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "1d493695",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -probs[torch.arange(5), ys].log().mean() #negative log likeklihood averange loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "bc42c1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7693, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ff38d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we are ready for backpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f1cffa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first set all gradients to 0\n",
    "W.grad = None\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "8cdf3c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#backwards fill all the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "f8f41a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0121,  0.0020,  0.0025,  0.0008,  0.0034, -0.1975,  0.0005,  0.0046,\n",
       "          0.0027,  0.0063,  0.0016,  0.0056,  0.0018,  0.0016,  0.0100,  0.0476,\n",
       "          0.0121,  0.0005,  0.0050,  0.0011,  0.0068,  0.0022,  0.0006,  0.0040,\n",
       "          0.0024,  0.0307,  0.0292],\n",
       "        [-0.1970,  0.0017,  0.0079,  0.0020,  0.0121,  0.0062,  0.0217,  0.0026,\n",
       "          0.0025,  0.0010,  0.0205,  0.0017,  0.0198,  0.0022,  0.0046,  0.0041,\n",
       "          0.0082,  0.0016,  0.0180,  0.0106,  0.0093,  0.0062,  0.0010,  0.0066,\n",
       "          0.0131,  0.0101,  0.0018],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0058,  0.0159,  0.0050,  0.0104,  0.0398,  0.0058,  0.0019,  0.0067,\n",
       "          0.0019,  0.0060,  0.0140,  0.0046,  0.0023, -0.1964,  0.0022,  0.0063,\n",
       "          0.0058,  0.0009,  0.0183,  0.0043,  0.0097,  0.0060,  0.0100,  0.0005,\n",
       "          0.0024,  0.0004,  0.0094],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0125, -0.1705,  0.0194,  0.0133,  0.0270,  0.0080,  0.0105,  0.0100,\n",
       "          0.0490,  0.0066,  0.0030,  0.0316,  0.0052, -0.1893,  0.0059,  0.0045,\n",
       "          0.0234,  0.0049,  0.0260,  0.0023,  0.0083,  0.0031,  0.0053,  0.0081,\n",
       "          0.0482,  0.0187,  0.0051],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "62660ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating weights based on gradients\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "8c87c276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7492127418518066\n"
     ]
    }
   ],
   "source": [
    "#since we updated weights, loss should decrease\n",
    "#re running forward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float()#input to the network using one hot encoder\n",
    "logits = xenc @ W #predict log counts\n",
    "counts = logits.exp() # counts, equivelant to N\n",
    "probs = counts/counts.sum(1, keepdims=True) #prob of next character\n",
    "loss = -probs[torch.arange(5), ys].log().mean() #negative log likeklihood averange loss \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "fe9b1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.76 to 3.74 yay, loss reduced\n",
    "#keep running, notice reduction in loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "f5221028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "1c5afb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of exmaple:  228146\n"
     ]
    }
   ],
   "source": [
    "#create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of exmaple: ', num)\n",
    "\n",
    "#initialize the network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "4a5a6422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4726529121398926\n",
      "2.4724340438842773\n",
      "2.4722201824188232\n",
      "2.472010850906372\n",
      "2.4718058109283447\n",
      "2.4716053009033203\n",
      "2.471409320831299\n",
      "2.471216917037964\n",
      "2.4710283279418945\n",
      "2.470843553543091\n",
      "2.4706625938415527\n",
      "2.4704854488372803\n",
      "2.4703118801116943\n",
      "2.4701414108276367\n",
      "2.4699742794036865\n",
      "2.4698104858398438\n",
      "2.4696500301361084\n",
      "2.469492197036743\n",
      "2.4693377017974854\n",
      "2.4691858291625977\n",
      "2.4690372943878174\n",
      "2.468891143798828\n",
      "2.468747615814209\n",
      "2.46860671043396\n",
      "2.468468427658081\n",
      "2.468332529067993\n",
      "2.4681990146636963\n",
      "2.4680681228637695\n",
      "2.4679393768310547\n",
      "2.4678127765655518\n",
      "2.46768856048584\n",
      "2.4675662517547607\n",
      "2.4674463272094727\n",
      "2.467327833175659\n",
      "2.467211961746216\n",
      "2.467097759246826\n",
      "2.4669857025146484\n",
      "2.4668753147125244\n",
      "2.466766357421875\n",
      "2.4666597843170166\n",
      "2.466554641723633\n",
      "2.4664509296417236\n",
      "2.4663491249084473\n",
      "2.4662492275238037\n",
      "2.4661505222320557\n",
      "2.4660532474517822\n",
      "2.4659576416015625\n",
      "2.4658634662628174\n",
      "2.4657704830169678\n",
      "2.465679407119751\n",
      "2.4655895233154297\n",
      "2.465500593185425\n",
      "2.4654135704040527\n",
      "2.465327501296997\n",
      "2.465242624282837\n",
      "2.4651591777801514\n",
      "2.4650766849517822\n",
      "2.4649956226348877\n",
      "2.4649155139923096\n",
      "2.464836597442627\n",
      "2.46475887298584\n",
      "2.464682102203369\n",
      "2.464606285095215\n",
      "2.464531660079956\n",
      "2.4644579887390137\n",
      "2.464385509490967\n",
      "2.4643139839172363\n",
      "2.464243173599243\n",
      "2.4641737937927246\n",
      "2.464104652404785\n",
      "2.464036703109741\n",
      "2.4639699459075928\n",
      "2.4639039039611816\n",
      "2.4638383388519287\n",
      "2.463773727416992\n",
      "2.463710308074951\n",
      "2.4636473655700684\n",
      "2.463585376739502\n",
      "2.4635238647460938\n",
      "2.463463306427002\n",
      "2.4634037017822266\n",
      "2.4633448123931885\n",
      "2.4632863998413086\n",
      "2.463228702545166\n",
      "2.4631717205047607\n",
      "2.463115692138672\n",
      "2.463059902191162\n",
      "2.4630050659179688\n",
      "2.4629504680633545\n",
      "2.4628970623016357\n",
      "2.462843894958496\n",
      "2.4627914428710938\n",
      "2.462739944458008\n",
      "2.462688684463501\n",
      "2.4626379013061523\n",
      "2.462587594985962\n",
      "2.462538242340088\n",
      "2.462489128112793\n",
      "2.4624407291412354\n",
      "2.462393045425415\n"
     ]
    }
   ],
   "source": [
    "#gradient descent\n",
    "for k in range(100):\n",
    "    #forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts/ counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "    print(loss.item())\n",
    "    \n",
    "    #backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    #update weights\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "84c0942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice the loss function saturates around 2.4 which more or less similar results from our basic neural nets\n",
    "#This is because our pytorch nerual net is very simple\n",
    "# we have 1 input layer and its weight to predict output\n",
    "#this is scalable approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "a3c7a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 important things to be noted\n",
    "\n",
    "# 1) xenc * W where xenc is 1 hot encode multiplied with weights. 1 hot encoder actually pick the respective row based on 1\n",
    "# and multiply. rows represents current char and we need to find the respective next char prob from that row only. \n",
    "#W is log counts. \n",
    "\n",
    "# 2) note the smoothing we added in previous network  \"P = (N+1).float()\"? this will smooth out the distribution of\n",
    "#probabilities, which will prevent of assigning any 0 probabilities for a bigram. More and more increase of count\n",
    "#prob will become more uniform. Similary we are doing in Gradient based approach!!!\n",
    "\n",
    "#If all weight in W is 0. but we take exp which will make exp(0) = 1 \n",
    "\n",
    "#this is called as regularization\n",
    "\n",
    "#basically we are augmenting the loss function to have small component called as \"Regularization Loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "637cc795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4834256172180176\n",
      "2.4833858013153076\n",
      "2.4833483695983887\n",
      "2.4833130836486816\n",
      "2.4832799434661865\n",
      "2.4832475185394287\n",
      "2.4832167625427246\n",
      "2.483186960220337\n",
      "2.4831581115722656\n",
      "2.4831299781799316\n",
      "2.483102798461914\n",
      "2.483076333999634\n",
      "2.48305082321167\n",
      "2.4830257892608643\n",
      "2.483001232147217\n",
      "2.4829773902893066\n",
      "2.482954263687134\n",
      "2.48293137550354\n",
      "2.4829089641571045\n",
      "2.4828872680664062\n",
      "2.482866048812866\n",
      "2.4828453063964844\n",
      "2.4828240871429443\n",
      "2.482804298400879\n",
      "2.4827845096588135\n",
      "2.482764959335327\n",
      "2.48274564743042\n",
      "2.482727289199829\n",
      "2.48270845413208\n",
      "2.4826903343200684\n",
      "2.4826724529266357\n",
      "2.4826548099517822\n",
      "2.482637643814087\n",
      "2.4826207160949707\n",
      "2.4826037883758545\n",
      "2.4825873374938965\n",
      "2.4825706481933594\n",
      "2.4825544357299805\n",
      "2.482538938522339\n",
      "2.482523202896118\n",
      "2.4825077056884766\n",
      "2.482492446899414\n",
      "2.4824774265289307\n",
      "2.4824624061584473\n",
      "2.482448101043701\n",
      "2.482433557510376\n",
      "2.48241925239563\n",
      "2.4824047088623047\n",
      "2.482390880584717\n",
      "2.482377290725708\n",
      "2.48236346244812\n",
      "2.4823503494262695\n",
      "2.4823367595672607\n",
      "2.48232364654541\n",
      "2.4823105335235596\n",
      "2.482297897338867\n",
      "2.482285261154175\n",
      "2.482272148132324\n",
      "2.482259750366211\n",
      "2.4822475910186768\n",
      "2.4822356700897217\n",
      "2.4822235107421875\n",
      "2.4822115898132324\n",
      "2.4821999073028564\n",
      "2.4821882247924805\n",
      "2.4821765422821045\n",
      "2.4821650981903076\n",
      "2.48215389251709\n",
      "2.482142686843872\n",
      "2.4821319580078125\n",
      "2.4821207523345947\n",
      "2.482109308242798\n",
      "2.4820988178253174\n",
      "2.482088327407837\n",
      "2.4820778369903564\n",
      "2.4820668697357178\n",
      "2.4820566177368164\n",
      "2.482046365737915\n",
      "2.4820363521575928\n",
      "2.4820261001586914\n",
      "2.482016086578369\n",
      "2.482006311416626\n",
      "2.481996536254883\n",
      "2.4819867610931396\n",
      "2.4819769859313965\n",
      "2.4819676876068115\n",
      "2.4819583892822266\n",
      "2.4819488525390625\n",
      "2.4819395542144775\n",
      "2.4819300174713135\n",
      "2.4819211959838867\n",
      "2.4819116592407227\n",
      "2.481903314590454\n",
      "2.4818944931030273\n",
      "2.4818851947784424\n",
      "2.4818766117095947\n",
      "2.481868028640747\n",
      "2.4818594455718994\n",
      "2.4818508625030518\n",
      "2.481842279434204\n"
     ]
    }
   ],
   "source": [
    "for k in range(100):\n",
    "    #forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts/ counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + (0.01*(W**2).mean()) #adding regularization loss\n",
    "    print(loss.item())\n",
    "    \n",
    "    #backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    #update weights\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d90a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now optimization has 2 components; Loss function not only all the probabilities to work out but in addtion to that\n",
    "#simultaneously tries to make all w's to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "f38d58cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cony.\n",
      "a.\n"
     ]
    }
   ],
   "source": [
    "#Finally generating sequence from the neural net model\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        \n",
    "        p = P[ix]\n",
    "        #---------------------\n",
    "        #xenc = F.one_hot(torch.tensor[ix], num_classes=27).float()\n",
    "        #logits = xenc @ W\n",
    "        #counts = logits.exp()\n",
    "        #p = counts / counts.sum(1, keepdims=True) #p comes from neural net\n",
    "        #----------------\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "a70331f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "prelay.\n",
      "a.\n",
      "nn.\n"
     ]
    }
   ],
   "source": [
    "#Finally generating sequence from the neural net model\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        #------befor p is coing from ix---------\n",
    "        #p = P[ix]\n",
    "        #---------------------\n",
    "        #Now\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True) #p comes from neural net\n",
    "        #----------------\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "fdc88f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###??much better output ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1f7734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
