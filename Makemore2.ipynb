{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0baf9c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Welcome to part 2 of make more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3215532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#previously we saw how to implement bigram prediction model i.e given a character, predict next char\n",
    "#and generate sequence of characters to produce words\n",
    "\n",
    "#we implement using count method and also using simple neural net grad based learning\n",
    "\n",
    "#problem with bigram model is, we noticed bigram models output is not that accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb78a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we want to expand wiht more context i.e tri-gram or 4 -gram etc... size of count table explodes exponentially\n",
    "#for single character - no of rows = 27 ie. 26 alphabets and 1 special char \".\"\n",
    "\n",
    "#but for 2 char - rows explode to 27 * 27 rows = 729\n",
    "\n",
    "#Similarly for 3 grams - 27 * 27 * 27 ~= 20K rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b94d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b4c5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQEAeAB4AAD/4RDiRXhpZgAATU0AKgAAAAgABAE7AAIAAAAIAAAISodpAAQAAAABAAAIUpydAAEAAAAQAAAQyuocAAcAAAgMAAAAPgAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHdlbGNvbWUAAAWQAwACAAAAFAAAEKCQBAACAAAAFAAAELSSkQACAAAAAzQzAACSkgACAAAAAzQzAADqHAAHAAAIDAAACJQAAAAAHOoAAAAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAyMDIzOjA5OjE5IDA5OjI5OjM1ADIwMjM6MDk6MTkgMDk6Mjk6MzUAAAB3AGUAbABjAG8AbQBlAAAA/+ELGmh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8APD94cGFja2V0IGJlZ2luPSfvu78nIGlkPSdXNU0wTXBDZWhpSHpyZVN6TlRjemtjOWQnPz4NCjx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iPjxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczpkYz0iaHR0cDovL3B1cmwub3JnL2RjL2VsZW1lbnRzLzEuMS8iLz48cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0idXVpZDpmYWY1YmRkNS1iYTNkLTExZGEtYWQzMS1kMzNkNzUxODJmMWIiIHhtbG5zOnhtcD0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wLyI+PHhtcDpDcmVhdGVEYXRlPjIwMjMtMDktMTlUMDk6Mjk6MzUuNDM0PC94bXA6Q3JlYXRlRGF0ZT48L3JkZjpEZXNjcmlwdGlvbj48cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0idXVpZDpmYWY1YmRkNS1iYTNkLTExZGEtYWQzMS1kMzNkNzUxODJmMWIiIHhtbG5zOmRjPSJodHRwOi8vcHVybC5vcmcvZGMvZWxlbWVudHMvMS4xLyI+PGRjOmNyZWF0b3I+PHJkZjpTZXEgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj48cmRmOmxpPndlbGNvbWU8L3JkZjpsaT48L3JkZjpTZXE+DQoJCQk8L2RjOmNyZWF0b3I+PC9yZGY6RGVzY3JpcHRpb24+PC9yZGY6UkRGPjwveDp4bXBtZXRhPg0KICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICA8P3hwYWNrZXQgZW5kPSd3Jz8+/9sAQwAHBQUGBQQHBgUGCAcHCAoRCwoJCQoVDxAMERgVGhkYFRgXGx4nIRsdJR0XGCIuIiUoKSssKxogLzMvKjInKisq/9sAQwEHCAgKCQoUCwsUKhwYHCoqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioq/8AAEQgCFQMgAwEiAAIRAQMRAf/EAB8AAAEFAQEBAQEBAAAAAAAAAAABAgMEBQYHCAkKC//EALUQAAIBAwMCBAMFBQQEAAABfQECAwAEEQUSITFBBhNRYQcicRQygZGhCCNCscEVUtHwJDNicoIJChYXGBkaJSYnKCkqNDU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6g4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2drh4uPk5ebn6Onq8fLz9PX29/j5+v/EAB8BAAMBAQEBAQEBAQEAAAAAAAABAgMEBQYHCAkKC//EALURAAIBAgQEAwQHBQQEAAECdwABAgMRBAUhMQYSQVEHYXETIjKBCBRCkaGxwQkjM1LwFWJy0QoWJDThJfEXGBkaJicoKSo1Njc4OTpDREVGR0hJSlNUVVZXWFlaY2RlZmdoaWpzdHV2d3h5eoKDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uLj5OXm5+jp6vLz9PX29/j5+v/aAAwDAQACEQMRAD8A+kaKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoooznpQAUUUUAFFFFABRRRkZxnn0oAKKKKACiiigAoooJx1oAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACoru4W0sprhyoWGNnJZsAADPJ7VLVDXNKTXdBvtKmmlgjvYHgeSEgOoYYJGQRnBpSvZ2GrX1MTRfFum/2Ppq3+qW1tquqQi6ttP1C+jWdvMJZEAABI52jCnp3Irk7fxFresN4E1ia3Vry8nu/9Ggu2WKQeTJjdkADBHXDHA75xXpttYQW1rBCF8zyUCK8gBbA/D+WBXO2Pw/stOOkC31TUtmjzSy2iM0RC+YpUqf3eSAGOO/qTVO13YlXtqX9G8U22qeDV8Q3ETWcCxSvPGx3GLyywcZHXBU896xL34iNp9lFc3OkbReaZLqNiPtP+tWNBI0bnb8j7CDxuHXnit7RfDFnovhttDEs17ZuZd32raWYSMzMp2qoxlj2qhb+AdLisYrO6uLy+t7ezlsbZLl1P2eGQBWVSqgk7QFyxJwOvJzLvrb5fj/wCla6v/W3/BMjxL8Qb6w0rUjpunwi4g0eHU4pJZyVxIxXBXb1UjPXB9qsar491HS7vU7ZfDb3j6VZRXt00N4oHlPvzs3AFiPLOBgZ9uMyH4ZadLa3MN3qurXX2nTV013lljyIlYlCNsYAYE9cc9881TTwtdXvjnWo7y41NNOutKtrRrkCMfayrS+YrHZwcOOVC9TjpxUv7vn+tv0EttfL9L/qWJPiOv8AZ+p3kGmiWGxns0U/aMebHchCr/c4IEgyvsea0/DmuanqviPxHaXsFvHbabeLbwNHKWYgxI/IKjrvznPHT3qrqPw40vUJrwpe6hZwXgt/NtbWRFjLQbfLYZQkEBQMZwQOmea2NN8PQaXrWpalb3Vyzak6yTQuymMOqKm8YUEEhFzyRxwBRpf+vL/g/eLW2plT+MzDcSRbtE+Ryvz6sVbg9x5XB9q0/EOsx6T4L1DV58FILJ5sRucMduQAwweTgA8VsVkeJvDsHijSP7NvLu6trcypI4tigMmxgwU7lb5cqMjvUtXjYpO0rnC6ZFr+haVo+oE6oYtK0uafWZdQu3dbuTychFRmJyH53AAADGTnFdT/AGo/hfS9DS6t/tC386Q3d2ZQpSWRSd5BHILcdRjjtXQX1lHqGl3NjcZMVxC0L+pDDB/nWN/YS674bsdP19JPMspYmkCnAleIgg57q2AfocHvVXvK/mvuu7/16E/ZXo/vtoGjeLE11rZrGzk8mW4uYXd22tH5LFdxXHRjj6bhXQ1ymp+F/seh6nBoqXMk+q3oknZJgjRpJKDJsORgBSxxnqe9dUihEVV6KMDJzS6D6/18jMm1m4imeNdD1KUKxAdBFtb3GZAcVcsrp7uDzJLSe0O7Hlz7d31+UkY/GqcvhjQbiZ5p9E06SSRizu9ohLE9STjk1csrCz02DyNPtILWLO7y4Iwi59cChAZep6m6+K9H0iLzQbhZbp3ikAwkYAIdSpypMi9CDn6VLH/yOtz/ANg+L/0ZJTjoMJ8WDXzcT+cLQWghyPLC7ixPTOSSO+OBxxTo4JR4snuDG3ktYxoHxwWDuSPyI/OhdPn+v6WCWz+X5oqXviSYaheWWjaa+pzWCoboLKIwrNyI1JB3PtO7HAAIyeapS+OoFZLiCzaXTW1NdM+1eZgvMX2EomPmUP8AKTkdGwCBzctvCq2fiO+1S11S+ii1CVZ7ixUp5Tyqqruzt3jIVcgNg4+oqCz8C2FobWM3NzLa2NxLc2ds5XZBK5Y7+FyxXe23dnGe55o7f1/XWwPqUdM+IX9p6rZ2kWlN5d1e3VotwLgFR5AbLgYyVJXHtuXryA3TfHzHw1Z6rrMVrbyarcuunwLcYVohnDu7AYG1dxOOhGBkgVch+H+nW0WnxWt5exJp+nzWEW11yRLjfITt++SucjAz2ol8CW/2TQUs9UvrO40JDHbXMQiZihUKysrIVwQB0HGKP6/r8PvDr/X9f8MVl8d32/T0fw5Nu1MTrZ7bgASyRk4B3KCqsgLhiOg6Vb0vU73xLq2uabf2MEOnWYjtiUnLu07IHcZ2gDaHUZB4I/J914Ljvbi3uLvWNSlntLpbi1lZo90A2lWRSE6MGbJOTzwRgVa07R08NWepS2bXV9JdXMt35bkEl3bO0YA4yQMnoAMnAo0s7/1t/wAH7g16f1/Wn3j/AApfzal4ZtJ7tt9wu+GV8ffeN2jZvxKk/jWxWd4f0w6PoNrYu4eSNSZXHRpGJZyPYsSa0ab3BBRRRSAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAqC+vIdO0+4vLptkNvG0sjeigZNT1xnxYvJLX4d3qQJIz3LJFlFJ2rncxPoNqkfjWdWfJBy7GVap7OnKfZHZ9elFY/hK8kv/CGl3E6uszWyLKJFKtvUbW4PuDWxVRlzRTLjJSipLqFFFFUUFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVzPxG/wCSdaz/ANcP/ZhXTVzPxG/5J1rP/XD/ANmFZVv4UvRmGI/gz9H+Rt6T/wAgWy/694//AEEVbqppP/IFsv8Ar3j/APQRVurjsjWPwoKKKjnuIbWFprmVIYl+88jBVH4mqKJKKQEMoKkEEZBHeloAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACisbxP4r0rwhpQv9anaNGcRxRxqXkmc9FRRyTXLD4sSMMr4A8akHof7J6/+PV20cBia8OenDTvt+ZLlFOzPQqK8+/4WvL/0T/xr/wCCr/7Kj/ha8v8A0T/xr/4Kv/sq1/srGfyfiv8AMXPE9Borz7/ha8v/AET/AMa/+Cr/AOyo/wCFry/9E/8AGv8A4Kv/ALKj+ysZ/J+K/wAw54noNFeff8LXl/6J/wCNf/BV/wDZUf8AC15f+if+Nf8AwVf/AGVH9lYz+T8V/mHPE9Borz7/AIWvL/0T/wAa/wDgq/8AsqP+Fry/9E/8a/8Agq/+yo/srGfyfiv8w54noNFeff8AC15f+if+Nf8AwVf/AGVH/C15f+if+Nf/AAVf/ZUf2VjP5PxX+Yc8T0GivPv+Fry/9E/8a/8Agq/+yo/4WvL/ANE/8a/+Cr/7Kj+ysZ/J+K/zDnieg0V59/wteX/on/jX/wAFX/2VH/C15f8Aon/jX/wVf/ZUf2VjP5PxX+Yc8T0GivPv+Fry/wDRP/Gv/gq/+yo/4WvL/wBE/wDGv/gq/wDsqP7Kxn8n4r/MOeJ6DRXn3/C15f8Aon/jX/wVf/ZUf8LXl/6J/wCNf/BV/wDZUf2VjP5PxX+Yc8T0GivPv+Fry/8ARP8Axr/4Kv8A7Kj/AIWvL/0T/wAa/wDgq/8AsqP7Kxn8n4r/ADDnieg0V59/wteX/on/AI1/8FX/ANlR/wALXl/6J/41/wDBV/8AZUf2VjP5PxX+Yc8T0GuZ+I3/ACTrWf8Arh/7MKxP+Fry/wDRP/Gv/gq/+yrJ8UeP7vxB4XvtKtvAnjCGW6j2LJNpR2Kcg84JPbsKyr5XjPZS9zo+q/zMa8lKlJLs/wAj0zSf+QLZf9e8f/oIqS+mkttOuZ4RGZI4mdBK21SQCRk9h7159ZfFCe1sLe3fwB4zZoolQldK4JAxx81SS/FIzwvFP8O/GUkcilXR9IBVgeCCC3Iq1lWMcfg/Ff5msJxSVySw8d6hd3ljpdxEtlqN1em1lW5tGT7KRAZcEBysm4qdrK2CPcEVTuNU1PxHeeF1uzbwH+1r2znQQF4ZzFHMm8KW+6Qp4OcZ6nFRf8Jzpn2I2f8AwqnxP9lLBzB/YKbCw6HbnGRgYPtTn8e2Ej2rSfCzxS7WRzaltCQmDjHyfN8vHpVvKsW/sfiv8w51bf8ArX/gfcReG9f8QQaHoOn2d3p/+lWl9IHms2Pk+Q4CKFWRRtwcY4wAMdMVctfH2sfYY7m/fS4heaJDqVuTDIFhd3VNjYZi+S4xgLzx3zVNfGWjpL5ifCTxIsmZDuHh+MH5/v8AOf4u/r3pv/CXaJ5Qj/4VF4i8sQfZgn/CPR48rOfLxn7uf4elCyvGdYfivPz819w+ePT+tv8Ag/eX7nxvr8Ph691KOK2kl0vVZLS4tBCfMuIVAJKBXbDqCSfvAhW6V3mkztdaPaXD3UN4ZYVf7RAu2OTIzuUZOAe3Jrze38a6VaSCS1+E/iaBxnDR6AikZUKeQe6gD6DFWrH4jwaZZpaab8NPF1nbR8JDb6KsaL9FDYFCyvF2tyduq7a9fmHPG/3npNFeff8AC15f+if+Nf8AwVf/AGVH/C15f+if+Nf/AAVf/ZUv7Kxn8n4r/MOeJ6DRXn3/AAteX/on/jX/AMFX/wBlR/wteX/on/jX/wAFX/2VH9lYz+T8V/mHPE9Borz7/ha8v/RP/Gv/AIKv/sqP+Fry/wDRP/Gv/gq/+yo/srGfyfiv8w54noNFeff8LXl/6J/41/8ABV/9lR/wteX/AKJ/41/8FX/2VH9lYz+T8V/mHPE9Borz7/ha8v8A0T/xr/4Kv/sqP+Fry/8ARP8Axr/4Kv8A7Kj+ysZ/J+K/zDnieg0V59/wteX/AKJ/41/8FX/2VH/C15f+if8AjX/wVf8A2VH9lYz+T8V/mHPE9Borz7/ha8v/AET/AMa/+Cr/AOyo/wCFry/9E/8AGv8A4Kv/ALKj+ysZ/J+K/wAw54noNFeff8LXl/6J/wCNf/BV/wDZUf8AC15f+if+Nf8AwVf/AGVH9lYz+T8V/mHPE9Borzx/i4sCGW88DeMraBeZJpNKwsY7k/N0Fdpomt6f4i0e31TRrlbmzuF3RyL+oI6gg8EHpWNfBYjDx56kbLvuvwGpJ7F+iiiuMoKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDzzxWiz/ABu8DRTgPGkN9KiN0DiMYbHqK9DrmvGfg1PFcFlNbX8ul6rpspmsb+FQzQseCCp4ZSAMjvisIaH8UwMf8Jfo7Y7nTeTXsSjSxVCkvaqLgmmpX/mburJrr66GesW9Cb4wanquleB/N0aSWEyXCx3E0RIZIyD3HTJCjPvjvXH/AAP1rWrvXL2yuLie505bfzG81iwik3ALgnpkbuO+Pauok8P/ABQmiaObxVokkbjDI2mZBHoRUNl4U+I+mwmHTvEXh+0iJyUg0gIufXArz5ZTGVdVfrMLLp73/wAifQ0M4pUssngXQvKT+LT+rrocl8adc1y38WxWSXNxa2CwK8CxOUWQn7zHHUg8e2Pfn0X4XalquqfD+2udYMks4Z1illPzSoD8pJ785Ge+Kxr3wl8RdSjVNR8Q+HrtEOVWfSA4U+oBrN0m7+Iura3d6XpHivR5odPjAmuY9NHlI5OBGvPJwD04GKmOWQp13N4qGuy97/5EnGZ3Qq5dSwSo8sovfTX9dd2ebv4s8W/8Jcbo316NT+0bfI3NgNux5ezpjPG2vqNCxjUyDaxA3AHoa81/4Q/4hnUPt/8Ab/h37ZjH2j+xx5mP97rVv+xPin/0N2jf+C3/AOvVYfKo0b82Jg7/AOL/AORKznOKWYqn7Khycqtpby09F09T0GivPv7E+Kf/AEN2jf8Agt/+vSHRfimBk+LtFAH/AFDf/r11fUaf/P8Ah/5N/wDInz/M+x6FRXjVvrfxC1LxKui6L4t0jUZVBa4ni079zAo7ls/Mc4AAz16iuk/sT4p/9Ddo3/gt/wDr1McJRn8OIh/5N/8AIkQrRnrHU9BqvEbw31wJ0gFqAvkMjkuxwd24EYHOMYJ/CuG/sT4p/wDQ3aN/4Lf/AK9H9ifFP/obtG/8Fv8A9er+o0/+f8P/ACb/AORL5n2O4tmvjBN9rjt1lEj+SIpGKlM/IWJAwcYyBkDtmn2ZujYwnUFhS6KDzlgYsgfHO0kAkZ9QK4T+xPin/wBDdo3/AILf/r0f2J8U/wDobtG/8Fv/ANej6jT/AOf8P/Jv/kQ5n2Z6DRXn39ifFP8A6G7Rv/Bb/wDXo/sT4p/9Ddo3/gt/+vR9Rp/8/wCH/k3/AMiHM+zPQaK8+/sT4p/9Ddo3/gt/+vR/YnxT/wChu0b/AMFv/wBej6jT/wCf8P8Ayb/5EOZ9meg0V59/YnxT/wChu0b/AMFv/wBej+xPin/0N2jf+C3/AOvR9Rp/8/4f+Tf/ACIcz7M9Borz7+xPin/0N2jf+C3/AOvR/YnxT/6G7Rv/AAW//Xo+o0/+f8P/ACb/AORDmfZnoNePfHLWNZsZtNtbOee20+WNmd4WK+ZID90kegwce/tXQf2J8U/+hu0b/wAFv/16z9c0P4irol3Nq3iPQbu0t4mnkil0kOCFBPQ8Z4rDEZbCpTcViIL/AMC/+RPSyvMIYDFRxFSnzJX007b/ACLXwW1XV9T8K3P9rSyzwwz7LaaYksRjlcnqAcfnjtXlvjbxN4kTx9qBmv7y1ltbpkgjjlZBGgPy4A45GDnvnNek6HpvxKvdAsbrT/E+iW9tPAksUK6WFCKwzjAOBjPapJ/B/wAQ7m8ju7nX/Ds1zF/q5pNHDOn0Y8iuaeURqUYwWKhp197/AORPXwfEGHo46ri3h7xmtFpp/wAP1O90Ke8uvDunz6mnl3kttG864xhyoLcdue1X68+/sT4p/wDQ3aN/4Lf/AK9H9ifFP/obtG/8Fv8A9eu9YCmlb6xD/wAm/wDkT5mdTmm5KNr9D0GivPv7E+Kf/Q3aN/4Lf/r0f2J8U/8AobtG/wDBb/8AXp/Uaf8Az/h/5N/8iRzPsz0GivPv7E+Kf/Q3aN/4Lf8A69H9ifFP/obtG/8ABb/9ej6jT/5/w/8AJv8A5EOZ9meg0V59/YnxT/6G7Rv/AAW//Xo/sT4p/wDQ3aN/4Lf/AK9H1Gn/AM/4f+Tf/IhzPszuY5521CaB7R0gRFZLkupWRjnKgA7hjA5Iwc8d6LWeecz/AGi0e2EcpSMu6t5qjGHG0nAPPBweOlcN/YnxT/6G7Rv/AAW//Xo/sT4p/wDQ3aN/4Lf/AK9H1Gn/AM/4f+Tf/IhzPsdzYzz3Nmkt3aPZysTmF3ViuCQOVJHIweverFeff2J8U/8AobtG/wDBb/8AXo/sT4p/9Ddo3/gt/wDr0fUaf/P+H/k3/wAiHM+zPQaK8+/sT4p/9Ddo3/gt/wDr1hWWpfEO51240W58X6RY6lA+0QXGm485eoeM5wwI7dfUCpeDpRaviIa/4v8A5EiVVRaUluevUhztOBk44Fef/wBifFP/AKG7Rv8AwW//AF6P7E+Kf/Q3aN/4Lf8A69P6jT/5/wAP/Jv/AJEvmfY8bl8WeLf+Eua6a+vV1MXG3yAzYDbseXs6Yzxtr3X4l6lq2m/Dy7utK3w3WEEskR+aFCfmIP6Z7Zz71hnwf8QzqH286/4dN4Bj7R/Y48zH+91q02hfFJ1Kv4s0VlYYIOmZBFcVLKFCMovFQ1/xf/In1GPz6hiq1CrHDJKnutNdtPTTT1OH+DOua7ceMmsmuri6sZIXe4WVy6xkDhhnoc4HvmtH446zrVnq1hZ21xPbac9v5mYmKiSTcQQSOuBt4963rLwj8RdNV107xD4dtFc5YQaQEDH1OOtOvPCvxH1GDydQ8R+H7qLOfLn0kOufXBpLJ0qHsvrUL/8Ab3/yJpLiChLNVjvq/upWtp9/a/6Fr4Oapq2qeCnfWJJZxFcNHbzTElnQAdzyQDkZ/DtXf153F4f+J8EKxQeKtDjjQYVE0zAUegAp/wDYnxT/AOhu0b/wW/8A1666WXQhBReIg7f4v/kT5/HYqOKxM60KfKpO9ux6DXnnwlRYH8ZW0ACW8HiW7WKNfuoPl4HoPah/D/xRnQxSeNNLgR+Gkh0wF1HqMnGa6jwj4Ws/B/h+PS7GSSc72lnuJjl55W5Z2Pqf5AV0SVLDYapTVRTc7aK+ltbu6Xoji1ck7G3RRRXkGgUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUyaVYIJJpM7I1LNtUk4AzwByaAM/xFpUut+H7vT4LuS0knjKrKhIwffBBx6gHn6cVn+B/CieD/Dv2DzFmmeZ5JZQMbyThfp8oX8c1H4Ku9W1e2vNa1V3it7+XdY2bKB5MI4BPfLdfTv3rp6xjGE2qttf0OeEYVJKvbW2noFFFFbHQFcv43m1K6s4dC0ew+0T6puje4mi3QW0Y+8zZ4J54H/6jb8U+JV8O2cHk25vNQvJRDaWitgysTzz2A7n6etbiFjGpdQrEcgHOD9aylad4JmM7VE6afqc/wCD/B9n4QsZ4bZ/PmuZTJLOUClv7qgDgADt6k10NFFXCEYR5Y7FwhGnFRirJBRRRVFhRRRQAUUUUAFFFFABRRRQAUUUUAFc58QIrufwDq8dgVEht2LFjgCMcv8A+OhgPciujrJ8Vf8AIm61/wBeE/8A6Las6qvTkvIyrK9OS8mZvw3iuofh5pKXu0t5O5GVs5RjuX8gQPwrqKw/BX/IiaH/ANeEP/oArcpUVanFeSFQVqUV5IKKKK1NgooooAKKKKACiiigAooooAKKKKACuO8eeAYfGEdvPE8cF9bhlWR1OHUg4U454Ygj8eOa7GioqU41I8sloZ1aUKsHCaujnPBmszajpkllqNpLaalprC3uonLMCccMrHO4Ec9Sf0J6Oo7iRobaWVImmZELCNMbnIHQZ4yelZ3hzxBa+JdFjv7MMhJKSwv96GQfeRvcf4Uo+7aDeooNQtTk7s1aKKK0NQooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKzI9fs5fE0uhxeY93DbieUquUQE4Ck9m749K06yND8O2+hz6jcJLJcXGoXJnmmlxuPovHYc4+tRLmurfMzlz3Sjt1NeiiirNAqO4uIbW3kuLmVIYYlLPI7YVQOpJ7VJXO+L9CvfEltZabFMkWnSXAbUPmId415CL9SOfw96ibai2ldkVJSjFuKuzVOnadeajbav5MU1zFEVguAc4RueO3Pr7n1q7TY40iiWOJQiIAqqowAB0Ap1UlYpJIKKKKYwooooAKKKKACiiigAooooAKKKKAMnV/E+l6HdRW2oSTieaKSaOOG1lmLomNxGxT03DjrzVW38c+HrpZGivmCJbpchnt5EEkbnarISo35YgfLnkgdTTtY8PXGp+ILHU4b6OAWlrcW/lPbl95lCjdneMY2DjHPqK5x/hhLNZafbza2udP0yCyhkjsypEkMqSJLzIRjcgyvcZ5FC8/63/4H3g9tP62/wCCdQvizR3DgXEgmS4+ytbmBxKJdhcLsxu5UFhxgjpWNqfim11rSPE+n2lvcqLTSjKZpYmRXEkLMMZGRxjrg9eOKdf+Al1QXsmoXFncz6hPFJdeZZExlY4yiBB5m5GBO7eGzn2qtceFZ/D/AIf8Q3Laq98lxoohk8+PMrPFCy7y+7nI6jGc96ifwSv2/Qzq/wAN+jNLwzqdrpvgLw99qdg01nEkUccbO8jeXkgKoJPAJ/CrMnjbQI7Nbs3rG3MKXDyLBIRFGxIV5Pl+QZB+9jofQ1i6b4fudY8FeErzTNRGnahptvHLDK8HnRsGi2sjpuXIIPYgggVYvPA093NqTHWCV1ixWz1ISW4YyBdw3xkMAh2uwwQw6enKpfw1/XQKNvZx9EXpfHfh+G8e1e6n8xLr7IxWymZRNtDhNwTGSCMc/NnjNNfx94fS3t5ftF0/2kzLFFHYTvIWiOJF2BNwZT1BGe/Ss/8A4QKdJpDBqsKRNrEOpohsydoijRBHnzOchB836VmP4c1TR/GGiJaXHnmS81G7e5GnSNFB52GCNh8DnIBLDOOladP67L/g/caf1+f/AAPvOj/4T7w61jPeR3rS20FpHevLHC5HkyEqrjjnlWBHUYORRb+N9OlvNQt5obqE2d8LFMwMxnk8sSfIFBPQn8BmsK5+Fzf2bLY6brItYLnTBYXPmWnmM5Ejyb1IdQuWkbIweDxjrV+fwRqD3l3Lb65FHDd6gl9LbvZF0kIhEbRv+8G5TtVgOMEc7qOv9d/8g/r8P8y5F450y81GK1sRcSQz6adQS8SB2QR5wPlxnPXjHoOp4fD4y0e30a2ubvUzcKbSK5luUtXCiNx8srgA+WpwT82MYPYGszRvAFzoqWSwaxHJ9m0uTTW3WeN6ltysMPwQeo5z/s1Fa/DaS10uWwTWFMV7pMOl3+bT/WrGpQOnz/I21iOdw6H6nT+u7/Swdf68v+CbqeNdCk1Yaal1L9o+1fY+bWUIJtm8IXK7QSpyOee2aqeIvE8mgeLNLguHP9m3FpdTTLFavLLuj8vBATJxhjkAdutVF8BTxXLPDqsKRf2zFqiR/ZCSoSJYxFnzOeFB3fpWrrfhy41XXbPUYb6O3FraXNt5T25fcZgo3Z3jGNg4xz6ik72ut/8Agf56DW9nt/wf8rCz+NvD1u1sJNRXbdeV5brG7L+9/wBXlgMLuyMZx1HrUUPj7w/cRu8VzcFVdowTZTLvdSwZVynzEFGyB0xzWXongG+0G4tms9djMH2a3hu4nsAxlaFAgeNi/wC7JVVBB3dOMHmlHgG9i0mxittbijv7C/uLyC4NjuiYTM7NG8Rf5h85GQwPANU7X0JV7anV6XqlnrWl2+o6XOtxaXCb4pVBG4fQ8j6HkVbqvY28lrYxQzzefIq/PIECBj3wo6D0H86sUPfQFsFFFFIYVnWWiafpepX+o2kZhlvir3HznYSo+9t6A8nJ71o1BfWUGo2E9ldp5kFxG0ci+qkYNS0nrbYmUU9barYmVgyhlIIIyCD1pa57wXYarpOiNpesYkWylaK0n3gmaAfcJHYgcY9q6GiEnKKbVhQk5RTasFFFFUWFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUVX1C+g0zTbi+vH2QW8bSSN6ADNJtJXYm0ldnO6tcalf+PtK0qxe4trOzQ3t7MgKrL/CkeehGeSP8K6qq9hexalp1ve227ybiNZY96lTtIyMg1YqYRtd3vcinG15XvfUKKKKs0K+oX0GmabcX1222C3jaRz7AZrO8JajqGr+GLTUdXhignugZVjjBAWMn5M5PXbg/jV+9hsb+J9Ov1hnSZPmt5MHeueuPr3qyiLGioihVUYVQMACos3O99DO0nO99P1FoooqzQKKKKACiiigAooooAKKKKACiiigAooooAKKKKACsnxV/yJutf9eE/wD6LatasnxV/wAibrX/AF4T/wDotqip8DM6nwS9CHwV/wAiJof/AF4Q/wDoArcrD8Ff8iJof/XhD/6AK3KVL+HH0FR/hx9EFFFFaGoUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAYnizXZvDeh/2nFai5iimjFwN2CkRbDMB3IyOPx7VsxSpNCksTB43UMrA8EHoajvLSC/sprS7jEkE8ZjkQ/xKRgiksrODT7GGzs08uCBBHGm4naoGAMnmotLmv0M0pc7d9P1J6KKKs0CiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKx/FGgf8JLop0x7praGSVGm2rkyIrZKdRjOBzWxXMeF9Yvtc1zX52lB0u2uhaWiBR95B+8bPUgkjFZ1HF2hLqZVHF2py+0dMiLHGqIoVVGFUDAApaKK0NQooqG8uksrGe6m/wBXBG0j/RRk/wAqNhN2V2c9Y6Pey/EfUtb1CHZbxWsdpYHcDuU/M7YHQ7uOexrp6z9B1Ua5oNnqawNbrdRiQRuclQenP61oVnTUVG8euv3mdKMVG8euv3hRRRWhqFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABWT4q/5E3Wv+vCf/0W1a1ZPir/AJE3Wv8Arwn/APRbVFT4GZ1Pgl6EPgr/AJETQ/8Arwh/9AFblYfgr/kRND/68If/AEAVuUqX8OPoKj/Dj6IKKKK0NQooooAKKKKACiiigAooooAKKKKACiiigAooooAK5X7De2HxS+228MklhqdjsuHHKxSxn5SfTKnA/GuqrO1/W7bw7ok+qXySvbwbd4hUM3LBehI7kVnUStdu1tTKrGLXNJ2tqaNFIrBlDKcgjII70taGoUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAEF5e22n2r3N/cR20CY3SyuFVcnAyT05IqSKGKFSII0jViWIRQASep4rn/ABlol34htdOsbcJ9k+3RS3u5sExKclQO+Tj8q6OoTbk1bQzTbm01ogoooqzQKp6tp0esaPdadPJJFFdRNE7xEBgpGDjIIq5XMeONUvNNtdHj02cwTXurQWxZQDlGJ3Dn6VFSSjBt7GdWUYwblsb9hZxadpttZW+fKtoliTPXaoAH8qsUUVSVlZFpJKyCiiimMKKKKACiiigAooooAKKKKACiiigAooooAKKKKACsnxV/yJutf9eE/wD6LatasnxV/wAibrX/AF4T/wDotqip8DM6nwS9CHwV/wAiJof/AF4Q/wDoArcrD8Ff8iJof/XhD/6AK3KVL+HH0FR/hx9EFFFFaGoUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFUNd0qPXdBvdMmfy1uoWj34zsJHDY74ODV+ik0mrMUoqSaZX0+1NjplraNKZmghSIyEYL7QBnHvirFc54G1m81zw4bjU2VrqK5mgkKrtHyuQOPpiujqaclKCa2IpyjOClHYKKKKs0CiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDF0/xB/aHivVdHjtsR6akRe43/AHncZ24x2HfNbVZ+m6LZ6Vd39zah/N1Cfz52ds5bGOPQe1aFRDmt7xnTU0vf31/PT8AoooqzQKr3MlmssCXjwCR3/cLKRlmH90Hv9KsVzXibSb3UPEHhu6s4fMisbxpLht4GxSuM4J5/Com2o3SuZ1JOMbpX2/M6WiiirNAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACsnxV/yJutf9eE//otq1qyfFX/Im61/14T/APotqip8DM6nwS9CHwV/yImh/wDXhD/6AK3Kw/BX/IiaH/14Q/8AoAqbxXqN1pHg/VtS08Rm5tLOWeMS525VSecfSppu1NPyCgrwivJGtRXF3fje70p9OhvLCGYzfZEmkiuMkNPJsB2hTtAOD820Nkhc4NVx42vNQl+xXWlR21td3N9p6zw3zGRZIFc7sBFwpCHndkHt3rSXup+V/wAP+HNI+9bzt+J3lFcL8OvEN5c6ZpGj6jaqr/2HbXkVyLgyNKpG07wVG1sjPVs5613VXJWf9egk7hRRRUjCiiigAooooAKKKKACiiigAooooAKKKKAK9nZ2lkkiWMEUCvI0jiJQu5z1Jx3NWK5bwTp93Yz+I2vLeSEXGtTzQ7xjeh24Yexrqaim+aN7WM6UuaCdrBRRRVmgUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVneIrx9P8M6neRZ8yC1kdNvXcFOP1xWjVa/1Kz0uBZtQnWCN5FjVn7sxwB+NTL4XrYmfwvWxQ8Ii5Hg7STfzSz3L2sckkkrFmLMN3JP1xWxRRRFcsUghHlio9goooqigrL17XodAhs5LmKSRbu8jtBsx8hfOGOewxWpWV4i0GLxFp0drPM8PlXEc6PGBkMhyOtRPm5Xy7mdTn5HybmrRRRVmgVz2r+PvCmg3xs9X1+xtrlfvQtKCy/UDp+NaOvXcun+G9SvLcgS29pLKhIzhlQkfqK4z4V+FdFHw70u/uNOt7u+1GEXV1dXMSySSyOSSSzAnv/nmvQw9Cj7GVeveyaSSte7Te7vordiG3eyNP/hbPgT/AKGex/76P+FH/C2fAn/Qz2P/AH0f8K3/APhHdF/6A9h/4Cp/hXgGs/ES6h8VTmy0jSYrGCZkW0ewjO9QcfM2N2T7EVlWxWV0EnOM9fOP/wAievlmUYzNJSjQt7qu76HsH/C2fAn/AEM9j/30f8KP+Fs+BP8AoZ7H/vo/4VfFnoR8NDWP7Bs9n2T7V5X2VN2Nm7b069q8Q0j4iXUvimBr3SNKlsZ5lRrRLCMbFJx8rbd2R7k0quLyqk0pRnr5x/8AkSsBkuOx8akqNvc3v37L+rHsH/C2fAn/AEM9j/30f8KP+Fs+BP8AoZ7H/vo/4Voa3p+iaPoF/qX9h2Mv2S3ebyxbJ821ScdPavFfC3j25vPGFnb6npWlXFne3CQmBNPiXy9xwCpAzwT3Joq4rK6UlGUZ6+cf/kRYDJsbj6NStRtaG9+vWy/4Nj1r/hbPgT/oZ7H/AL6P+FH/AAtnwJ/0M9j/AN9H/Ct//hHdF/6A9h/4Cp/hR/wjui/9Aew/8BU/wrp5su/ln/4FH/5E8f3zA/4Wz4E/6Gex/wC+j/hR/wALZ8Cf9DPY/wDfR/wrf/4R3Rf+gPYf+Aqf4U19A0OONnfSdPVVGSTbJwPypc2Xfyz/APAo/wDyIWmYX/C2fAn/AEM9j/30f8KP+Fs+BP8AoZ7H/vo/4VotB4STRF1hrTShpzRrKtybdNhVsYOcd8iprrTPDVk9ul1p2mxNcyiGENbIPMcgkKOOuAfyp82Xbcs//Ao//Ih7/kZH/C2fAn/Qz2P/AH0f8KP+Fs+BP+hnsf8Avo/4Vv8A/CO6L/0B7D/wFT/Cj/hHdF/6A9h/4Cp/hRzZd/LP/wACj/8AIh75gf8AC2fAn/Qz2P8A30f8Kz/EHxP8FX3hnU7S08R2Us89nLFFGrHLsyEADjuTXX/8I7ov/QHsP/AVP8KP+Ed0T/oD2H/gKn+FRN5fKLSjP74//IkyjOUWrnF+F/iZ4M07wnpVlfeIrOC5t7SOOWJmOUYKAQePWr958Tfh5qFjPZ3niOwlt7iNopYy7YZWGCOB6Gul/wCEd0QdNHsP/AVP8KP+Ed0X/oD2H/gKn+FKH9nqCjKM3p3j/wDIhCM4RST2PPJNa+EMzK0mtwMyrCuf7QuPm8pt0Zb5vmZT0Y5PbOKlHiL4TL5e3XoAYria5Qi/uOJJQRI33u4ZuOgzxiu+/wCEd0X/AKA9h/4Cp/hR/wAI7ov/AEB7D/wFT/Crcsue8Z/+BR/+RK99HEaX4v8AhZo1zb3Gna/axSW1mtlEWu5nCwqchMMSDg9zz71sf8LZ8Cf9DPY/99H/AArYvdK8OabYzXl9pumwW0CF5ZXtkCoo6k8U2703wzY28c95p+mQxSSJEjvboAzOQqjp1JIFPny9/Zn/AOBR/wDkQtJdjJ/4Wz4E/wChnsf++j/hR/wtnwJ/0M9j/wB9H/CtebTPDVveW1rPp2mpPdFhBG1smZNoy2OOw5qx/wAI7ov/AEB7D/wFT/Cjmy7+Wf8A4FH/AORD3zA/4Wz4E/6Gex/76P8AhR/wtnwJ/wBDPY/99H/Ct/8A4R3Rf+gPYf8AgKn+FH/CO6L/ANAew/8AAVP8KObLv5Z/+BR/+RD3zA/4Wz4E/wChnsf++j/hR/wtnwJ/0M9j/wB9H/Ct/wD4R3Rf+gPYf+Aqf4Uf8I7ov/QHsP8AwFT/AAo5su/ln/4FH/5EPfMD/hbPgT/oZ7H/AL6P+FH/AAtnwJ/0M9j/AN9H/CuE+K3iQeG9ch0nQtK021PkrNJO1jE7NkkAAMpGOOuP5V1vw2udO8X+E/t1/oenJcwztBIyWiBZCAp3AY44YfiDXNHFZXKq6SjO684//InsVsmxtHAxx07ckvv12+8vf8LZ8Cf9DPY/99H/AAo/4Wz4E/6Gex/76P8AhXnnxP8AFTaD4pOkaHpWm2iW6I0krWETtIWGcDcpAGCPfOa73wC+l+K/B9tqV5oWnRXBZo5dlqgVmU4yOOh/nmiGKyudR01Gd15x/wDkQxGTY3DYOGNqW5J2t311V/U29D8ZeHPEkzxaFrNnfSoMtFFKN4Hrt6496268v+Kui6doen6T4l0aygsNUsNTtxHPbRiMujPtZGxjcpB6H+pr1CtcVQpRpwrUW+WV1Z7pq19t912PIi3ezCiig5xx1rgKM/RdbtNfsDeWHmeUJXiy64JKnB/CtCsbwnoH/CM+HINMa4+1PGzs02zZvLOWzjJx1x17Vs1EOZxXNuZ03NwTnv1CiiirNAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArn/Fuh3WvRaVDatGI7bU4bq4EjEbo0zkDjk8iugrG1PXzp/ijRdHW2806p55Mm/HlCNA2cY5znHas6nK42lsZVuRwtPZ2X3s2aKKK0NQooooAKraiLltLuhYPsujC4gfAO19p2nB4POKs0UnqrCaurGF4K1mTX/BunahcPvnki2zNgDMikqxwOmSCfxrdqrbPYQTNYWjW8csa+Y1vEVBUE9So6ZPerVTBNRSbuyaaagk3dohu7WK+sZ7S5XdDPG0Ui5xlWGD+hrzPRP+FheAtMTw/a+F4PFGn2hKWd7DqMdq/lZJVXVweRnHHH1616lRXfh8W6MHTlBTi7OzvuuujT69xuN9Tz3/hM/iJ/0S1v/B/b/wDxNcpfaLrOo662rXXwbLTu/mSIPEUIjkb1K4x9ex75r22itJYnCz0lhYP51P8A5M3oV8Rh25Uakot6aOx55/wmHxC27f8AhVh24xj+3rfGP++a4maO68K6gfEN38Jo7Pyn8xPN8SQ+Wj9cqmOT6AZ9h0r2DxL4lsfDWnia8Z3nmOy3toRulnfsFH9elcZ4V8E6zfeKP+Ei8byvM8ID2dtLIH8tjyMgAKNvHAA+bnjHPPWx2HclCOEg361NP/JzlWOxWHbpYack5aOzsredvyJ28Y/EC4gKv8Ky8ci4KvrkHzAjoQV/Q1zGk6Rrmi60NUsPg6y3CndHv8RwssR9VBH/AOrtXtlFdDxOGk05YWDt51P/AJM6qVfEUYyhTqSSlvZ2v6nnv/CZ/ET/AKJa3/g/t/8A4mj/AITP4if9Etb/AMH9v/8AE16FRV/XKH/QND76n/yZz8r7/l/kee/8Jn8RP+iWt/4P7f8A+Jo/4TP4if8ARLW/8H9v/wDE16FRR9cof9A0Pvqf/Jhyvv8Al/kea3nxC8c6esTX3w18hZpkgjLa9B88jnCqAF6k/wCJ4FWP+Ez+In/RLW/8H9v/APE1T8W3Wpap8W9A0pLdxY2Ekd42053fOB5hA6AH5Rn37EV6dWFPMKE5yX1aFlpvU/8AkznpTlUnNXdk7f1oee/8Jn8RP+iWt/4P7f8A+Jo/4TP4if8ARLW/8H9v/wDE16FRW/1yh/0DQ++p/wDJnRyvv+X+R57/AMJn8RP+iWt/4P7f/wCJo/4TP4if9Etb/wAH9v8A/E16FRR9cof9A0Pvqf8AyYcr7/l/kee/8Jn8RP8Aolrf+D+3/wDiaP8AhM/iJ/0S1v8Awf2//wATXoVFH1yh/wBA0Pvqf/Jhyvv+X+R57/wmfxE/6Ja3/g/t/wD4mj/hM/iJ/wBEtb/wf2//AMTXoVFH1yh/0DQ++p/8mHK+/wCX+R543jX4hIhZvhcwCjJ/4n0H/wATSReN/iBPCksPww3xuoZWXX7chgehHy16GQGUg9CMGvN/g34jk1HQJdHu97SaecQyEHDxHoM+q/yK1jLMcPGpGDw0Nb9anT/t8wnU5KsabfxX7dLeRP8A8Jn8RP8Aolrf+D+3/wDiaP8AhM/iJ/0S1v8Awf2//wATXoVFbfXKH/QND76n/wAmb8r7/l/kee/8Jn8RP+iWt/4P7f8A+Jo/4TP4if8ARLW/8H9v/wDE16FRR9cof9A0Pvqf/Jhyvv8Al/kee/8ACZ/ET/olrf8Ag/t//iaP+Ez+In/RLW/8H9v/APE16FRR9cof9A0Pvqf/ACYcr7/l/kePeJU8T+LTE2s/CN3lhGElj8RQI4HpkDkfWtPSNb8a6FpkWn6V8Jfs9tF91F1+A/UkkZJ9zXp1FQsThVLmWFhf1qf/ACZ0Sr4idJUZVJOC2V9F8jxvxJbeJPFdxFcax8IneeIbRLH4ihRiv904HI/lWzpviHxxpGnQ2Gm/Cb7PawLtjjTX4MAf988nPOT1r0uihYnCqTksLC786n/yYTr4ipSjRnUk4x2V9F8jzG807xp8Q7/TrXxLoMHhrQ7O6S7uIzfJczXbIcqgKYCrnrnmvTqKKyxGKddRioqMY7JXtrvu27vzZgo2CsHxrrk3h3wnd6haBWulKJAjDIZ2YADHfrn8K3qq3tlZaiqQX0MVwI3WZUkAO1geGx/WuGabi1HcVRSlBqLsyeDzfs8f2jb5uwb9vTdjnHtmn0UVZYUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFZ9zolpd67ZavL5n2mxSRIcN8uHGDkVoVy11qF2vxUsNPS4cWj6ZJK8IPyswfAOPWs6jSSuuqMqsopLmV9UdTRRRWhqFFFFABRRRQBzd7ol4vxC07XNPVPJa1ktb8FsHZ95CB3O79BXSUyZpFgkaBBJIFJRC20MccDPb61k+FPEA8S+H4r8xeRPuaK4gzkxSKcFf5H8azXLCVur1MY8kJuK3ev5GzRRRWhsFc94n8WRaEYrKygbUNYuuLWxiPzN/tN/dUevt9SKviLxZPFqA0HwvCt9rcg+bP+qtF/vyH+n/1gbfhjwnBoAlu7mZr/Vrrm6vpfvOf7q/3V9v/AK1YSnKb5Kfzfb/gnNKpKpLkpfN9vTu/y69ir4c8Jy218dd8Szrf65KMb8fu7Vf7kY7fX/E56qiitIQjBWRrTpxpxtEKKKKs0CiiigAoorM8SaqNE8M6hqRIBt4GdM93xhR+JIFKTUU2yZSUYuT6HPeDf+Jr4s8Ta+eUa5Fhbn/YiGGI9iSDXaVz/gXSjo3gjTLWQESmESy5673+Y5+hOPwroKyopqmr7vX79TLDxcaSvu9X6vUKKKK2NwooooAKKKKACiiigArirRE0H4tz20aiO11uwWRFUYHmw/LgD/c5rta4z4ig6fBo/iJBzpF+jSkf88ZPkcfjlawr6RU+2v8An+BzYnSCqfyu/wDn+Fzs6KAQQCDkHoRRW50hRRRQAUUUUAFFFFABRRRQAUUUUAFcvo+lXsnj7Wtc1OAxII47OwywOYh8zNx6tz+daPibxDF4a0f7bLC1xI8qQw26HDSuxwFHvjJ/CtZSSoJGDjkelZyUZyS7amMlGc0r6x1/MWiiitDYKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACq7XdomoJaPPCLt0LpEWG9lHUgdcVYrnL/RryX4h6VrECKbWC0mhnYsAQTyvHfmom2krIzqSlFKyvqjo6KKKs0CiiigAooooAKoWWl6fpEl5PaRrAb2fzpyXOGkPGcE4BPtV+sjxVolv4g8N3VjcyiDK745848l15V8+xHPtmoltdK7RE9uZK7Wxr1xer+Jb/XtTl8P+CmUyxnbe6oRmO0HcL/ef+X5kYuk6t4l8f6XBpsLfYLOIeVqWqwtk3BBwVhPuOSenPp19B0jR7HQtMisNLt1gt4xwq9Se5J7k+tYKbrr3dI9/wDL/P7jlVSWJXuaR79X5L/P7u5V8O+GrDwzp5trBWaSQ757iQ7pJ37sx71r0UV0RioqyOuMYwjyxVkFFFFUUFFFFABRRRQAVxnxD/4mJ0Tw4vP9q36+cvrDH87/ANK7OuLs/wDic/F2+uPvQaHZJbp6ebL8xI/4Dwawr6xUO7t/n+BzYnWKh/M7f5/hc7Tp0ooorc6QooooAKKKKACiiigAooooAKzvEGlrrXhy/wBNbH+kwPGpPZiPlP4HBrRopNKSsxSipJxfU53wDqjat4H02eXPnRxeRMD1Dxnac+5xn8a6KuL8I/8AEp8Z+JtBPEbTrqNuPVZR8+PYMAK7SsqDbpq+60+7Qxw8m6ST3Wn3aBRRRWxuFFFFABRRRQAUUUUAFFFZHii51W28PznQLVrm/kxFEAQBGWON5z2Gc/8A1s1MpcqbJlLli5diXUtCs9V1HTry88xn06UzQoGwhYjGSO+Oo9DWlWb4d0j+wfD9pppne4aBMPK7El2PJPPbJOB2FaVEVpe1mxQWnM1ZvcKKKKosKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACsnVtfTStY0mwkgaQ6nK8SyBsCMqueR3zWtWZqug2+r3um3U8kscmnXHnxeWQNxxjByDxUT5re7uZ1Ofl9zfT89fwNOiiirNAooooAKKKoa1rdh4f0uS/1WdYYI/Xqx7Ko7k+lJtJXYpSUVd7Fm8vLbT7OW7vpkgt4V3PI5wFFcMF1D4lTBpBNp/hVWyE5SXUcdz/dj/n/KSz0bUPHN5FqviqFrXSI232WkMeX9JJvU/wCz/wDXz3SqEUKoCqBgADgCuezrb6R/P/gHLaWI30h+fr5eXXqYHiG4vPDXhuObw5p8MsFkymW1VcHyB97Zj+IdfzrV0vU7TWdLg1DT5RLb3CbkYfyPoR0Iq3WXqmp6d4U0T7TPCYLKFlQrbw5EYZsZwOgGck/1rX4G5N6WNmuRuTfu2+41KKZDNFcwRzW8iyxSKGR0OQwPQg0+tDUKKKKACiiigAooooAZPNHbW8k8zbY4kLux7ADJNcl8NIZJPDc+sXK4n1i7lvGB6hS2FH0wMj61L8SL2S18FXFta/8AHzqTpYwj+80hwR/3zurotOso9N0u1sYP9XbQrEn0UAf0rD4q3ovz/wCG/E5vir/4V+L/AOAvxLNFFFbnSFFFFABRRRQAUUUUAFFFFABRRRQBxfiX/iUfETw5rI+WK736ZcH13fNGP++s/lXaVzHxF059Q8D3zW+Rc2YF3Cw6q0Z3ZHvgEfjW3pGopq+i2eoRY2XUCSgDtkZx+HSsIe7UlHvr+hzU/drSj3s/0f5fiXKKKK3OkKKKKACiiigAoopssscELyzOscaKWd2OAoHUk0AV9T1K10fTJ7/UJRFb26F3c/yHqT0Aqn4Z1W81vQ4tQvrD7CZ2Zooi+5vLz8jHjgkdv/1UsT6P4v0OOYJFqFhK29PMQ4JVuuD6EVq1mruXMnoZLmlLmT9233hRRRWhqFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFGaACiiigArnPH9/f6X4H1C+0mcwXVuEdXCg8b13Dn2Jro6bLHHNG0cyK6MMMrjIP4VE4uUXFO1yKkXODina4kEy3FvHNH92RA6/QjNPqG0urW7tUmsZ4ZoGyEeFwynBwQCOOCCPwqaqWxS1QUUVzvifxbHobRWFjA2oazdcW1jGeT/tMf4VHr7fUhSnGCvImc40480iz4k8T2PhmxWa73SzzHZbWsQzJO/ZVH9axdF8MX2r6pH4h8abZLtObPTgcxWQ9f9p/f/62LXhvwlLaXza54jnXUNdmGDJj93bL/cjHYe/f8TnqaxUJVHzT26L/AD/rQwjCVV89Tbov1fn+QUUUV0HUFMmhjuIXhnjWSKRSro4yGB6gin0UAZOi6NY+FdHktraeRbON3mH2iXKwqeSoJ6KP8a0re4hu7aO4tZUmhlUMkiNlWB7g06SNJo2jlRXRwVZWGQwPUEVgaB4UXw3qV0dNvpl0ucbk05xuSGQnJZGPIHt71nZwajFafkZWcGoxXu/kdDRUcdxDLLLFFLG8kJAkRWBKEjIBHbjmpK0NQooooAKKKKAOL13/AInHxP0HSx80WmxSalOP9r7kf4hufxrtK4zwV/xNfEniXxA3KTXYs7c/9M4RjI9iTn8K7OsKOqc+7/4CObD+8pVP5n+Gy/IKKM8470VudIUUUUAFFAOelFABRRRQAUUUZ5x3oAKKKKAGyIssbRyKGRwVYHuDXH/DaRrXR77QZmJl0a+lthnqYydyN+OT+VdlXFxn+x/jA4HEGv2AYf7U0P8A9h/OsKnuzjP5ff8A8GxzVvdqQn8vv/4NjtKKKK3OkKKKKACiijIBAJ5PSgBskiQxtJK6oijLMxwAPUmsnxJ4eTxLYRWNxdzQWvnK9xFEcfaEH8BPUDOOnpVHxB4XuvEurQx6jf7dBjUM9jCCrXEgPR2/u9OB/TNdJHGkUaxxKERAFVVGAAOgrLWd4yWn5mNnU5ozjp+f/AGwQRWtvHBbRrFDEoREQYCgdABUlFFam2wUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVyvjrW73TbfStN0iQQ3+tX6WUVwVDeQpBZ5ADwSFU4B4yRXVVla/4ftfEFtbpcyTQTWlwtza3MBAkglXOGGQQeCQQQQQTR1Xy/wCCHcp3eh2umRw6nHdXwOmh55d13I/2kCNhiTLYbkhhkcEcYqnb6xLoHhjRLy5tftLanPAL2cSBTHJOVG/BHIDMBjsAPSt+PTXa3ljv7yW882MxsGCqu08H5VA/M59sVkQ+HU1Xwdb6DriybbN4UZlOPO8l1ZGB9GCrn0yRQt/u/PX9A6ff/wAD9R+meL4dYlX7Das0P22a1kkdwpRY03ebjup+Uf8AAga6FWV1DIQysMgg5BFcvqfg6I6RrcGmBWm1mYNMLh8IqHYrquAduUU9uuPbG3Jp8rFRb6jc2kaqFWGFItqgDHG5Cf1o6L5flqHX7/8AgF6mrIjs6q6syHDgHJU4zz+Brk/+EUv38Yrq81zGyC4MgIch0QW/lJgbcElmdj2+71rW07Sry11S8nm1G6ljkkVlVxFiQCNRztQEcjtjpQBy/hfT5f8AhX2rQQNsS3vrt9NmwQU2OdrYPIIdWBB7exrtdIvhqmi2OoBdoureOcL6blDY/WuMhu7/AErzvBz2qfb9Qiurm2uYpS0Y82dzg5UHKq+4/TAzkU651m71Fk8KeAmAWzjWC61QjMdoijG1f7z4H4fmRzxnClDlWuyS89b/AKHL7SnRgox1tdW69LI0PEXiy4TUP7A8Kwrfa3IPnJ/1Vmv9+Q/yH/1gbnhjwnb+H1luZ5mvtVuvmur+X78h9B/dX2q14d8N2HhnTvs2noWdzvnuJDmSd+7Me9a1ONNt89Tf8v67jhSk5e0q79F0X/B8wooorc6QooooAKKKKACiiigDmdW8FwXesJrOj3kukapuHmz24BWde4dDw3Hf+dbWpatYaPbpPql3FaRSSCJXlbaCx6DP4GrlQXlla6jaPa39vHcwSDDRyqGU/gaz5FG7huzL2ajd09G/uJkdZEV42DKwyrKcgj1paytA8OWPhq2mt9LM628snmLFJKXWLj7qZ6Dv+NZd147tNK1qSw16wvdNTzNlvdvEXhn9MMucE+n50OooxTnoJ1VCKdTT8vvOprJ8U6t/YfhXUtSzhoIGMf8Avnhf/HiK1q4z4gf8TK60Hw4vP9pXwedfWGL53H8vypVpONNtb/qFebhSbW/T1eiKxvJPh18Fku44BLd21qrLG/Rp5WGA3tvcZ9hWxF4SabRoY9Q1S+l1LKPPepcOhZgwZlVVICocEbQMYPetbW9GsvEGi3WlapGZLW6TZIoOCO4IPYggEH1FMsNMubRY1utVur0RDC+aEUtxjLFVG4/kParpxUI8q6WLjFQgoLZGBFrc1l4X1vxSLQ30sc1wfJ8zYRBA7IFUkHsjNjuWNW18aQXN9e2mm2r3MtoLUnLhBJ5zlflPfbtOfcEcYqWz0TGm6xoVyri0unneKVf7k5ZmHsyszfht96LbwlZaXdNfaWp+0pp6WcEcr5jGzeVY8ZLEyHLcnBPqaey+40er08/zVvwubySJIu6N1dckZU55BwR+YrP1LX7DSbu3t755ka4dUV1gdo1Zm2qGcAquW4GSOaq6NoEumeHNO06K8ksmtYFSQWqxlXfHzNl0PU5PbrTNUsdZnubG3hhtL+xidZZpLu6MUrOrZU7UhKkLgEDK5IHTHNbSsT0uOt5TZeOZ7BP9Re2f2wJ2WRHCOf8AgQdPxBPet2si2spJ/FVzqsyMiRW4s7cNwWG7dI30J2gf7pPQitekvhX9ddPwsN7v+un+YVzHjzXrzRdHs4NIKpqOrX8On20rruWFpCcyEd9qhiB64rp6y9f0C08RaelreNLE0MyXEE8LBZIJUOVdSQRke4IIJBFLt/WnUCq/hq2t1tLhb2/8yykE0sjXUjNcgKflf5sEZIOMYGOAKzLPXZdI8Fabrs9obptUlglvHEm0xee6qDyOQu9Rj0WuktbGaIH7ZfTXjFSvzqqLj6KBk/X8MVjW3htL3wWPDGrCUQWxSASIdpljjYNGQfdVUHuDn2NP/gf8H9A/r+vxJLDxdFql3JFp9o8qQ6kbGR2cLwIRJ5oHdeQPcHNdAjrLGrxsrowBVlOQR6g1zWo+D4hp2vjSAou9YTYRM2I4h5aREKADj5UB6HJA9K1/7MdIoorS+nsYYo1RYLdItigDHG5CaOn3f8EOv9f1/wAMTNqVsmrR6YzsLqSFp0TY2CilVJ3Yx1YcZzzXnPiyR9K1CW6hB8vw9qNteoB/Bb3AKyIPbcpPsDjtXcPpd2fF1jqIeN7a3sJraRnfEjO7xsDtC7cfuznkdelUZPD7avY+IXvEMb6wnkxK45SNEKoT6HcWf23D0rKpFzptLfX89P0Ma8HOnKK+Xr/w50qsHUMpBUjII7ilrmvh9qb6p4G055s+fAn2aYHqGjO3n3wAfxrpCQqlmIAAySe1VCSnFSXUunNTgprqLR061ztr450XUddj0rSZJtQlYnzJrWIvDDwTln6YOMcZ61Y8R+G4/EsUFvdX13BaIxM0FvJsFwD0Vj1x/jS9opRbhqT7RSi3T1NSC6gvbYy2NxFMhLKJI2DruBweh7EVzuh+E7mDVxrniPUpNS1UBli2EpBbqeCqJ9O5/nzW9pumWWj2KWemW0dtbx/djjGB9fc+5q1RyKVnLdB7NT5XPdfdf+tgooorQ1CiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAK5nx14sk8KaRbmws/t+qahcpZ2Fru2iSVuhY9lGMn8Omc101ed/FCRLTX/A17csI7WHW1WSVuFQshAye1d+XUoVcTGM1dau3eybt87WJm7RI7mL4pWljJe6j4i8L2USLvlzbSbIh6bjWdoer+PPEhlGieMfDF00X30WylVlHrg4OPfpXW/Erw9feJvBVxYaUQbkSLKsZbaJdp+7k8e4z3ArgfhL4D8QaR4qbVdWtZLC3hhePbIRulLcYwOw659h+GFTNq8ayhGjCz68iPfwuWYKrltXE1K9qkdo33+W7v5bG7qZ+JWjWD3uqeKvDNrbp96SS0kAz6e59hUGiX3xC8R2rXGi+LvDF3Gh2vsspAVPuDgj8RWt8XPC2qeJ/DdsNGXzpbSbzGt9wBkBGMjPBI9PQn6VjfBzwZrXh+6v9R1mFrNJ4hFHA5G5znO4gdMYwM+poebV1XVNUYcvfkX+QU8swUsqli5V7VU/hv59t/O+w3WvAnxG1+5t7jUPEGjLLbRyRxvbQyRMA4AYEjqMDp7mreleFfiVomnR2Ol6x4at7eP7qLZScn1J6k+5r0aaK6e8tnguUjgQt58TRbjLkfLhsjbg89DnpxQIroak0puUNoYgqweV8wfJy2/PIIwMY7da1jjOWo6ipQu/7q/r7vmfNqhBSc1uzhv7M+LH/QweHf8AwCkrD1fX/G+g30dnq/jXwtbXEgBEbWcpIB6E4ztHucV6rbRXUc1y11cpNG8m6BVi2GJNoG0nJ3HIJzx1xjivC/iH8OfE1744vL3TrKS/t72QPHIjD5eANrZPGMYz0xj8M8Tm1anBOFKDf+Bf5Hv5Ll+ExuIdPFVeSKTe6V36vTzO5jsPirNEskXiPw28bgMrLaOQwPQg1a8MeKteh8XSeE/G9tZrqLWxurO9sd3k3UYOGGG5Vx6fX2zv+ENIuNB8Iabpl7KJZ7eELIwOQDknAPoM4/CuV1eRLn9oLw7DbsJJLPSrmS4VefLViApPpk16GGrrGQnCrTivdbukk00r7rz0179zyK9ONKq405XSdk+6vv8AqeiUUUV4wgoqlG2qf2O7SxWY1LY+yNZWMJbnZltu7HTPHHOM0+5bUBp6m0jtmvPk3JLIwj6jfggE9M445OM4oAtUUUUAcN4m8Va/P4uTwn4Ht7NtQS3F1e3t9uMNrGThRheWc9cfT3xg65qnj7w3HG+t+MPDFp5p+RWspWZvXCjJx71p6NIlt+0B4lhnYRyXel2ssCtwZFXKsR64NYHxf8D69rXiG31XSLZ76A26wtHGRuiIJPT0Oe3fOa9TH4qWApwjQpRleMXdxu22rvfttp29T0cnweHxuLVLFVOSOut7bdLvT7zasYvidqdlHeaf4o8M3FvKMpLHaOQ1Zeta5420O6js9a8Z+FbeaUAiJ7KVjjsTjOB7nFdX8LfDeoeF/BotNX+W4mnafyd27ygQoC5HH8OePWvOPiP8O/Et944u7/TrKS/t71laORGHyfKBtbJ4xjr0xiuGrm+IjRU40YXfTkX+R6OByrAV8wqYarXtTje0rpXt5vT/AD6HWXeifFPV9LMcfivQo4pwrLPaQSK2Mggqw7H9RVK28EfE+DWYdTn8T6Nd3VvC0MMlxbuxRScngAc+55rvvBmj3OgeDdN0y+kElxbxYkIOQCSTtB9BnH4VqSxXTX8EkVyiWyK4mhMWWkJxtIbPy454wc57YroWPlNKUqUL/wCFHzmIw1KNaSjLmSej72ej/U4b+zPix/0MHh3/AMApKP7M+LH/AEMHh3/wCkruY4roahNJJco1qyKI4BFhkYZ3MXzznI4wMY75otorqOa5a6uUmjeTdAqxbDEm0DaTk7jkE5464xxWv9oz/wCfcP8AwCP+RPJ5nmGuap4/8NxRya34w8MWglOEDWUpZvXCjJx71csv+FoX+npf6X4j8LX8LjdHi2kCSY7ZHI9Pasj4weB9d1vXrbVdItnvofs4haKMjdGQxOcdwd3b3zXXfCzw1qPhfwebXV/kuJ7hp/J3BvKBCgLkcZ+XPHrXJDNq7ruDow5e/Ij6DEZZgqeWQxUK96jesbr8t1buy94F8WSeK9HuHvrP7Bqen3L2d/a7twjlXrtPdTnI/rjNdNXnfwtkS71rxve2zCS1m12RYpV5V9qgHB7j3r0St8xpQpYqUIKy0du10nb5XseBBtxCiiiuAoKKKKACvP8AVfFHijXfF9/4e8BxafBHpQQX+pagGZRIwyI41XqcdSffpxn0CvO/h7IkPxC8fWUrBLn+0o5xE3DGNo+GA7j/AOtXp4GMFCrWcVJwimk9VrJK7XW1/QiW6Rka1rnjnw9cRW+s+NPC9rNKMrG1lKzY9SBnA9zxWtBZ/FO6t457bxL4alikUMkiWjlWB6EHuK5D4nfD7xHqPja41LTLOS/trzZsMbDMZChdpBPHTOenNep+BNEu/DvgnT9M1Fw9zCrFwDkIWYttz7ZxXJSzavKrKEqMEl15F/kfQY7LMFQwFHEUa/NOVrxvtpr5qz013OBv/EfjTTNXGmX/AI48KQXhIBja0l+UnszdF/Eit3+zPiweniHw5/4ByV5x4m+F/iqbxle/ZbF7uG7uXljug42kMxOWJPBGec/hmvfdGspNN0KwsZpfOktraOF5P75VQCfxxRQzavUlJTowSX9xf5BmuWYLCUKM8NX53Jaq+33bej1PM7Twf8UtLN1HpGv6FapeTtczv5LN+8brtUoQBwOM1pQaN8WYLdIm8S6BMVGC8lm5ZvrjFd9FFdLfXEk1yklu4XyYRFtMWAd2Wz82TjsMY70WsV1H9o+13KT75S0O2LZ5acYU8ncRzzxnPStqeMdKPLGnDr9lPd3/AK+4+cjRjBWTf3nn9p4f+J1hbiCx1jwvbQr0jh090UfgKyP+Ej8af25/Y/8AwnHhT7dv2eV9kkxuzjbu+7nPGM5zXq1nDdR6dHFfXS3FyExJPHF5YY+oXJx+Zr51Pwl8W/8ACRfYvsZ2eb/x/bx5e3P385z746+1Y4jNq9LlVOjB/wDbi/yPoslyzA4xVPrVb2fKtNUvz7dlrqenTW3xbsoWuItR8Oai0Y3fZfs8kZl/2Q2eD6Z4rqPBviiDxj4VtdYt4Wt2l3JNbuctDIpKsp+hH5YraLLb25aaQBI1y8jnAAA5JNcB8Ff3ngKa5UHybrU7qaFiMb0MhAI/I13znHE4OdWUEnFxSaVt1K6dtOl+58/bllZM9CoooryDQKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAqhrWiad4i0ifTNZtUu7OcYeN/wBCCOQR2I5FX6KqM5QkpRdmgPPV+DmlxqEt/EvimCJeEij1VgqD0Ax0pf8AhT9h/wBDX4t/8Gx/+Jr0GivR/tXG/wDPx/gR7OPY8+/4U/Yf9DX4t/8ABsf/AImj/hT9h/0Nfi3/AMGx/wDia9Bopf2rjf8An4/wD2cex41qfgmxsPHmj+Hl8UeKCt/FLJK7aq25MA7MHHQlWB69q6T/AIU/Yf8AQ1+Lf/Bsf/iaj8S6FDdfGPw7cPPMskkbyDa2Avk4ZVHsSWz6hjXotc9HNse5TvUe/l2Ry0E5TqKS2en3I8+/4U/Yf9DX4t/8Gx/+Jo/4U/Yf9DX4t/8ABsf/AImvQaK6P7Vxv/Px/gdXs49jz7/hT9h/0Nfi3/wbH/4mt/wr4H0Xwetw+lRSyXV0Qbi8upTLNNjpuY9vYYFdFRWdXMMVWg6c5tp9BqEU7pBRRRXCUFFFFABRRRQBzvirwNovjD7PJqkU0d3akm3vLWUxTQ564YdvY5FYH/Cn7D/oa/Fv/g2P/wATXoNFd1LMMVRgoQm0l0JcIt3aPPv+FP2H/Q1+Lf8AwbH/AOJo/wCFP2H/AENfi3/wbH/4mvQaK0/tXG/8/H+AvZx7Hn3/AAp+w/6Gvxb/AODY/wDxNcvH4NsX+J0vhj/hJ/EwijshN5n9qN5hlyDtzjG3Yc4x1r2mvPYdEsx8d5bna/nLpf2sOXP+sL+X+Ww7cf15rmxGaY68LVHujlxEWuTl/mV/QX/hT9h/0Nfi3/wbH/4mj/hT9h/0Nfi3/wAGx/8Aia9Borp/tXG/8/H+B1ezj2PPv+FP2H/Q1+Lf/Bsf/iaRvg5pcqlLjxJ4pnibh4pdVYq49CMdK9Cop/2rjf8An4/wD2cexR0XRdO8PaTBpmjWqWlnAMJEnb1JJ5JPcnk1eoorzpSlOTlJ3bLCiiipAKKYs0TuyLIjMpwyhhkH0p9ABXLeJ/h5ofirUItRuvtVlqUK7FvtPuDBNt/ukjqPqK6mitqNerQnz0pNPyE0nozz7/hT9h/0Nfi3/wAGx/8AiaP+FP2H/Q1+Lf8AwbH/AOJr0Giuz+1cb/z8f4E+zj2PPv8AhT9h/wBDX4t/8Gx/+Jo/4U/Yf9DX4t/8Gx/+Jr0Gij+1cb/z8f4B7OPY8V0LwbY6v4417RD4o8TCPTvL8h01Rg78Yk3HHOGwBgCup/4U/Yf9DX4t/wDBsf8A4mjwjolnZ/FfxdNAJBJB9n2sXJ3ecnmSZ9csAfbHHFeg1zYXNMcoO9R7y/NnLhYtwbn3l+bPPv8AhT9h/wBDX4t/8Gx/+Jo/4U/Yf9DX4t/8Gx/+Jr0Giun+1cb/AM/H+B1ezj2PPG+DOizjy9R1zxJqFsT89tdaozRyD0YAA4/Gu8sbK202xhsrCBLe2gQJFFGuFRR0AFT0Vz18ZiMQkqs20hqKWwUUUVylBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVS1OzuLyGNbbUJ7HY+92gVCzjB+X5lIAyQemeKu02RDJEyB2QsCNy4yPcZyKT20GjgvB+takngfTPEWs6jf6o13HslgEUIRGMmBJuCrsUAHJJxz69YW8eSz+ILDU9MtdTv9MuNHuLhrCARb1aOZFL8sASPmHDHPYGt238CWdr4W03Q4dS1FYtMuFuLe43RGQMpJAPybGHzHqv6gGmWfw/sLCOBbTUtSjaCGeAPvjLNHM+9kOU/vDgjB96b3uvP8v8ANhpb7vz/AMiinjGKLxRcXzXk82kT6PZXVrAFHLzSuq7Rwct8nU8e1dPoetJrlrNMtnd2bQzNC0d1FsJIx8ykEhlIPBB/UEVj3vw90i9aUGa8hhexgso4YJFQW6wOXiZDt3BlY5ySRx0rd0rTm0uy8iS/u9QkJ3PcXbKZHPT+FVUcDoAPzzT0t9/5k63+78v8y7RRRSGcB4z1CXS/iJ4dvYbG4vzDa3bG3tQDK42jO0EjJA5xnJxxzxW34e+IHhvxPMLfS9RX7Xjm1mUxyD1GG649s1S1z/krHhf/AK9rv/0AVpXvhGzlbVJ9Lmm0i91SNFnurMhWLISVf68kE8ZB+hqcP7Nualvff5LcnDOk3UU1rff5Lf8A4Bv0Vipe6nYatBYXVm0+mx2W+XV5JkX94v3tyDpkYORx19Kv6ZqljrOnx32lXUV1bSj5JYmyD7ex9q0cWtTWUGlfoW6KKKkgKKKKACiiigAooooAKKKKACiiigArgNS1NdI+Ll1eyW1xcRR6CpkW3Te6r5/Lbepx7c139cbF/wAltm/7AK/+j6xq2vG/dHPWsnC/8yN/RPEej+I7X7RomowXiYBIjb5kz/eU8r+IFadcZceALfTtS1TW/CkNlbavdophN1BvjhkBJZl7ruBwcex9q3LXX4DriaDeFhqq2i3MmyFxEw6MUY9gffuO+a7Jwi9aeq/E7504PWlqvxRyni/ULzw/48sddjupxpltDHFqVt5reV5UsjJ523OAUYIc4+7uqpBr97/wsXVdVeaSexj0M3NlZvdeVCVWVlDksdgL7Sdx6Kw9K7u98PaZqM11JfW7T/a7Y2k6PM+x4jn5dmdvc84zz1qneeCdAv7iaa6sWdprIWDqLiVU8gHIQKGAGDyCACD3rBaW+f4p/wBfeYuzb+X4W/4P4eZzz+OdVvbqzt7Czt7d/wC3Bptx5rPh1Nv5wK5QEZ4HI7cdc05vF97qGjwX9/oVulm2rxWcezUX3iQXRi34EY4BVWxnnJBx33V8DaAm4i1nLNcpdtI17MXMypsD7i+c7eDzyOuakXwdoi6VHpotpvssd39tRPtc2RNv8zdu3Z+/zjOM9qa6eq/S/wCpLvb+vP8A4BS0fxTfah4wv9Au7G3t5rAl5WW43b4WA8t0GASSSwYHG3A5ORVbx5e3UmpeHPDtrNJbRa3fNHdTxsVbyY42kZFYcgttC5HOM1tR+FdIi1G0v0glF1aNK0U32qUt+9OXDfN84JA4bIGBjGKs6tothrlvFDqMJkEMomhdHZHikHRlZSCp5PQ9yKOiH1diFfDOjx31jeQWEEE1gGFu0UartDLtI4HTBrVqtaWEVnllaWWRhgyTSl2I9MnoPYVZoAKKKKACiimSyxwQvLPIscaKWd3bCqB1JJ6CgB9cf4i+Jei6Fqf9kW6z6nrLMESxtIySWPQFj8o/DJHpWrcahe3+rTaTZ2dzBay2JkTWY2QojtwoQHO49T+A4wc07S/DNrp7WNxdTTajf2VsbeO9u23SbScsfqeBnrgYz1ztFQjrU18v6/4c6IKnDWpr5f5/1c53wTJeS/EDxfJqcMdvdOti0kMb71jJhPy7u+Ome9d3XHeG/wDkqPjX6WP/AKJNdjXHRd4v1l+bPPw7vBv+9L/0phRRRWx0BRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABQeRRR1oA868IeI7mDTItHsoft+oM2oXQ+1XLKPLju2QLvIYkkkD2AqxF8UIb3QbvWtO01pbLT7aC4vVkm2Sosi7iFXaQxVTnqM9BW5beDvDogh+yQOBbyTNHLFey71MjEyrvD7sFuSucZHTip38H6E8wkGnqn7mOBo45HSOSOM/IropCuF7bgfShba/1v8A8AHuzmPB+vSnxNr2iW3+kSw6pcTzC4nbckJ2hRHnO75gRjIC8ZIyAetGq3rfKNCvoiRw8skG1fc7ZScfQE1hTaf4I0vVjfT3UNteW11JcPM2oyDy5ZAA4Y78DcAvyHg4HFdj1HsaWvIkt7JfgD+JvzZ55L8TrmLQrfUho8TibQ21byvtRBBV0Ux52f7eQfauk0DxBe6lrGqaXqumx2N1YLDKDDcecksUobaclVIIKMCMdupzWNr3w202Xwzf2nh21WC+ms5LOB7i9m8uKN2BKjlsLkZCgYHbFdTpmk22mrI8MRWecKZ5HmeVmKjAG9ySQOcD3PHNV/X4v9LB/X4L9bnO65/yVjwv/wBe13/6AK7GuO1z/krHhf8A69rv/wBAFdjXPS+Kfr+iOaj8dT1/RARkYPIrD1Lw208FrHomozaGLUysiWaKImZweWTGGwx3Y7nPrkblFdMZOL0OuMnF3Rz50XXjp2lwf8JO/wBotZt93cfYk/0xN2dhXOE44yKsDS9W/tXUrj+3X+y3MOy1tfsy4tH2gbw3VucnB9a2KKftH/SRXtJeX3I546Frx0bT7X/hKJftdtP5lzefY0zdJuJ2Fc4XggZHpVsaXqf9uXt2dbkNnPB5cFl5C4t3wPnDdW5BOD61rUUe0l/SQe0k+33I5w6Brp0C0sv+EqnF7DceZNffZE3Tpz+7K9AORyPSrw0zUR4huL46zIbKS38qOw8ldsT8fvA3Ung8e9atFHtJP/hkDqSfb7kc2fD2unw5FYf8JXcC9SfzG1D7Km505+Tb0A5HPtWiNNvx4kbUDq8psTB5Y0/yl2B/7+7rn2rToodST/4ZA6kn/wAMjmv+Eb1v/hGv7OPiy6+2+f5n9o/Z037f7m3pj3rROmXx8SJqA1eYWaweWdP8tdjNz8+7rn2rUoodST/4ZA6sn/wy6nNjw5rI8OzaefFV4buSfzVv/JTfGnHyAdMcHn3q+dKvj4gtr8axOLWG38qSx2Lslfn94T1B5HHtWrRQ6kn/AMMgdWT/AOGRzg8OasNBvLE+J7w3M8/mx3vlLvgXI+QDoRwfzrEjsp/+F4QN9ulxbaCvmjA/0j94V+b05IbjuBXfVxsX/JbZv+wCv/o+sa1SXu+vZGFetP3fXsupo/8ACO6n/Y+o2Z8S3vn3c3mQ3Wxd9su4HYo6EYBHPrTr3w3c309n52uXq20Nq1vcQRkL9p3KVLlhyG5zx3Fb9Fb+0kdHtZ/0kctp3hC+03w7eaVH4n1OUzFfs91KQ0tsoP3VPfjjmr50O9N5pM39uXmywiCTxYG28O3G5/fvxVXU/Fa2Hiu20Oe3WEXYQRXE8jRrMWLbljYKVLqADtLAnPHSuV8I+L73SvDdla3lktwr2V7dW9xLf4aQwzYKyFxhAd4wxY9Dmj2snq/P8tRurNvXf0XXT+vvOwHh6++z6vGfEN+W1CQvA/y5shknbH7c459KedBvDJo7f29fAaeu2ZRjF6cDmT8u3qawB8Qb+XbDaaLDJdHVv7M2yXTxJuMHnK+TFuxjg8cdRmrH/CdXS+fcvorNplk80V9dRXAJgaJSXO0gbl3KVGDnkHHPC9pK1/62X6NC9pO//AXp+jNUeH7wDWAdfvz/AGif3PI/0Lr/AKv8x19BQfD12V0cf2/qAOnHMxDD/Ten+s9enb1NYeh6jfah8SluLqMW8NzoEdxHCl00qjdL1IIADY4OOvrUknjvUItWlik0SI6fBrCaTLcJekyK8gTY4jKAFcyKD8wI96fPLS+//B5Re1l0/Jfy3/I2h4fuhLrD/wBu6gRqKbYl3DFlwRmP0PPf0FMPhu6NvpEX/CQalu06QPK+8ZvBkHbLxyOMfjXOzfEy4hs9QlOjRvJaWcN4qJeZV1eUx7d+zaSMZypZTnGeKvah411DSL22ttT0i2V5JreKX7PemXZ50xjUj92CAPlPz7c5IGdpo9pLTz/4YPayV/L/AC/yNgaDcfbNVn/tvUNuoReXHFvG20O3G6Pjg9/rUJ8M3J07S7b/AISHVN9hN5jz+aN90N2dshxyO30rHh8d6rcX1vbxaHa4udRutOjZtQYfvYQ53EeVwpEZ55IPY9abZfES81WzibSNAa7u1tkuLm2W6C7AZHjIRiuG5ic5O0dOmeEqktP66f5D9rNfL/hjohoU39q6lef2zqGy+h8pLfzB5dsdoG+MY4bjP1NQr4Tt5/DsOjazeXerW67hMbuTLXGWLDeRj7pxj6CsSy8aakr3sd5awTzya1Lp1jFE5UYSMv8AMcHspPfJPYVKnjLXpNRhsf8AhF0juvsLXk8Ml+NyqsmwquEIYngrkjOedpo9pKy17flf8he0muu36aHYQQRWtvHb20axQxIEjjQYVFAwAB2GKkrB8KeJo/E9jPcRrFG0MojeJXbzI22glZEZVZGBJGCOcZ71vVLvfUzvfU47w3/yVHxr9LH/ANEmuxrjvDf/ACVHxr9LH/0Sa7Gueh8D9Zfmzmw3wP1l/wClMKKKK3OkKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKD0460UEZGDQB5pZeK9ekgsbfTV0i0a5TVJHP2Nyga3n2ghRIPvZyeevOe1a3iLxLqJ+Cdz4j01PKv5dIW5QR8+UzoCWH+7kn8K6FPDGgxbfL0TTU2Bwu20jG0P98Dj+Lv696vW9la2dklnaW0MFrGmxIIowqKvoFHAHtSteLX9df+B9w07ST/roZWj6NpNn4PstKiEUtg0CKCzZE5IyWJ/iLH5ie5NbdULHQ9L0xgdO0+2tsZ2iKMKFz1CgdB9Kv1Und3JirKwUUUUhnHa5/wAlY8L/APXtd/8AoArsa47XP+SseF/+va7/APQBXY1hS+Kfr+iOaj8dT1/RBRRRW50hRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABXGxf8ltm/wCwCv8A6Prsq42L/kts3/YBX/0fWFb7Pqjmr/Y9UdlRRRW50mZdeHtNvb/7ZdQySS7o3wZ5Nm5OUbZu25B74z09BWa/w98MyWcNrJYSvBDDNBGjXkxCpKwaQcv3IBz1BAxiuf8AGNzrCeKlOmwa0Ft5bBw1vHNJDJGZz520INowp+YNuJGMAAZqnNLrFz4f1sXNv4lh163tr5SYPOEEuSTC0W3hjgIFCcj5t3fKv7t/X+v0Kt71vQ7GLwNoENylxHbXHnJdLeB2vp2JmVPLDnL8nbwc5z3zUo8HaCNYudUFgPtN1kzjzX8qUldpZos7C23jdtzjvXJT2eoW0sMDnX30650l5IpIpbl5Y70kE78HcnGNoOFB3DjNRQN4qh1OA6gdQuWNoq3ZgilRrWYWfzPHj91MhfOF4YOe9N9V/Xb9PusTHWz/AK7/AK/fc7LSPCGi6Fdpc6ZbzRyxwfZoy93LIEizkIA7EBQegHTtWfo/g/ytY1i91qKKYXWpfbbZEuZHRR5aIC0ZAXcNpOcHqOeKb4ITU5NN1C11yOZsSKqXTGeMXKmMfMI5fniPqASN2SD1rmLLR/FdtqFrbul7JbKZtGmme4cs8RYul597qEAXd1LE+1P7VvL/AIP52+8V/dv/AF/VrnWD4ceF1tWtxYT+U0C2xX7fccRK25UHz8AHoB06DireoeC9C1S+a8vbSV53EO9lupkDGJt0bEKwBZT0Y8+9cvexeIDqV/BCNSjv4dVtzp0iGU2zWX7sOGI+Q/L5u4N82cY/hqjYw6vaaJZX0y+IZkk1iaLU43e6aZbQSTeUUjPzbcmLJQZK+opLWz/rp/n99xvS/wDXf/LT5HUaFovhjVYUvtLgugLPUrmVDJcTqVudzJK21m7ksMHjngVMnw98Mxm0MenyK1orJEwu5slGYuUc78uhYk7WyOTxXB2cGuWOnhdLtdWIj1C+kNjNDdRG4je53K4mH3ZNpBHmAqwJzjk17DQtkwe7X9bmDeeCPD99Fdx3VizLd3Iu5NtxKpWYDAkQhgY290xmpI/COixT+cltKJPsjWRP2qU7omOWB+bkkkksfmzzmtqij+v0/LQClp2kWWlGZrONw85UyySSvI77RtGWcknAHrV2iigDjvDf/JUfGv0sf/RJrsa47w3/AMlR8a/Sx/8ARJrsawofA/WX5s5sN8D9Zf8ApTCiiitzpCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACuI+LNzPa+ChJazSQv9qQbo3KnGG7iu3riPi1bzXPgoJbxPK/2qM7UUscYb0rSl8aOLH3+q1Ldjw9tTvnuEne9uGmjBCSGViyg9cHPFepfBm+u7y51cXd1NOFSLb5shbHL9M15f/ZGpf9A+6/78t/hXp/wYs7m1udX+1W8sO5ItvmIVzy/rXfWUVTdj5LK/afXIXv1/Jnq1FFFeYfdBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHm/xkvLmz0rTWtLiWAtM4Jicrn5R6V5D/AGtqPneb9vufM27d/nNnHpnPSvXfjLa3F1pWmLbQSTETPkRoWx8o9K8j/sjUv+gfdf8Aflv8K9Khy+zVz4nN/afW5ct+n5HsPwdu7m78P37XdxLOy3WAZHLEDYPWvRK87+DltPa+H79bmGSFjdZAkQrkbB616JXFW/iM+my2/wBUhfsFFFFZHoBRRRQAUUUUAFFFFABRRRQAUUUUAFeS/GS/u7PU9MFpdTQBoXJEUhXPzD0r1qvJPjLZXV1qmmG2tppgsL5MaFsfMPSt8Pb2iueVm3N9Uly+X5nmi6tqKyPIt/dB3xuYTNlsdMnPNe6fCy4mufA0MlzNJNIZpBukYsevqa8M/sjUv+gfdf8Aflv8K9z+FcEtv4FhjuInicTSfK6lT19DXTibcmh4eS8/1l819n+h2VFFFeefYhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAI7bEZsE7RnCjJNR204urSKcRyRCVA4SVCrrkZwQeh9qi1TP9kXezzd3kvjyd2/OD93bzn6c15HMniGbQnmP/AAkwvIPDVnJEqC7XN6rNuyo+8/3dynOe4NC1dv66/wCQ3t/Xl/mer6nrNppMlkl4ZAb65W1h2RlgXbJAJHAHB5NX68q1D+1r/WLdr6x1d7628RxzQsIJzbC02nYwwNgxkbs/MDnNLplvfajod5/bR8WRai9l9mu4FSVEFyWAWSFh15yQyHYF+90AoXw3/rZP/geouv8AXd/8OeqVmQa/aXOsNp9ulxKyM8bTpCTEsigFkLjgHDDrgHkA5BFZPgJ74aPcW2tw3aarb3Dx3csySCOdh0khLAAxkYwF6cg88nlNNtb+wvJmisdWj02bxFcm+CRXCyPCUPlOo+8yb8ZK+2eM0fat5f5f5h9m/n/n/keqUV5lp+na1c63oVprE+ufYz9vO9JriP8AdCZTbec6kYfaD94gkcHuK6bx/Yz3fhgS2FtJc31pd21xbJEfmLLMhIH1GQc8YzR0TDrY6eivO4E1LTW8Q32txS3C6XfnUYJJGKrMxtUAjjP90MWXoegHJzXZrqn2e3hXUYZhdGJWlFtayyoGI5AZVPejp9wdTRormpvF4TxNHpMNhJKrSRRtPuK7S8byEbSuchUBI9GHfAN6016O61iezEVyFRY9haymXls5ySuAOO+KANeisXwteXN7p9417O07xahdQq7KoOxJmVRwAOAAK2qA6tBRRRQAUyaaO3gkmmYJHGpd2PYAZJp9ZniS6ksfC2qXcFx9mlgtJZEmwp2MFJB+YEdR3FTJ8sWyoq7SNCGaO4t45oGDxyKHRh3BGQahvNSs9Pe3S9uY4HupRDArtgyOeigdzxXlN3401Zbe7vI/EoQWem6XdiEJAVkeZyJQxK5wQB0IIzwRU2teI21HXLSO91AQ3Vj4njjj0wlFBgCny5eRuIbOd2cc47VbVpcvn+qX6kJ3jfy/S56zRXlemeJ77WtDvLmXxgLK6+xbZrRLRBLp95uAVTuHGWO0I2S3UHqa6vwHrU2s6Nc/2ndE6tb3Dw39qSv+hyDjYuOq4wQxznOfYL+v6/rsN6G3HrWmy6sdMjvI2vArN5OeSFwGx2OMjIHTIz1q9Xk2ga02nWcKxasy2F34iv4Ly/Lxt9mG+UpyVwu9goyfbGMirmia5r2q+KdM0y91+S2SSG8lUxRQBrxIblFikwUOA8ZOduARyMULWy7/AOVwel/L/Ox6bRXK+M7m+06/8P3unPcux1D7PJaxudk6vFJgMPZgpz2wayNE1zUotG/tPVJLu8n0+8urGSOMhFupXuvLjAB4woA57Bu/NC1/r+u6B6f16/5HoNFUpdZ0yCYw3Go2kUq8NG86hlPoRmqMN1q6+Lms5ntZ9Pe3eXEUTK9uQyhAzFiG3AuegxsPWjqBt0UUUAFFFFABRRRQBDd3kFhavc3koihTG526DJx/M1NXN+P9VuNE8E3uoWV59jnhMeyUhT1kUEfMCOQTXKah4r1G28Tai0fiIfZrPXbG0S0KwbGimSPepO3dwWYgggjHOaFq7f1ul+odLno0upWcGo29hNcxpd3Ks0MJb5nCjLED0FWa8Xn8WXt5faNq0d2bnXLe31PfpJVM20yp8sewAPngAZJ3dR1rYXX9R1Hw/cXmg+MY9RuZJoDp8CQIu+XkvbO2B94A5GAycnOMCjoM9Qqja61pt9qE1jaXkctzCu541PO3cVyPUBgQSOhGOtZvhLV7fXPBsF9FqUlwZEbz5n2q8MnO9CBwpU5GDnp1PU8B4R1yS20HRLI641jYXVndyjUMxN/pCy/LFuZSo+Us+08t9Bih6PUS1R6/RXnfg3Wtd17xKItY1WS0kt9MsrubTY44gDJIJA4bKlwOEbGQQeM44rd8d3F1YaTZajYy3SyW2o2u6G3Y/v0eZEZCo+9kNTas7edvxsG/9eVzp6K4HR9Yvw2q3fiC7uI00PUZ5blog2wxGEFYgoHzgb8+vyg967uGTzoElCsgdQ21xhhkdCOxpdE/T8Q629fwH0Vm/wBqv/wlX9kGBdhs/tIm38k79u3bj8c5rSo6X/rsHWwUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUARXVrb3ts9veQRzwv96OVQytznkGpQAqgKMAcADtRRQBWOm2LXIuGs7czrJ5ok8obg+3buz67QBn0GKlWCNJ5JlXEkgAds9QM4/makooAhtbK1sUdLK2ht1kdpHWKMIGYnJY46knqamoooAKKKKACiiigAooooAKKKKACiiigAIBxkdOlVb7TbXUY4Uu4yywzLOgVyuHU5B4Izz2q1RQAVm2ugWNlqs2owG7+0TsWkD30zxknHIjZyg4AAwOBwK0qKACiiigAooooAKKKKACiiigAooooAKKKKACggHqM0UUAUdX0qLWNMksZpJIo5GRmaLGTtYNjkEYOMHjoTTrjSNNu5jLd6fazykAF5YFZj+JFXKKAM9dHgTXk1VJJVkS1+yrCNojCbt2cYznj1x7VoUUUeQBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVWv9QtNLs3u9RnS3t4xl5ZDhVHqT2FWawfHX/JPfEH/YNuP/AEW1TJ8sWyoq8kie38V6FdzW8VvqltI9yQIAH/1pxn5fXjmrtlqVpqP2j7FMJfs0zQS4UjbIvUc/WuX8Gtq1xofh6G/0+zXTl0yORJo52lbzFWPYSCi7TgseCf8AGkfHGq23hvWr25itXmsdaOnLIkbLHHHvRfNcbicAMSeR+FaNWk4/1ukQneKf9bNnf0V51qvizxFZQeLfInspI9BtkmguFtifOZk37G+fHHfA7g8VqTeIdVXxXZ6fDLb+TdaLJfASQk7JVKDsRlTuPH61F/6+Tf6FW/r7v80djRXnOmeMvEknhrRtXvxp8n9uGGC1gt4WDRStuLMSzgMNq8DI5wM9z1Xhy58QSzX0PiGzjijikX7JcLtUzIRzuRXfaQeM559BVW1fkI0bvVtPsbu2tby8ghuLt9kELuA8p9FHU1brjfHSj+2/BzYGf7bUZx/0xkrpZl1I6xbNBJbDThG/2hHRjKz8bCpzgDrnIpLVfP8ARP8AUHo/l+rLtUbDWtN1O5ubfT72G4ntSFniR8vET03DqKvV5pr2iakvjTVvFHhjLavpphV7Uthb6AxgtEf9rup9aV7b7DtfY7+DVrG51CawguUe6gAMsIzuQHpn0z29auV51beOrF7LXPE+mL5yNBZosT/KySszpsf0Ks3P0qfWtd8Y6Po+uXs9rAlvZ2Jura6eNFPmr1iaNZXypHIbI9Kb03BK+x31FckNe1NvGmn6YksAgvdHkvMNFkpKrIB0IyvzHj9ayNH8U+Kr7wbD4huptFihuFCBGjdPKbz9hfJb5vlzhByTgZNP+vxa/QS1/ryudxqurWGh6bLf6tdJa2sQy8r9B/jVtWDoGU5VhkH1ry7xF4gvNZ+HPjuzvwrHTR5UcvkNC0iMiuCyMSQefbtwK9DuLiW08OyXFssTTQ2pdBNJsQsFyNzdh6mlsrvy/EdtUl5/hYv0V5/ZeMtYk1C+sjJaTtHov9pQz/ZJI03gkMoBbLp6MCPqaTTfFniF9M0C41GTTd/iLyEtRHC4+zkxNJIWy3zcKMAY5PfHLt/Xza/QX9fgn+p2mpazp+kfZ/7Suo7c3UywQhusjscBQB71Ne3tvp1jPeXsnlW9uhkkfBO1QMk4HNcB40OtrotnHqxspZY/EVn9kkh3L5kfmDaXXnac5BxnpU9z4s1/T7LxjDcSafPe6DAlzBMts6RyK0ZfaybycjaRndU391v+tk/1GleSX9btHeQypcQRzQtujkUMrY6gjINPrz/+09Un+ImkPJqHl2jaC95JbrECud8e7v1Pr2/Op9L8T+JdSi0vWYdNjk0a8jMtwp2K1vEVyrK3mkuRwCCo9sVT0/rza/Qla/15J/qdzRXJaLq3iDXdP0nWbR9PXTtRRnlhZWD26MD5ZVs4ds4BBAHpiuY0LxBr2n+DNJ1CTUVu3vteNpMJoBko9y6scg9eOOw6Yos72f8AWth9L/1s3+h6pRXnmveMdb0pfGhtXtJDoiW8tsJoSciRSWVsMM+x/Q1fl13xHa6rbaPObSe9vkluontYP9VCgQbSJJFDNufrnoOnPC3DY7Siud0mbxZd6fYvqVvY2FzHdMt6jfP5sIztaPaxCsflOCTjmuioAgvL230+1a4vJViiUgbj3JOAAOpJPAA5Jqrb69p95Day2c/ni7dkiCqQxKkh8g4I2kHOenTqQK5vVRear8V9K09bmL7Fplq2oyQmEn5yfLTJ3cnBcjjjHQ1p6e/2n4gavvHFlaW8MQ/u7y7ufxwv/fNC2T/r+rg9NDoPMTzPL3LvAyVzzj1xTq5nw5eSar4k1+6lt7MpZ3AsLe5ig2yuqqGdWYkkgM2McDIPFTeLtW1DQ7OyvrFrf7P9tghu0miZmMckiplWDAKRu7g/hR287fjsHfy/Q6CiuNj8W3+p3Gu2umRwxTWj262BmQsJxIM7iARwcNg9hzXTWOrWWpy3UdjcJObSXyZthyFfAJH6/nkdqALlFFZeh6w+r/2gJbYW7WV69rgSb9+0Kd2cDGd3SjrYDUoqG8uksbGe6lWRkgjaRliQuxAGeFHJPsKqXWtW9poa6rJFdNAyowjjt3aXDkAfIBuzzzxxzQBo0VTv9Th042omjnf7VOsCeTCz7WbOC2B8q8dTwKuUAFFc94q1fUNEk0q4tntvsc1/FbXayxkuFkO0MrBgBgkdQfwrOtPFeoas+rw6fFDBPa38ENoJkLedE6o5YgEYypc+w6jiha/16f5g9P69f8jsqKp6fq1lqouDp9wlwtvMYZGQ5AcdR+HT6g1bZlRC7sFVRkknAAoAWiqlpqun38hSwv7W5dUWQrDMrkI33WwD0PY96lt7qO5edY85gk8t8jvgH+TCgCYnAJxn2FQWV7Df2qz2zEoSVIIwVYHBUjsQQQRVLTNYfUNX1axkthD/AGdMkYcSbvMDIHBxgY+9jHNU9IJh8aeILROImS2u9vo7h0b8/KU0DsdBRWZqN9qNtchLO1sJY9ud1xfGFs/7ojbj3zUNtqWrS3UaTWWmJGzAM0epM7Aey+UMn2yKBGzQTgZNc54t8Q3GiPpUFr5EJ1G7+zteXQPk2y7SxLcjJOMAZHJ/Cuan8Ra1qfhfxA8s1s+nrcrp9peQQtG1wruiNIDuIwN7DIGCRkGjfb+tv8x7Wv8A1v8A5Hf2uoW93ZfbInxbnJWR/lDKP4h7Hse45qz16VzHjG6ax0rTtNs7ezmGo3kVj9muoPMjaM/f+XIHCKx5yOOldDcSrZWMsqp8kEZYIo7AdB+VJtJN9EJJtpdSC91iw0+ZIbu5VZ5BlIVBeRx3IRcsR74qO31/Srqxuby2v4Zbe1JWd0bIjYDJU+jDI461zPwrie98Jr4kv287U9cdrmeU8kLuISMeiqoAA+tXtetYLbUtHs4ECJqmria5x/GUhZxn8Yk/KqaadmF01dHTQyebCkmxk3qG2uMMM9iPWn1zOseILzRfGFhbXLW7aXeW07ALE3nLLGFIG7dhsgnA2g5x1qtofijUNX07TbyUW1oDc3S6hE4J8qKJpEBDZwCGCAk5zk9KQHX0VW07UbXVtPhvrCUTW0y7o5B0YetSXUrwWkssaCR0QsELbQxA6ZwcflQ9Fdgtdht7eRWFo9zcbhDHy7AZ2Dux9h1PoKnBDKCpyDyCO9Z2iagNe8N2OoSQCNb62SZoS24KHUHbnAz19KpeCZHfwlbI7Fvs8k9spPdYpnjX9EFDVnZhuro3qKp3epw2d9Z2ksc7SXjskbRwsyqVXcdzAYUfWhtThXWY9MMc/nSQGcOIWMYUEDBfGAeenWgC5RVODU4Z9VutPSOcS2qI7u8LLGwbONrkYY8c46VcoArX2o2emwCa/uYreMsFUyMBuY9APU+w5qCz13TNQvGs7W7RrpE8x7dgVkVc43FCAQOeCRzXJ+FWbxD8RPEmr3/zjSLj+zbCJukACgyOB/eYkc+gxWx4zVdP8Oatq1sNl99iNusw6qCePyLZpdE+/wDSH1t/Xmblnew38BmtiWi3FVfHD44yPUZ79/pViua8SX154Y0fTJdK+zLZQ3Vva3EUsRLeU7rHlGDAKRnuD+FUo/Ft/qVxrtrpkcMU1o9utgZkLCcSDO4gEcHDYPYc099v62/zEdlRVPT9WstVFwdPuEuFt5jDIyHIDjqPw6fUGrbMqIXdgqqMkk4AFAC0VUtNV0+/kKWF/a3LqiyFYZlchG+62Aeh7HvUtvdR3LzrHnMEnlvkd8A/yYUATVVt9QhurN7i3Dv5ZZXjC/OrL1Uj1qrpmsPqGr6tYyWwh/s6ZIw4k3eYGQODjAx97GOapWL/AGf4haraJxHcWVvdlfSTc8ZP4qqf980f1+odL/12Ny1uob21jubWQSRSLuVh3/w+lS1z3hp/K1nxFYL/AKq3vxJGP7vmRI7D/vosfxrL1vxBq2leKL/T5buCK3n0x7rTJDb5/eocPG3PzdVIAwcE0r/18rjS/r52OxuLiG0tpLi5kWKGNSzuxwFA71Fp2o2mradBf6dMs9rcJvilUEBl9ea5Dw/4kv8AxJo+nv5yLKLKWTVIzCAUlBKeWOfl+dX9eF96xPA+t6vpGgeBreV7OTTNUhNuIliYSxFY2cPv3YOdpGNoxnqarvfy/X/IXS/r+h6nRXD6V4l8T64mmarpWnRSaXezESo4RTFDkgSB/NyzDAyu0Z5AxjNZ9r408Rx6Bf69qB017Ozu57LyIoXV5JBMIo23F8AZPIP588Lrb+v61D+v6+49IorntEuvEra5cQazZR/2cYQ8N0AkbiTODGUWR8jHIPHoaq+IfE82n+KLfSBc2mmW7WUl3Lf3gyp2sF8tAWALc5PJ47c5oA6uqthqVpqcUsljMJUimeFyFIw6nDDn0NZHgvU9Z1rwxDqOuxQW89wC0UccLRlUyQGYMxPIAbHHBrn4vG2sL4Xm1G6tlkFvrE9ldTWVq0nkQRsy+b5e4k8gZ5OMk4OKHo7P+tUv1DdX/rr/AJHoNUtU1nT9Fgjm1S6jto5ZFiQv/E7HAAA9zVXw3qn9teH4r6O/tL4Sl/LubRSI2XcQvykkg4xkE9c15ldT6ndfDTULjUL77bJ/wkaxpvQLyl4qjkdsAcY4o+1y/wBbpfqH2b/1s3+h7JRXA3XjXVNBvPENrrItLyTT4raa2e3iMIPnuUCsGc9GA5yMj0rd0S68StrlxBrNlH/ZxhDw3QCRuJM4MZRZHyMcg8ehoWoPQ6GiiigAooooAKKKKACiiigAqhrmkpruiXelz3E9vDdxNFI8G3ftYYIG5SOh9Kv0Umk1ZjTad0UdG0tdF0a102K4muI7WJYo5J9u/aowAdqgdB6Vx3iHwy+i+HL2DT5davI9U1Rbq8ktljkmt8kMzIioN4yqjbhuD0PQ9/RTd27/ANdxLRWOB0DRbnUrS80y7v8AVL3Qbq1khlj1HTks3VmwPkARG6bskrjpg9a07bwBa217b3h1jV57q3tHs0mmnRj5bY4xs28bR2+ua6uih6/1/XcP6/r7jl/+EA0t/BNv4YuLi8ntLUqbedpFWeFlOVZWRQAR2OPzrU0PQk0SB1+3X2ozSYD3N/N5kjAdBkAAAZPAA6mtSinfcDH1zw5Frt5ptxNe3du2m3AuYVg8vBcAj5tyEkYY8DFW5tO83WLbUPtt2n2eN4/syS4hk3Y+Z1xyRjg9smrtFLYArNsdHFjq19fi+upmvSpeKXy9ibRgbcKD09Sa0qKAOfl8EaFNHrcb2f7rXNpvYlOFZgMbhjo3fPqM1BZ+BLG30e706+1HVNVhurdrUtf3PmNHEwwVUgDHbnk8DniunopWQ7s5ex8CWtlqVpqH9r6tcXdpataRyzzo37tscEBAONo7duc0yP4f6fH4RtvDw1DUTBZ3C3NtceZGJoXVy4IIQKeSeqnrXV0U/wCv1EcjP8OtPuLPWreTVNUK62FF2xmQkkKFJGUIBIUZ447YFbt9otvqfhyfRb+SWa3ntzbyOSFdlIxnIAAP0FVtK8QjU/Ees6QbOSB9LMQMjuD5vmKWBAHQYH/6q2qN1Yd2nfscknw+tEuftL6zq8tx9gbT2lkmjJaI9MjZjI+nuQTzU0/gLTbrwjY+H7i6vXi04xm0u1kVLiBkGFZWVQMgcdPrXT1DcXcFqYhcSrGZpBFGGP3mPQD34NG/9f11YjnbnwJaXWm29pPquqO8N3HeNdPKjyyyRkFNxZCMDHRQB+dLeeBbW9m12SXU9QB12BYLoKYsKiqVGz93wcEjnPWunooeugLQ58eDrQalpd8L29E+nWptAwdB58RIO2QBeeVH3dtVtG+H+m6HdlrO91JrJZDJFpktzutoWJzlUxnGSSASQDzjNdTWL4p8RDwzpUd81nJdK9xFAQjhdm9wgY59z2FG7/rq/wDMOn9dClofgLTvD92Hsb3UmtI5Glg0+W53W9uxzyi4z3OASQM5HNRf8K80waHNpS32orA159sgZZl3WknmGQGP5cfeJPzButdZRQBx918N9Pu7fV45tV1YtrEccd5IZkZnCDAIyhAP4fTAq/rngyz16DTzc31/b3unEm31C1lWKdcjDchdpBAGRtxXQ0UAY8PhyOCGxSLUdSU2k/nu/wBoy12xBBExx8w56cDgYwBir+o3v9n2El19muLry8Zito98jZIHC98Zz9BWdoPiIa5eavb/AGOS1bTLv7KwkdWLnYrbuOBw3rUmqeIbbTdQt9OjjkvNSulLRWkGN2wdXYkgKo9SfYZPFH6/1+QfoVfD+myjWtX127Ro5dRkRIY3XDJBGuFyOxJLtjtuGeale2fT/GBv1Um31G3S3lIGfLlQsUJ9mDsM+oX1oj1+aLVjYappz2m21e6a5EqvCEUgEZ4O7npjGO5q1aam8uhtqdxbSLGyGaOGONnkMeMr8o5LEc4Hrjmjz7f8N/n8w8v6/rYyYvCa6V4X1LT7LUNTaS6klmE0MqpMru5f5CBgcnGSM46nAGNLVdCh13w+ul6s7SRsYmlZTguUZW/Ur+tPttW8zVhZTxmIzQC4tiylS6cBgQeQykjI9GHvVnUJ7m20+aaxtPttwi5jtxII/MPpuPA/GjZf10Dd/wBdSnb+HrK18RXOsxhvtFxDHDtz8iKmcED1w2M+n41W0DQ57Xw+bTVmxcy3EtxM9rO67mdy33htPfGPQCt1SSgLLtJHIznFNmmjt4JJp3WOKNS7uxwFAGSTQ9NwWpWttLt7SYSxSXbMBjEt5LIv/fLMRVHw9pd5pr6ub3yQLy/kuYvJkLEKyqMHKjB+XtmmyeJkXxZp2ipau6X9rJcx3W8bcJt4x1/iHXFblHn8vx/zQf1+H/BMzw9Zz2GixW91c3Vyys5WS8cPMVLEruI6nGK06Krm8QakLLyp95iMvmeS3l4zjG/G3d7ZzjmgCxWb4hs5r/Qbm2tbi7tp3A8uWzkCSBgQRhj0Gevtmrk13BbyQRzyqj3D+XEpPLtgtgfgpP4VmeHvEP8Ab0uqx/Y5LVtNvWs2Ejhi5Cq27jgfe6ZNG4ba/wBf1oTa1oVr4g02Ox1Pc8KzRTMFON5RgwB9iRzTbXw/aWWvahrEO5rq9VAwZvlTaoUYHbOBk1q0UAYWgaFNaeF7aw1VyLkM0s72txIm6RmLMdy7ScsxOKlvtFkXSr1NInnW9mt3jhe6vJpERiMAkMW6delbFFDs9BptO5y2heERpV5eI+Fs/s1va2phndJdkYJJYrggl2Y8Hnir2laClneXkzteDddeZFuvpWDLsUZILkHkHrWzIxSJmVGkZQSEUjLewzxWV4W19fE/h231ZLZrUTM48p2DFdrleSOP4ad3e4rWQzSdMu7HxBrl9deQLe+ljkh2SEsAsaodwKgD7ueCaNDtnk1HU9YmRkN86JCjDBEMYIUkdslnb6MKgvPE866pd2ekaXJqTafJCl4I5gjp5gyNoIw2FIJyRweM10NJbAU7vRtMv5vNvtOtLmTG3fNArtj0yRUcPh/RreZJoNJsYpUO5HS2QMp9QQOKvu6xxs8jBVUZZicAD1plvcRXVtFcW7iSKZA6OOjKRkH8qAOR8X211quqwWM02padZQItzBeafbG48yfLDY67WG0DBwy8kjBGKks9N1vWvBmpadrtyJpnkZbK7e28h3UYaN3j/hIcdOOAOBmuuooWisPrc56XSY/Etxo2q3Ut1Z3Gms7G2QgfvGXYytkZ45wQRkHg81ds7GeLxBqdy9zdy21wsQWGdw0cbAEHy1xwCMZ9TmtSigRz+leFBoUb2ujandWuntI0iWm2N1hLHJEZZSQuSTg5x2xUuvaY7WNlPYo8s+l3KXMabtzSAAq65PUlGbHvituigDKutHsNa1DStVm3SNYlpbcdBl1AyR7Dt6/Ss+Xwt/Z/hjWbLQH23momeQSTucK8rMcAgcAFjgAfzNb9vdwXYkNtKsgikMTlTnaw6j6ipqGkxptfIzk0aA2NrBJ50Qt4ljVLa6liUADGPlYZ6dTUg08W1jcRWZld5EIH2m5kkGcYHLFiB9Ku0US969+olpa3QwtHhufDXgewtLyNZ7qytI4PLtmLiV1UKApIB5PqBiqy+H7q28I2Gnw3l1DeRTpNJLaSbd8hcu+7I5jJZiR3HFdNRTbu7sNlYKKKKQBWXeWU8viPTruG5u0jhSRZoVkAgdSONy45bOMHsN1alFAGEfC8dvr11q2k30+nz3oX7XGiq8czKMByrA4bHGQRnuDVjUtDTUPDt9pktxK7XkTI08hBYMRgHAAAxxwABxWrRS6WHfW5hT6fF4s8Liw1qKSIlo/tUQOCHjdWIB9CV6jqDmrEHh6ztfEF1rMIb7VPBHCFJ+RAgbBA9fmxn0/GtWimxLRWMLQNCmtPC9tYaq5FyGaWd7W4kTdIzFmO5dpOWYnFS32iyLpV6mkTzrezW7xwvdXk0iIxGASGLdOvStiih2eg02nc5bQvCI0q8vEfC2f2a3tbUwzukuyMEksVwQS7MeDzxV7StBSzvLyZ2vBuuvMi3X0rBl2KMkFyDyD1rbop3d7k20sYukaXeWXiDXL258jyL+aOSHy5CWAWNUO4FQB0zwTTNMi3atqmvXAMcUyJBBuHPkxbjux7s7EewWt2ikMxvDtlLCl9f3SGOfUrk3DRsOUTaqIp99iKSOxJqbVfD+n6zeadc38ReXTbj7RAQcYbaRz6jkHHqBWnRR+gGPZ+GbDThqx0/wAy2k1aZp55EIJDlQCVyCB3OCDyTWba+ALK0tdBt49T1Ex6C5e1DGL5sqVw/wC75GGI4xXVUUbfh+Gwf19+5yumfD/TtI1GSexv9TjtGmM66b9pzaxyE53KmM9ecEkZ7VNZ+BtNt/Duo6HdTXV/Y6jLJNMtyUyGkbc20oq4+bkeldJRR0sO+tzD8PeFofD4z/aep6k4Ty431G480xJ/dXAA7DnknA5rndctdQ1PxHPcvfatotxYN5di1rp5u4ZoyoJkYbGXcTkcFSAPeu+ooeruJaIzdAbVJfD1o2vBE1Fo/wB/5S4GfXGTg4wSMnBrLsPBUWm2/l2es6pG326S+8zdDu3ybt6keXgqdx4I9Oa6aijrcOljP0bRbTQrF7ayDbZJnnkd8ZeR23MxwAByegAFYM/w60+bT7uyTU9Thtrm+F+YkkjIjk8zzMLuQ8FhnnJ98V11FHW/9f1oBzV34F03UdQ1S61K4urtdVtUtbiCQoE2ISVI2qGBBJOc1Z8PeFofD4z/AGnqepOE8uN9RuPNMSf3VwAOw55JwOa3KKNgeoUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAV5BriXP2H4g3i6tqiS6RcLLZBL6QLCwhR+gbkZJ+U5HPAr1+sx/DWhSLciTRdOcXZ3XIa0Q+cfV+Pm/Gl1+X+RSaW5weo6kdb8R6xpOqeIo9DkhtYJLJ3keMhWjy0yESICQ+Qcg9AOmaVLtLbxLr0F/qjSRSeHbecSSuYRLJ+8BlVCflJIXp3xXez+H9Guvs32rSLCb7Jj7P5lsjeT/uZHy/hUt1o+mX11FdXunWlzcQgrHLNArugPUAkZFEldWXn+T/z/AK6KLta/l+n+R5do6S69L4ItJ9X1JYb7QJTdfZ76RDI6rHgkg8MCx56+vFGq6sba9W60/Vrlxb+JILJ5rm9ZX2hlR4REDgoBnLNyTk46GvTI/DuixXFvcRaPYJNartgkW1QNEPRTjKj6UkvhzRJ555p9H0+SW4KtM72qFpSDkFiRzggEZq7+9f8Are/5aE29239bW/4Jz/hllf4m+MWQhgVsSCDnI8pqh8RW73vxP0Swe+vobS60+6aeC3u5IlfaY8fdIweTyMH3rrbfSNNtL6a9tdPtYLu4/wBdcRwqskn+8wGT+NJPo+mXV+l9c6daTXcalUuJIFaRVPUBiMgVPb+ulh9zyvTvEF1Fo/h/T9U1ZoLCa/v7SS9u5HO4xSMsMbuHU8jPVudozmpdS0y1jn8LR3euf21DHr8ka3JdgIkaF28oOXO4KcDduJ7Z4r0keG9DXTpNPXRtPFlK2+S2Fqnlu3qVxgn3qSTRNKl0+Kxl0yzezhIaK3a3UxxkdCq4wMe1C6P0/QH1+f43/wAzzbX9Puoda1vwzp8tyZNdhjvdImFw+IHXiUBs8KMBsdDvx3p1z4ivrv4d6h4u0yKa2nt7KK0VBu/csGAuHCnglckAkcbDXqRhiaZJmiQyopVHKjcoOMgHsDgfkKSO3hihMUUMaRsWJRVAUliSePckk/WjZf1/W2g+t/6/q6ODvxJZeLtJg0O9u5dP1TTbl7tPtcjgKiqY51YsSrEtjIIzmuWmspT8FtN1qW+vr2+vDYCQXd27xki4Ug4OQp7EgZ9cnNetW+haRZwSwWmlWUEUw2yxxW6Ksg9GAHPXvUZ8NaEdOTTzounGyjbelsbRPLVvULjAPvR/wPzb/UXT+uxh+BLuDVFvry7EsWupOYtRtpHP+jOOioucBCACGH3upOaoeN72N9dmsUvpkmh0iW5MLXRt4YhuwJty/MzgjAHQcnIzz2MGjaXbag9/babZw3kihXuI4FWRgBgAsBkjFLdaRpt9dQ3N7p9rc3EAIilmgV3jz12kjI/ClJXX9dv6/rUcXZ3/AK3PNtKnn8Q6p4KS71W+eO/0GSS8WC9dBK6rFydpGG+Y5Iwfeq+i69dfZvC+karqFwNPuNSv7Wa5knYPJ5LsIYmkznn65O0D1r0qPw3ocMkEkOi6fG9uhSFltUBiU9VU44HsKP8AhGtCGnvYDRdO+xyP5j232RPLZv7xXGCfeqbu7+v53/AlbW/raxzHw7S2t9c8Y21rOZVTV+N8xkYDyY+5JJAPH4YpnhMFviv4zk1Di9U2yW6t1FtsyCvsWznHeuzstL0/Td/9nWNtaeZjf5EKpuwMDOBzgcVHfaLpupTxz31jBPNECI5WQb0B6gN1A9qW1vS35f5D3v5u/wCJkeO5VbwPrYgZWkS3KS7eSqnG4H/gJzVbx9cFNL0vTba6ltJdQ1CGESwzNEUiU75DkEcbEYenNdJLplnLpcunG3jW0mjaN4kUKpVhg8D61TtdGjmtLJNctre8udOf/R7iRAxyBgSDP3WI6++fajr9wdPvKmu8eK/C5T7xup1OP7n2eQn9Qv6VX+Jks0Hw21u4tbie2nhtjJHLBK0bKR7qQfwrX/s+S48RDUbraEtYmitUBzy2C7n34AA9AfWrV9p9nqdo1rqVpBeW7/ehuIhIjfVSMUmropOzXkefeLL3ULC6n1F/N1DSILOEXSWd40N1p55JlVcgSBgeR1+XHSt/4kRR3fwv17epZfsEkijJHIXIraPh/RjOkx0ixMsaqiP9mTcqr90A44A7elXpYo54XinjWSORSro65DA9QQeookuaLXqKHutPseaHTrK88Y+ELOGaVbZtIut/2e5ZWP8AqiRvU7h+BFZmneIbqPR/D2n6pqzQWE1/f2kl5dyOdxikYQxu4dTyM9W52jOa9Nj8N6HDJBJFouno9uhjhZbVAYlPVVOOB7ClHhvQ106TT10bTxZSvvkthap5bt6lcYJ96pu7+/8AO5K0Vv62sUfBtubTSZ7ca4dbjjuX8ufBIjU4PlBizFwucZJJ7dqxtI1HUZ9X8ValpUb6myXiWlrYy3xRAsYxI67shcuX6DnbXaW1rb2VrHbWcEdvBGNqRRIFVR6ADgVFZaZY6aJhp9pDbefIZZfKjC73JyWOOpPrS6/L/IfQ4DxGr6h4q8F3XiTT002Y3dyjxC88xVUQMR842jnGaw9Sje30PxzrNpe3dvdWGuiS3MFy6IDiEHKqQHyDj5sjHTHNeuXmnWWorGuoWdvdLE4kjE8SvscdGGRwfeqjeF9AeGeJ9D01o7l/MnQ2kZWVv7zDHzH3NC0d/wCun+Q3rb+u/wDmWtQsk1TSZ7OZnRbiIoWjcqy5HUEcgjrXkNvrN3p0Oi6peJcMPDM503XDvciQsxjSTbnDEfK+f9sV7NHGkMSxxIqIgCqqjAUegFRtZWrxyo9tCyTNvlUxgiRuOSO54HJ9BR1v/X9WuL7Nn/X9aHAubmy8R+GtI1F5re01YXVzcgTMu+4IDLDuByFUE4UHB2isTUbjVI4LyxXU79bOz8U2tpaXK3L7zE5QvEWz84UnHzZx07V6ve6fZ6lb+RqNpBdw7g3lzxh1yOhwRjNV59A0e6tIbW60mxmt4G3RQyWyMkZ9VUjAP0oWjX9dU/y09Ae39dmv+CcZDHb23j9vCt/f3q6fBpv2q0We+k3zu8jb2Mm4M2wAAAnjr71f+EjRn4a6esUolCSTqW3bj/rn6munvtG0vUxCNS020vBAd0P2iBZPLPquRx+FTWljaafB5Fhaw2sWSfLhjCLk9TgULa39bg9X/Xb+meWzZ0O7+I+p6RG4vLJoWiYMzlAYF3NjPOASefSrM17Y2Om6hrVn42eSzm0ti0NjvmZTkYnHmSvtf5gvOAc89K9CttE0qyu5rqz0yzt7i4/100VuivJ/vEDJ/Gm23h/RrK3uILPSLG3hus/aI4rZFWb/AHgB8340ulvK34WKvr8zzC6nZ5vFukT3GbZvDq3CWov3uMSYk5yf4iFBIHH161e0t7TT9X8AfZ7ryra502ZbgfaDsdvIjYBsnGRkkDtmvQbbQNHs2U2ek2MBWIwKYrZFxGTkpwPu5JOOlNbw7oj2MNk+j6e1pA++G3Nqhjjb1VcYB9xVLR/15/o/wJ/r8F/keWW13Jd6Hpy2ur3iq3jCS1LQXj8wGR8LweRgDHp2rVvFjj8SX3hS88RSaXBbWMb6fLe3MpkfcXLyCTzU3MrYHzZwAPfPey+GtCnAE2i6fIFlM432qHEh5L9Pve/Wpr/RdL1UxHVNNs70wHMRuIFk8s+q7gcfhU9Ev62S/wCCO+rf9bt/8Ai0mL7R4atIrm8Ooh7dVe6KGMzjGN+O2etcR4ain+2zeDbxriWXSL9rp7h5GLTWx+aHL5ySSwUj0jYV6SBgYHSmLDEs7zLEgldQryBRuYDOAT3AyfzNN6u4vs2PLvDV++vRaZq0vipLTVor1ku7AbzJI24qbdozLgD0IQYAz61a8KalY+IdN0rUb7Xrm31xbySO7tI7ohnfcymBoieEAweAMBc56mu9j0XS4tUfU4tNs0v3GHu1gUSsPQvjJ/Oli0fTINSk1GHTrSO+lGJLpIFErj3fGT+dC/r+v6/O4+v9f1/Xy8k0y7TQfCmLKVrWO48UyWl/LHMwaG2NwwznPyZO1d3B+brWn4rkv9JuvFlhpl7eLp8eg/blK3T7rS43MAFfOQGC525xxnHNejro2mILsLp1oovTm6AgX9+fV+Pm/HNM/sDRxp0mnjSbH7FIcvbfZk8t/quMGps+W39bW/4PqVdc1/P9b/8AA9DO8M21npdtBG2pzz3mpQpceVd3hkZiEXcUVjwOhIXgZ7V0NVIdJ063e3eCwtYmtUMduyQqDCp6quB8oOOgq3Wknd3IirKwUUUVIwooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//9k=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = Image(r'C:\\Users\\welcome\\Desktop\\AndrejKarpathy\\net.JPG')\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10ef1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read bengio paper\n",
    "#https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "#Taking 3 previous words & predicting 4th word in a given sequence\n",
    "#these words were part of Vocabulary(17K). each words are index from 0 to size of Vocabulary\n",
    "#We also have lookup vector matrix of size 30 i.e. Size of Vocabulary(17K) * 30 i.e feature vector\n",
    "#we pull the corresponding index row from lookup table which represents the vector represent of the word\n",
    "#so, 30 * 3 = 90 parameters\n",
    "#then we have hidden layers based on hyperparameter tuning\n",
    "#output layer with size of Vocabulary predicts prob for each of words in vocab and passed to softmax fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ade2d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "789b5494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets try to implement bigram model using Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc4d7d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd2f072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt').read().splitlines() #reading text file and split data and store as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06960cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d6acd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:8]\n",
    "#note we have 32k words in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f081ac0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf93a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2791f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ------> e\n",
      "..e ------> m\n",
      ".em ------> m\n",
      "emm ------> a\n",
      "mma ------> .\n",
      "olivia\n",
      "... ------> o\n",
      "..o ------> l\n",
      ".ol ------> i\n",
      "oli ------> v\n",
      "liv ------> i\n",
      "ivi ------> a\n",
      "via ------> .\n",
      "ava\n",
      "... ------> a\n",
      "..a ------> v\n",
      ".av ------> a\n",
      "ava ------> .\n",
      "isabella\n",
      "... ------> i\n",
      "..i ------> s\n",
      ".is ------> a\n",
      "isa ------> b\n",
      "sab ------> e\n",
      "abe ------> l\n",
      "bel ------> l\n",
      "ell ------> a\n",
      "lla ------> .\n",
      "sophia\n",
      "... ------> s\n",
      "..s ------> o\n",
      ".so ------> p\n",
      "sop ------> h\n",
      "oph ------> i\n",
      "phi ------> a\n",
      "hia ------> .\n"
     ]
    }
   ],
   "source": [
    "block_size = 3 #defines no of characters do we take to predict the next one? i.e. context length \n",
    "X, Y = [], [] # X are inputs to neural net & y is output\n",
    "\n",
    "for w in words[:5]: #take 1st 5 words and do it, later change to all words in vocab\n",
    "    print(w)\n",
    "    context = [0] * block_size #fist list is [...] \n",
    "    for ch in w + '.': #append . at the end of each char to identify end of word\n",
    "        ix = stoi[ch] #current character\n",
    "        X.append(context) #current 3 char bigram\n",
    "        Y.append(ix) #append corresponding output\n",
    "        print(''.join(itos[i] for i in context), '------>', itos[ix])\n",
    "        context = context[1:] + [ix] # remove 1st and add new char at end to get next set of 3 gram\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02179c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77e1efbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d781b173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80cc67ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cd3fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from 5 words, we created 32 different 3 char grams  & output of 32 labels i.e. X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e6f72bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we need to build the Embedding K dimensional Vector lookup table/matrix\n",
    "#in the paper, we had 17000 words into 30 dim lookup table spaces\n",
    "#for our use case, we convert into 2-dim lookup spaces to start with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ffd0fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27, 2)) # 27 characters, each having 2 dimensional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53119ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3811, -2.3058])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8f2062d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5518, -2.7745],\n",
       "        [ 0.1137,  0.8131],\n",
       "        [-0.1437,  1.3491],\n",
       "        [-0.4776,  0.0066],\n",
       "        [ 1.1734,  0.8163],\n",
       "        [-0.3811, -2.3058],\n",
       "        [ 1.9296, -0.9842],\n",
       "        [-0.8469,  0.1822],\n",
       "        [ 1.3014,  0.4948],\n",
       "        [-0.3776, -1.1845],\n",
       "        [-0.3624,  0.0178],\n",
       "        [ 1.2107,  1.0093],\n",
       "        [ 1.8541, -0.1154],\n",
       "        [ 0.7312, -1.1715],\n",
       "        [-1.2039, -1.5440],\n",
       "        [ 0.6417, -2.2491],\n",
       "        [-0.0514, -1.3565],\n",
       "        [-0.5864,  0.9522],\n",
       "        [ 0.1695, -2.2090],\n",
       "        [-0.3813, -1.0885],\n",
       "        [-0.0313,  0.4521],\n",
       "        [ 0.3518, -0.4101],\n",
       "        [-0.9862,  0.2956],\n",
       "        [ 0.5426,  1.9128],\n",
       "        [-0.3417, -1.0265],\n",
       "        [-1.8595, -0.6128],\n",
       "        [-2.7499,  0.0711]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82208ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3811, -2.3058])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for each rows of X, we have 2 dim lookup matrix C\n",
    "#we need to embed all of integers in X using C, but let us take 1 example and see how it works\n",
    "#lets say we need to embed Row \n",
    "C[5] #direct referencing i.e finding embedding feature vector for 5th character from input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed4b2d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternatively...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60444038",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "one_hot(): argument 'input' (position 1) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m27\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: one_hot(): argument 'input' (position 1) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "F.one_hot(5, num_classes=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec8bf6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "696831eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m27\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27) @ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee642707",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9390e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "C.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60372902",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C #note using one hot we lookup 5th row from C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "955b41ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3811, -2.3058])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[5] #notice how using 1-hot and direct referencing bring same value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc5a3db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#So, 2 ways for indexing...we use direct referencing since much faster than one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f77a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding a single integer is easy... We can simply refer C[5]\n",
    "#but input X has shape of 32 rows, 3 columns\n",
    "#pytorch has powerful indexing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98a1d7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3811, -2.3058],\n",
       "        [ 1.9296, -0.9842],\n",
       "        [-0.8469,  0.1822]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can index using list\n",
    "C[[5,6,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7f1a60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3811, -2.3058],\n",
       "        [ 1.9296, -0.9842],\n",
       "        [-0.8469,  0.1822],\n",
       "        [-0.8469,  0.1822],\n",
       "        [-0.8469,  0.1822]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[[5,6,7,7,7,]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80475362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3811, -2.3058],\n",
       "        [ 1.9296, -0.9842],\n",
       "        [-0.8469,  0.1822]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[torch.tensor([5,6,7])] #index can be with tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fb1202d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3811, -2.3058],\n",
       "        [ 1.9296, -0.9842],\n",
       "        [-0.8469,  0.1822],\n",
       "        [-0.8469,  0.1822],\n",
       "        [-0.8469,  0.1822]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[torch.tensor([5,6,7,7,7])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7d9cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can also do multi dim indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "745f8b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5518, -2.7745],\n",
       "         [ 0.5518, -2.7745],\n",
       "         [ 0.5518, -2.7745]],\n",
       "\n",
       "        [[ 0.5518, -2.7745],\n",
       "         [ 0.5518, -2.7745],\n",
       "         [-0.3811, -2.3058]],\n",
       "\n",
       "        [[ 0.5518, -2.7745],\n",
       "         [-0.3811, -2.3058],\n",
       "         [ 0.7312, -1.1715]],\n",
       "\n",
       "        [[-0.3811, -2.3058],\n",
       "         [ 0.7312, -1.1715],\n",
       "         [ 0.7312, -1.1715]],\n",
       "\n",
       "        [[ 0.7312, -1.1715],\n",
       "         [ 0.7312, -1.1715],\n",
       "         [ 0.1137,  0.8131]],\n",
       "\n",
       "        [[ 0.5518, -2.7745],\n",
       "         [ 0.5518, -2.7745],\n",
       "         [ 0.5518, -2.7745]],\n",
       "\n",
       "        [[ 0.5518, -2.7745],\n",
       "         [ 0.5518, -2.7745],\n",
       "         [ 0.6417, -2.2491]],\n",
       "\n",
       "        [[ 0.5518, -2.7745],\n",
       "         [ 0.6417, -2.2491],\n",
       "         [ 1.8541, -0.1154]],\n",
       "\n",
       "        [[ 0.6417, -2.2491],\n",
       "         [ 1.8541, -0.1154],\n",
       "         [-0.3776, -1.1845]],\n",
       "\n",
       "        [[ 1.8541, -0.1154],\n",
       "         [-0.3776, -1.1845],\n",
       "         [-0.9862,  0.2956]],\n",
       "\n",
       "        [[-0.3776, -1.1845],\n",
       "         [-0.9862,  0.2956],\n",
       "         [-0.3776, -1.1845]],\n",
       "\n",
       "        [[-0.9862,  0.2956],\n",
       "         [-0.3776, -1.1845],\n",
       "         [ 0.1137,  0.8131]],\n",
       "\n",
       "        [[ 0.5518, -2.7745],\n",
       "         [ 0.5518, -2.7745],\n",
       "         [ 0.5518, -2.7745]],\n",
       "\n",
       "        [[ 0.5518, -2.7745],\n",
       "         [ 0.5518, -2.7745],\n",
       "         [ 0.1137,  0.8131]],\n",
       "\n",
       "        [[ 0.5518, -2.7745],\n",
       "         [ 0.1137,  0.8131],\n",
       "         [-0.9862,  0.2956]],\n",
       "\n",
       "        [[ 0.1137,  0.8131],\n",
       "         [-0.9862,  0.2956],\n",
       "         [ 0.1137,  0.8131]],\n",
       "\n",
       "        [[ 0.5518, -2.7745],\n",
       "         [ 0.5518, -2.7745],\n",
       "         [ 0.5518, -2.7745]],\n",
       "\n",
       "        [[ 0.5518, -2.7745],\n",
       "         [ 0.5518, -2.7745],\n",
       "         [-0.3776, -1.1845]],\n",
       "\n",
       "        [[ 0.5518, -2.7745],\n",
       "         [-0.3776, -1.1845],\n",
       "         [-0.3813, -1.0885]],\n",
       "\n",
       "        [[-0.3776, -1.1845],\n",
       "         [-0.3813, -1.0885],\n",
       "         [ 0.1137,  0.8131]],\n",
       "\n",
       "        [[-0.3813, -1.0885],\n",
       "         [ 0.1137,  0.8131],\n",
       "         [-0.1437,  1.3491]],\n",
       "\n",
       "        [[ 0.1137,  0.8131],\n",
       "         [-0.1437,  1.3491],\n",
       "         [-0.3811, -2.3058]],\n",
       "\n",
       "        [[-0.1437,  1.3491],\n",
       "         [-0.3811, -2.3058],\n",
       "         [ 1.8541, -0.1154]],\n",
       "\n",
       "        [[-0.3811, -2.3058],\n",
       "         [ 1.8541, -0.1154],\n",
       "         [ 1.8541, -0.1154]],\n",
       "\n",
       "        [[ 1.8541, -0.1154],\n",
       "         [ 1.8541, -0.1154],\n",
       "         [ 0.1137,  0.8131]],\n",
       "\n",
       "        [[ 0.5518, -2.7745],\n",
       "         [ 0.5518, -2.7745],\n",
       "         [ 0.5518, -2.7745]],\n",
       "\n",
       "        [[ 0.5518, -2.7745],\n",
       "         [ 0.5518, -2.7745],\n",
       "         [-0.3813, -1.0885]],\n",
       "\n",
       "        [[ 0.5518, -2.7745],\n",
       "         [-0.3813, -1.0885],\n",
       "         [ 0.6417, -2.2491]],\n",
       "\n",
       "        [[-0.3813, -1.0885],\n",
       "         [ 0.6417, -2.2491],\n",
       "         [-0.0514, -1.3565]],\n",
       "\n",
       "        [[ 0.6417, -2.2491],\n",
       "         [-0.0514, -1.3565],\n",
       "         [ 1.3014,  0.4948]],\n",
       "\n",
       "        [[-0.0514, -1.3565],\n",
       "         [ 1.3014,  0.4948],\n",
       "         [-0.3776, -1.1845]],\n",
       "\n",
       "        [[ 1.3014,  0.4948],\n",
       "         [-0.3776, -1.1845],\n",
       "         [ 0.1137,  0.8131]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X] #since X is 2 dim array of size (32, 3) i.e. 32 different trigrams generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b621a8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "576fa43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[13, 2] #integer 1 as input i.e. char a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88fb0fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1137, 0.8131])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X][13, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a565eca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1137, 0.8131])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[1] #see we pulled correct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "385c6a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] #very simple to reduce input dimension to feature space vector through indexing\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "751c7880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[1]: tensor([0, 0, 5])\n",
      "C[0]: tensor([ 0.5518, -2.7745])\n",
      "C[5]: tensor([-0.3811, -2.3058])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745],\n",
       "        [-0.3811, -2.3058]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('X[1]:', X[1]) #i.e. \". . e\" indexez 0 0 5--> 3 gram\n",
    "print('C[0]:', C[0]) #randomly generated values for feature space for char a with 0 index\n",
    "print('C[5]:', C[5]) #randomly generated values for feature space for char e with 5 index\n",
    "emb[1] # is nothing but one hot pulling of respect X & C rows based on dimension\n",
    "#understand this how embedding is done....\n",
    "#note the size of each embed vector of size (3, 2) for each trigram letter, 2 dimensional feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d438da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input layer is completed \n",
    "#we have take list of words, converted into different 3 grams & its output as next char\n",
    "# we converted each 3 grams into index\n",
    "#then we created 2 dim lookup embedding layer and using that we pull respecting lookup embedding by indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be32508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we construct hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1dac0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn(6, 100) # since we have 3, 2 dimensional embed vector coming from input layer, taking 100 as hidden neurons\n",
    "b1 = torch.randn(100) #bias for each 100 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "189028dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43memb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[38;5;241m+\u001b[39mb1\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)"
     ]
    }
   ],
   "source": [
    "emb @ W1+b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9044365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#why emb layer shape is (32, 3, 2) will not match with (6, 100)\n",
    "#somehow we need to transform to match the shape\n",
    "#there are many ways, but we go with 'cat' function in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db6091ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745],\n",
       "        [-0.3811, -2.3058],\n",
       "        [ 0.7312, -1.1715],\n",
       "        [ 0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745],\n",
       "        [ 0.6417, -2.2491],\n",
       "        [ 1.8541, -0.1154],\n",
       "        [-0.3776, -1.1845],\n",
       "        [-0.9862,  0.2956],\n",
       "        [ 0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745],\n",
       "        [ 0.1137,  0.8131],\n",
       "        [ 0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745],\n",
       "        [-0.3776, -1.1845],\n",
       "        [-0.3813, -1.0885],\n",
       "        [ 0.1137,  0.8131],\n",
       "        [-0.1437,  1.3491],\n",
       "        [-0.3811, -2.3058],\n",
       "        [ 1.8541, -0.1154],\n",
       "        [ 0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745],\n",
       "        [-0.3813, -1.0885],\n",
       "        [ 0.6417, -2.2491],\n",
       "        [-0.0514, -1.3565],\n",
       "        [ 1.3014,  0.4948]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[:, 0, :] #fetch all rows, 0th index & all dimensions\n",
    "#this will fetch 32*2 embedings of just the first char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f75147df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5518, -2.7745,  0.5518, -2.7745,  0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745, -0.3811, -2.3058],\n",
       "        [ 0.5518, -2.7745, -0.3811, -2.3058,  0.7312, -1.1715],\n",
       "        [-0.3811, -2.3058,  0.7312, -1.1715,  0.7312, -1.1715],\n",
       "        [ 0.7312, -1.1715,  0.7312, -1.1715,  0.1137,  0.8131],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745,  0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745,  0.6417, -2.2491],\n",
       "        [ 0.5518, -2.7745,  0.6417, -2.2491,  1.8541, -0.1154],\n",
       "        [ 0.6417, -2.2491,  1.8541, -0.1154, -0.3776, -1.1845],\n",
       "        [ 1.8541, -0.1154, -0.3776, -1.1845, -0.9862,  0.2956],\n",
       "        [-0.3776, -1.1845, -0.9862,  0.2956, -0.3776, -1.1845],\n",
       "        [-0.9862,  0.2956, -0.3776, -1.1845,  0.1137,  0.8131],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745,  0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745,  0.1137,  0.8131],\n",
       "        [ 0.5518, -2.7745,  0.1137,  0.8131, -0.9862,  0.2956],\n",
       "        [ 0.1137,  0.8131, -0.9862,  0.2956,  0.1137,  0.8131],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745,  0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745, -0.3776, -1.1845],\n",
       "        [ 0.5518, -2.7745, -0.3776, -1.1845, -0.3813, -1.0885],\n",
       "        [-0.3776, -1.1845, -0.3813, -1.0885,  0.1137,  0.8131],\n",
       "        [-0.3813, -1.0885,  0.1137,  0.8131, -0.1437,  1.3491],\n",
       "        [ 0.1137,  0.8131, -0.1437,  1.3491, -0.3811, -2.3058],\n",
       "        [-0.1437,  1.3491, -0.3811, -2.3058,  1.8541, -0.1154],\n",
       "        [-0.3811, -2.3058,  1.8541, -0.1154,  1.8541, -0.1154],\n",
       "        [ 1.8541, -0.1154,  1.8541, -0.1154,  0.1137,  0.8131],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745,  0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745, -0.3813, -1.0885],\n",
       "        [ 0.5518, -2.7745, -0.3813, -1.0885,  0.6417, -2.2491],\n",
       "        [-0.3813, -1.0885,  0.6417, -2.2491, -0.0514, -1.3565],\n",
       "        [ 0.6417, -2.2491, -0.0514, -1.3565,  1.3014,  0.4948],\n",
       "        [-0.0514, -1.3565,  1.3014,  0.4948, -0.3776, -1.1845],\n",
       "        [ 1.3014,  0.4948, -0.3776, -1.1845,  0.1137,  0.8131]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1) #concatenate across dimension 1 i.e. column\n",
    "#basically we are pulling each char embedding seperately and concatnating across dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36ceb34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have shape of 32 * 6 which can be multiplied with 6 * 100\n",
    "#final output will be 32 *100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79fc92e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1)\n",
    "#above code is not dynamic, since currently trigrams we extracted each char embedded manually\n",
    "#how abot change block_size = 5 or 7  ??\n",
    "#we use torch unbind function for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "69675a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5518, -2.7745,  0.5518, -2.7745,  0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745, -0.3811, -2.3058],\n",
       "        [ 0.5518, -2.7745, -0.3811, -2.3058,  0.7312, -1.1715],\n",
       "        [-0.3811, -2.3058,  0.7312, -1.1715,  0.7312, -1.1715],\n",
       "        [ 0.7312, -1.1715,  0.7312, -1.1715,  0.1137,  0.8131],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745,  0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745,  0.6417, -2.2491],\n",
       "        [ 0.5518, -2.7745,  0.6417, -2.2491,  1.8541, -0.1154],\n",
       "        [ 0.6417, -2.2491,  1.8541, -0.1154, -0.3776, -1.1845],\n",
       "        [ 1.8541, -0.1154, -0.3776, -1.1845, -0.9862,  0.2956],\n",
       "        [-0.3776, -1.1845, -0.9862,  0.2956, -0.3776, -1.1845],\n",
       "        [-0.9862,  0.2956, -0.3776, -1.1845,  0.1137,  0.8131],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745,  0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745,  0.1137,  0.8131],\n",
       "        [ 0.5518, -2.7745,  0.1137,  0.8131, -0.9862,  0.2956],\n",
       "        [ 0.1137,  0.8131, -0.9862,  0.2956,  0.1137,  0.8131],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745,  0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745, -0.3776, -1.1845],\n",
       "        [ 0.5518, -2.7745, -0.3776, -1.1845, -0.3813, -1.0885],\n",
       "        [-0.3776, -1.1845, -0.3813, -1.0885,  0.1137,  0.8131],\n",
       "        [-0.3813, -1.0885,  0.1137,  0.8131, -0.1437,  1.3491],\n",
       "        [ 0.1137,  0.8131, -0.1437,  1.3491, -0.3811, -2.3058],\n",
       "        [-0.1437,  1.3491, -0.3811, -2.3058,  1.8541, -0.1154],\n",
       "        [-0.3811, -2.3058,  1.8541, -0.1154,  1.8541, -0.1154],\n",
       "        [ 1.8541, -0.1154,  1.8541, -0.1154,  0.1137,  0.8131],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745,  0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745, -0.3813, -1.0885],\n",
       "        [ 0.5518, -2.7745, -0.3813, -1.0885,  0.6417, -2.2491],\n",
       "        [-0.3813, -1.0885,  0.6417, -2.2491, -0.0514, -1.3565],\n",
       "        [ 0.6417, -2.2491, -0.0514, -1.3565,  1.3014,  0.4948],\n",
       "        [-0.0514, -1.3565,  1.3014,  0.4948, -0.3776, -1.1845],\n",
       "        [ 1.3014,  0.4948, -0.3776, -1.1845,  0.1137,  0.8131]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(torch.unbind(emb, 1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "201cc591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is exactly as same as previous manually conatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "db4465ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can do the same things n methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "baaa0b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#say, we have\n",
    "a = torch.arange(18)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "197504d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d45752b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [ 2,  3],\n",
       "        [ 4,  5],\n",
       "        [ 6,  7],\n",
       "        [ 8,  9],\n",
       "        [10, 11],\n",
       "        [12, 13],\n",
       "        [14, 15],\n",
       "        [16, 17]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(9,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ea3b2e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "        [ 9, 10, 11, 12, 13, 14, 15, 16, 17]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(2, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d6b08b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [14, 15],\n",
       "         [16, 17]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(3, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "256d6107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#as long as given size matches, it will work\n",
    "#in torch, calling this view is very efficient because of its speed\n",
    "#reason for this is every tensor has storage, which is one dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02632aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.storage of tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage\n",
    "#refer: blog.ezyang.com/2019/05/pytorch-internals/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83fb3e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c0280964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5518, -2.7745,  0.5518, -2.7745,  0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745, -0.3811, -2.3058],\n",
       "        [ 0.5518, -2.7745, -0.3811, -2.3058,  0.7312, -1.1715],\n",
       "        [-0.3811, -2.3058,  0.7312, -1.1715,  0.7312, -1.1715],\n",
       "        [ 0.7312, -1.1715,  0.7312, -1.1715,  0.1137,  0.8131],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745,  0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745,  0.6417, -2.2491],\n",
       "        [ 0.5518, -2.7745,  0.6417, -2.2491,  1.8541, -0.1154],\n",
       "        [ 0.6417, -2.2491,  1.8541, -0.1154, -0.3776, -1.1845],\n",
       "        [ 1.8541, -0.1154, -0.3776, -1.1845, -0.9862,  0.2956],\n",
       "        [-0.3776, -1.1845, -0.9862,  0.2956, -0.3776, -1.1845],\n",
       "        [-0.9862,  0.2956, -0.3776, -1.1845,  0.1137,  0.8131],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745,  0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745,  0.1137,  0.8131],\n",
       "        [ 0.5518, -2.7745,  0.1137,  0.8131, -0.9862,  0.2956],\n",
       "        [ 0.1137,  0.8131, -0.9862,  0.2956,  0.1137,  0.8131],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745,  0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745, -0.3776, -1.1845],\n",
       "        [ 0.5518, -2.7745, -0.3776, -1.1845, -0.3813, -1.0885],\n",
       "        [-0.3776, -1.1845, -0.3813, -1.0885,  0.1137,  0.8131],\n",
       "        [-0.3813, -1.0885,  0.1137,  0.8131, -0.1437,  1.3491],\n",
       "        [ 0.1137,  0.8131, -0.1437,  1.3491, -0.3811, -2.3058],\n",
       "        [-0.1437,  1.3491, -0.3811, -2.3058,  1.8541, -0.1154],\n",
       "        [-0.3811, -2.3058,  1.8541, -0.1154,  1.8541, -0.1154],\n",
       "        [ 1.8541, -0.1154,  1.8541, -0.1154,  0.1137,  0.8131],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745,  0.5518, -2.7745],\n",
       "        [ 0.5518, -2.7745,  0.5518, -2.7745, -0.3813, -1.0885],\n",
       "        [ 0.5518, -2.7745, -0.3813, -1.0885,  0.6417, -2.2491],\n",
       "        [-0.3813, -1.0885,  0.6417, -2.2491, -0.0514, -1.3565],\n",
       "        [ 0.6417, -2.2491, -0.0514, -1.3565,  1.3014,  0.4948],\n",
       "        [-0.0514, -1.3565,  1.3014,  0.4948, -0.3776, -1.1845],\n",
       "        [ 1.3014,  0.4948, -0.3776, -1.1845,  0.1137,  0.8131]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(32, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0cb24975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(32, 6) == torch.cat(torch.unbind(emb, 1), 1)\n",
    "#notice all values match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c7ae3d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we have shape of 32 * 6 which can be multiplied with 6 * 100\n",
    "#final output will be 32 *100 \n",
    "\n",
    "h = emb.view(32, 6) @ W1 + b1\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "20ab447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dynamic\n",
    "#h = emb.view(emb.shape[0], 6) @ W1 + b1   or\n",
    "h = emb.view(-1, 6) @ W1 + b1 # -1 pytorch will infer the shape and substitute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "68b41ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying tanh non-linearity \n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4236cff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6220, -0.9719, -0.7335,  ...,  0.9998,  0.9714, -1.0000],\n",
       "        [-0.1017, -0.9877,  0.1331,  ...,  0.9999,  0.9391, -1.0000],\n",
       "        [ 0.6104, -0.0460, -0.9774,  ...,  0.9993,  0.9823, -1.0000],\n",
       "        ...,\n",
       "        [ 0.1101,  0.9998, -0.8803,  ...,  0.9801,  0.9534, -0.9982],\n",
       "        [-0.9219,  0.9402,  0.9112,  ...,  0.9921, -0.5225, -0.1126],\n",
       "        [ 0.6361,  0.4983,  0.4296,  ..., -0.6538,  0.9367, -0.9837]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3f281009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fc4baeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b6642b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn(100, 27) #100 neurons from previous hidden layyer & 27 output for each characters\n",
    "b2 = torch.randn(27) # for each output 27 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4d6842ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fffac9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "75cb9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6ff2a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = counts / counts.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "96e6e5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "18b062fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1b150e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.5538e-09, 1.8068e-10, 8.7513e-12, 9.0693e-08, 1.0914e-10, 6.5721e-01,\n",
       "        2.3180e-08, 6.3663e-13, 8.9007e-14, 2.7387e-14, 1.0836e-02, 1.3015e-06,\n",
       "        2.4762e-06, 3.9192e-07, 8.2852e-12, 1.4261e-02, 5.4215e-09, 6.5086e-13,\n",
       "        2.0945e-03, 1.1840e-03, 1.6622e-07, 3.6648e-10, 2.8984e-05, 1.9983e-11,\n",
       "        8.8203e-08, 1.4262e-09, 9.9738e-01, 7.0366e-10, 2.2123e-03, 2.0402e-10,\n",
       "        9.1759e-09, 1.3052e-11])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[torch.arange(32), Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d772d7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.2337)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8af69293",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########---------------summarizing all these codes----------------------##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "445925ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "24d97ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e221680c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] # 32, 3, 2\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) #32, 100\n",
    "logits = h @ W2 + b2 # 32, 27\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdims = True)\n",
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "21c889a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instead manuall calculating, use pytorch cross_entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d3311f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "56224d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] # 32, 3, 2\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) #32, 100\n",
    "logits = h @ W2 + b2 # 32, 27\n",
    "#counts = logits.exp()\n",
    "#prob = counts / counts.sum(1, keepdims = True)\n",
    "#loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "29e2e740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., nan])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#always go with inbuilt functions, it avoid intermediate tensors and much more efficeint when backpropagating\n",
    "#exp creates problem when we have larger positive values as shown below\n",
    "#makes to infinity\n",
    "logits = torch.tensor([-100, -3, 0, 100])\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum()\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c6430db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.7835e-44, 4.9787e-02, 1.0000e+00,        inf])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c85d55e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.5079e-05, 3.3309e-04, 6.6903e-03, 9.9293e-01])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how pytorch handles this ?? By offestting\n",
    "\n",
    "logits = torch.tensor([-5, -3, 0, 5]) - 5 \n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum()\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "08fa1a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice we found max positive value in logits and we subtracted across all tensor values\n",
    "#we take max positive values, because only positive values are impacted by exp fn\n",
    "#These are all handled in pytorch inbuilt functions... always go with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5ba34d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "39461712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25614026188850403\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "    #forward pass\n",
    "    emb = C[X] # 32, 3, 2\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) #32, 100\n",
    "    logits = h @ W2 + b2 # 32, 27\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7a68a031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([13.3349, 17.7903, 20.6014, 20.6123, 16.7355, 13.3349, 15.9982, 14.1721,\n",
       "        15.9144, 18.3614, 15.9394, 20.9265, 13.3349, 17.1091, 17.1320, 20.0602,\n",
       "        13.3349, 16.5894, 15.1017, 17.0582, 18.5859, 15.9669, 10.8740, 10.6870,\n",
       "        15.5056, 13.3349, 16.1795, 16.9742, 12.7426, 16.2010, 19.0844, 16.0197],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([19, 13, 13,  1,  0, 19, 12,  9, 22,  9,  1,  0, 19, 22,  1,  0, 19, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#very low loss, may be overfitting... we have only 32 inputs and >3K parameters to train\n",
    "#also notice loss might not go to 0???\n",
    "logits.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6e68ced5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0cb003af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note the output indcies are very close to Y, but for some cases it will differ..\n",
    "#why ?\n",
    "# ... --> e , ... --> o etc....\n",
    "#model not able to understand what exactly follows next to \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "110f8928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets run for full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6a8de5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3 #defines no of characters do we take to predict the next one? i.e. context length \n",
    "X, Y = [], [] # X are inputs to neural net & y is output\n",
    "\n",
    "for w in words: #take 1st 5 words and do it, later change to all words in vocab\n",
    "    #print(w)\n",
    "    context = [0] * block_size #fist list is [...] \n",
    "    for ch in w + '.': #append . at the end of each char to identify end of word\n",
    "        ix = stoi[ch] #current character\n",
    "        X.append(context) #current 3 char bigram\n",
    "        Y.append(ix) #append corresponding output\n",
    "        #print(''.join(itos[i] for i in context), '------>', itos[ix])\n",
    "        context = context[1:] + [ix] # remove 1st and add new char at end to get next set of 3 gram\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "753dd01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a44a6658",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "39ac931d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fb83cd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a2431800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.505226135253906\n",
      "17.084484100341797\n",
      "15.776531219482422\n",
      "14.833340644836426\n",
      "14.002605438232422\n",
      "13.253263473510742\n",
      "12.579920768737793\n",
      "11.983102798461914\n",
      "11.47049331665039\n",
      "11.05185604095459\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    #forward pass\n",
    "    emb = C[X] # 20k, 3, 2\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) #32, 100\n",
    "    logits = h @ W2 + b2 # 32, 27\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(loss.item())\n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5417ffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice how is the slowness in execution\n",
    "#Why ? because each time we forward pass & back propagate the entire vocab\n",
    "#to avoid this, we go for Mini-Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6fa39ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 3, 0, 2, 0, 3, 3, 1, 4, 0, 4, 0, 1, 0, 1, 0, 0, 0, 4, 0, 1, 4, 4,\n",
       "        1, 3, 3, 3, 3, 4, 3, 0])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0, 5, (32,)) # generate a seq of 32 no, within range of 0 - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9984a6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 75742, 156264,  53545,  72398, 211931, 108077,  86556,  23515,    415,\n",
       "         49513,   5636, 158838, 118937,  80390, 173835,  75917, 180864,  99353,\n",
       "        121669, 163209, 134389, 121900,  72775, 219647, 138289, 173017,  48982,\n",
       "        211106, 131426,  73903,   3233,  59391])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0, X.shape[0], (32,)) # range should be based on input X, hence replacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5159cf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5880894660949707\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    #mini-batch\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # we always take random 32 3 grams\n",
    "    #forward pass\n",
    "    emb = C[X[ix]] # 32, 3, 2 #only for those 32 trigrams we use embeding \n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) #32, 100\n",
    "    logits = h @ W2 + b2 # 32, 27\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "10b2ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice loss is decreasing much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "40ac4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to find optimal Learning Rate??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e798251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lre = torch.linspace(-3,0,1000)\n",
    "lrs = 10**lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "11321639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6429295539855957\n",
      "4.4282917976379395\n",
      "3.935779094696045\n",
      "3.0473814010620117\n",
      "4.210423946380615\n",
      "3.9744513034820557\n",
      "4.994667053222656\n",
      "3.660200357437134\n",
      "2.9018282890319824\n",
      "3.1173131465911865\n",
      "3.2965853214263916\n",
      "2.6542530059814453\n",
      "3.590480327606201\n",
      "3.286015033721924\n",
      "2.9853434562683105\n",
      "3.1621365547180176\n",
      "2.941763162612915\n",
      "3.520233392715454\n",
      "3.7448227405548096\n",
      "3.4754390716552734\n",
      "4.37583589553833\n",
      "3.641357421875\n",
      "3.3464248180389404\n",
      "4.120748043060303\n",
      "3.779367685317993\n",
      "4.095457077026367\n",
      "3.549668312072754\n",
      "4.193580627441406\n",
      "3.467397689819336\n",
      "3.094820499420166\n",
      "3.507441997528076\n",
      "4.3153510093688965\n",
      "3.9768052101135254\n",
      "3.3396244049072266\n",
      "3.720527410507202\n",
      "3.4206864833831787\n",
      "3.499589204788208\n",
      "3.1026296615600586\n",
      "3.157339096069336\n",
      "3.383962869644165\n",
      "3.080890417098999\n",
      "3.3731117248535156\n",
      "3.9139556884765625\n",
      "3.268319845199585\n",
      "3.5498335361480713\n",
      "2.755221128463745\n",
      "3.030646324157715\n",
      "3.3320720195770264\n",
      "3.743419647216797\n",
      "3.5402424335479736\n",
      "3.2971882820129395\n",
      "3.3087098598480225\n",
      "3.3579564094543457\n",
      "3.4498836994171143\n",
      "4.132833003997803\n",
      "3.7354798316955566\n",
      "2.969670534133911\n",
      "4.4300737380981445\n",
      "3.066075563430786\n",
      "3.5895767211914062\n",
      "3.562133312225342\n",
      "3.303727149963379\n",
      "3.1276674270629883\n",
      "3.291886568069458\n",
      "3.935635566711426\n",
      "3.678471565246582\n",
      "3.04370379447937\n",
      "3.762622594833374\n",
      "3.714677333831787\n",
      "3.3991150856018066\n",
      "3.82041597366333\n",
      "3.2248032093048096\n",
      "3.799503803253174\n",
      "3.872016191482544\n",
      "3.731024742126465\n",
      "4.010011196136475\n",
      "3.1928226947784424\n",
      "3.6815922260284424\n",
      "3.3989853858947754\n",
      "3.2335801124572754\n",
      "3.324812412261963\n",
      "3.1762568950653076\n",
      "4.426131725311279\n",
      "3.91127347946167\n",
      "3.2661235332489014\n",
      "3.1494851112365723\n",
      "4.132532596588135\n",
      "2.8996903896331787\n",
      "3.4090657234191895\n",
      "3.290844440460205\n",
      "4.193548202514648\n",
      "3.4789416790008545\n",
      "3.644883394241333\n",
      "3.3384768962860107\n",
      "3.1212446689605713\n",
      "3.593881845474243\n",
      "3.4878592491149902\n",
      "3.0525312423706055\n",
      "3.9093542098999023\n",
      "3.112048625946045\n",
      "3.8285748958587646\n",
      "4.1705851554870605\n",
      "4.041462421417236\n",
      "4.363536834716797\n",
      "3.984748363494873\n",
      "4.14410924911499\n",
      "4.1142168045043945\n",
      "4.434704303741455\n",
      "2.970134973526001\n",
      "3.413630962371826\n",
      "3.4276528358459473\n",
      "2.9903652667999268\n",
      "2.840341806411743\n",
      "3.006544589996338\n",
      "3.1556570529937744\n",
      "3.4550650119781494\n",
      "2.8545732498168945\n",
      "4.519283294677734\n",
      "3.7772529125213623\n",
      "3.764273166656494\n",
      "4.000394821166992\n",
      "3.75508713722229\n",
      "2.8265066146850586\n",
      "3.2133848667144775\n",
      "3.736670732498169\n",
      "2.7862982749938965\n",
      "4.575716972351074\n",
      "3.4014837741851807\n",
      "2.8057003021240234\n",
      "3.4918272495269775\n",
      "3.0416979789733887\n",
      "3.8929977416992188\n",
      "3.103346586227417\n",
      "3.4796142578125\n",
      "4.422215938568115\n",
      "3.1605520248413086\n",
      "3.6521692276000977\n",
      "3.6500649452209473\n",
      "3.3859124183654785\n",
      "3.0792958736419678\n",
      "3.352888584136963\n",
      "3.123760223388672\n",
      "3.8426270484924316\n",
      "3.6445364952087402\n",
      "3.652386426925659\n",
      "3.0282669067382812\n",
      "3.343391180038452\n",
      "3.1092467308044434\n",
      "3.6434082984924316\n",
      "3.8808798789978027\n",
      "3.5167415142059326\n",
      "2.839595079421997\n",
      "3.9082627296447754\n",
      "3.341157913208008\n",
      "2.9907498359680176\n",
      "4.36450719833374\n",
      "4.734198570251465\n",
      "4.589133262634277\n",
      "3.6098735332489014\n",
      "3.5329642295837402\n",
      "3.573892116546631\n",
      "3.6935477256774902\n",
      "3.186427354812622\n",
      "3.21178936958313\n",
      "3.8373851776123047\n",
      "3.2551004886627197\n",
      "3.991086006164551\n",
      "2.9226274490356445\n",
      "3.228431463241577\n",
      "3.7167460918426514\n",
      "2.928950071334839\n",
      "3.532817840576172\n",
      "3.726203441619873\n",
      "3.241556167602539\n",
      "3.349025249481201\n",
      "4.30997896194458\n",
      "3.0995378494262695\n",
      "3.2720351219177246\n",
      "3.10036563873291\n",
      "3.3159451484680176\n",
      "3.748628616333008\n",
      "3.018015146255493\n",
      "3.21722674369812\n",
      "3.143775701522827\n",
      "3.4417426586151123\n",
      "3.9019711017608643\n",
      "2.922250509262085\n",
      "3.2145578861236572\n",
      "3.6293563842773438\n",
      "3.7024903297424316\n",
      "4.476352214813232\n",
      "3.8561973571777344\n",
      "3.585749387741089\n",
      "3.279587507247925\n",
      "3.700212001800537\n",
      "3.0248734951019287\n",
      "3.2168195247650146\n",
      "3.4447457790374756\n",
      "3.8811302185058594\n",
      "4.271888256072998\n",
      "3.3829121589660645\n",
      "3.1486127376556396\n",
      "3.396527051925659\n",
      "3.53973650932312\n",
      "2.7279722690582275\n",
      "2.8101422786712646\n",
      "3.8243415355682373\n",
      "3.174623489379883\n",
      "3.0398881435394287\n",
      "3.3329949378967285\n",
      "3.3447391986846924\n",
      "4.411139488220215\n",
      "3.4087424278259277\n",
      "3.225252628326416\n",
      "3.625994920730591\n",
      "5.359546661376953\n",
      "3.4919111728668213\n",
      "3.18850040435791\n",
      "4.229095935821533\n",
      "3.4526894092559814\n",
      "3.4413115978240967\n",
      "3.5998852252960205\n",
      "2.712128162384033\n",
      "3.76950740814209\n",
      "3.0165445804595947\n",
      "3.6597776412963867\n",
      "3.58351993560791\n",
      "3.1161746978759766\n",
      "4.0417561531066895\n",
      "3.6727564334869385\n",
      "3.084047317504883\n",
      "3.510374069213867\n",
      "4.025818347930908\n",
      "3.2777678966522217\n",
      "3.1247341632843018\n",
      "3.374626398086548\n",
      "3.0348618030548096\n",
      "2.819334030151367\n",
      "3.1270885467529297\n",
      "3.879621982574463\n",
      "3.515432119369507\n",
      "2.7063496112823486\n",
      "3.005279779434204\n",
      "4.146823883056641\n",
      "3.199965715408325\n",
      "3.036407709121704\n",
      "3.2538015842437744\n",
      "4.379068374633789\n",
      "3.316411256790161\n",
      "3.0453295707702637\n",
      "3.3786401748657227\n",
      "3.175715923309326\n",
      "3.0199084281921387\n",
      "3.782130002975464\n",
      "3.0217907428741455\n",
      "3.682537317276001\n",
      "3.5519418716430664\n",
      "3.417128562927246\n",
      "3.7804996967315674\n",
      "2.784358263015747\n",
      "3.0695438385009766\n",
      "3.6751368045806885\n",
      "3.4880998134613037\n",
      "2.981062650680542\n",
      "3.3467965126037598\n",
      "3.9814226627349854\n",
      "3.657444715499878\n",
      "3.3582370281219482\n",
      "2.4011199474334717\n",
      "3.1644859313964844\n",
      "3.8748929500579834\n",
      "4.276465892791748\n",
      "3.092859983444214\n",
      "3.6518404483795166\n",
      "3.0462324619293213\n",
      "3.581477403640747\n",
      "2.951850175857544\n",
      "3.930553913116455\n",
      "3.483334541320801\n",
      "2.763401508331299\n",
      "3.708756685256958\n",
      "3.9922537803649902\n",
      "2.6733145713806152\n",
      "2.8388736248016357\n",
      "3.583407163619995\n",
      "3.459237813949585\n",
      "3.3495893478393555\n",
      "2.9875566959381104\n",
      "3.061957359313965\n",
      "3.628544807434082\n",
      "3.578185796737671\n",
      "2.8210830688476562\n",
      "3.5366315841674805\n",
      "3.0153393745422363\n",
      "3.3293111324310303\n",
      "3.208106756210327\n",
      "4.193140983581543\n",
      "2.950446128845215\n",
      "3.1638922691345215\n",
      "2.778949022293091\n",
      "4.017586708068848\n",
      "3.415679693222046\n",
      "3.57257342338562\n",
      "3.469961643218994\n",
      "3.467808485031128\n",
      "4.513515472412109\n",
      "3.7375564575195312\n",
      "3.411919355392456\n",
      "2.701988935470581\n",
      "2.876936435699463\n",
      "3.588454484939575\n",
      "3.157029628753662\n",
      "3.5273537635803223\n",
      "2.7085981369018555\n",
      "4.201243877410889\n",
      "2.6568281650543213\n",
      "3.196777105331421\n",
      "4.142100811004639\n",
      "4.7636189460754395\n",
      "3.87530517578125\n",
      "2.8718390464782715\n",
      "3.031101703643799\n",
      "2.7744193077087402\n",
      "3.0721018314361572\n",
      "3.6437556743621826\n",
      "3.165071964263916\n",
      "3.2877840995788574\n",
      "2.9200592041015625\n",
      "2.680354356765747\n",
      "4.196836948394775\n",
      "2.8694310188293457\n",
      "2.6052892208099365\n",
      "3.5560905933380127\n",
      "2.92073392868042\n",
      "3.4097776412963867\n",
      "3.4030601978302\n",
      "3.360877752304077\n",
      "4.608551025390625\n",
      "3.690600872039795\n",
      "2.9494800567626953\n",
      "3.725236177444458\n",
      "4.201926231384277\n",
      "3.772636890411377\n",
      "2.9625754356384277\n",
      "4.342464923858643\n",
      "3.091214656829834\n",
      "2.9819157123565674\n",
      "3.3456356525421143\n",
      "4.213131427764893\n",
      "3.304666519165039\n",
      "3.132331132888794\n",
      "3.1740028858184814\n",
      "3.819368362426758\n",
      "3.8538904190063477\n",
      "2.735921859741211\n",
      "3.1839370727539062\n",
      "3.5965447425842285\n",
      "3.217500686645508\n",
      "3.406235694885254\n",
      "3.514366626739502\n",
      "3.3767733573913574\n",
      "2.7764029502868652\n",
      "2.9387853145599365\n",
      "4.205175876617432\n",
      "3.9149770736694336\n",
      "4.163467884063721\n",
      "3.8095011711120605\n",
      "3.34184193611145\n",
      "3.566143751144409\n",
      "2.778663158416748\n",
      "3.7569141387939453\n",
      "3.088961362838745\n",
      "4.041112899780273\n",
      "2.973783254623413\n",
      "3.6291916370391846\n",
      "3.4494354724884033\n",
      "3.3453826904296875\n",
      "4.2627997398376465\n",
      "3.845794439315796\n",
      "3.212587833404541\n",
      "3.034727096557617\n",
      "3.747042179107666\n",
      "2.6756937503814697\n",
      "3.6186201572418213\n",
      "2.690455436706543\n",
      "3.01910138130188\n",
      "3.5759806632995605\n",
      "3.463071584701538\n",
      "2.656209945678711\n",
      "2.6898696422576904\n",
      "3.4135124683380127\n",
      "2.9299068450927734\n",
      "2.8394088745117188\n",
      "3.066514015197754\n",
      "2.9828040599823\n",
      "2.693804979324341\n",
      "3.132599353790283\n",
      "2.875821590423584\n",
      "3.5299232006073\n",
      "4.05129861831665\n",
      "2.908344030380249\n",
      "3.2017147541046143\n",
      "2.506523847579956\n",
      "3.8639872074127197\n",
      "3.6118459701538086\n",
      "3.675002098083496\n",
      "2.6756255626678467\n",
      "4.337649345397949\n",
      "2.9887356758117676\n",
      "3.1197216510772705\n",
      "2.99655818939209\n",
      "3.1368789672851562\n",
      "2.5525615215301514\n",
      "2.9850757122039795\n",
      "3.0377652645111084\n",
      "3.1673896312713623\n",
      "2.1771109104156494\n",
      "3.0867390632629395\n",
      "2.777134895324707\n",
      "3.6618289947509766\n",
      "3.132748603820801\n",
      "3.3922152519226074\n",
      "3.321915864944458\n",
      "4.081713676452637\n",
      "3.860034465789795\n",
      "3.4884822368621826\n",
      "2.96169376373291\n",
      "3.0541584491729736\n",
      "4.138567924499512\n",
      "2.94748854637146\n",
      "3.1088204383850098\n",
      "2.9025979042053223\n",
      "2.7367491722106934\n",
      "3.2005972862243652\n",
      "3.4722495079040527\n",
      "2.766526937484741\n",
      "2.696420192718506\n",
      "3.7211802005767822\n",
      "3.4352638721466064\n",
      "3.5034427642822266\n",
      "2.829871416091919\n",
      "3.0861058235168457\n",
      "2.7843477725982666\n",
      "3.2643191814422607\n",
      "2.911116600036621\n",
      "3.368001937866211\n",
      "3.151808261871338\n",
      "4.374082088470459\n",
      "3.1544039249420166\n",
      "3.2551779747009277\n",
      "3.103222370147705\n",
      "3.010754108428955\n",
      "2.691262722015381\n",
      "3.371856689453125\n",
      "4.120704650878906\n",
      "3.774728775024414\n",
      "3.2861273288726807\n",
      "2.9645326137542725\n",
      "2.828592300415039\n",
      "2.337601661682129\n",
      "3.096266269683838\n",
      "2.9369988441467285\n",
      "3.1302595138549805\n",
      "2.8437106609344482\n",
      "3.108254909515381\n",
      "2.838009834289551\n",
      "3.393028497695923\n",
      "3.3572542667388916\n",
      "2.9842445850372314\n",
      "3.994352340698242\n",
      "3.0536937713623047\n",
      "3.018378734588623\n",
      "3.1457653045654297\n",
      "3.1206212043762207\n",
      "2.981691837310791\n",
      "3.728271245956421\n",
      "3.5326836109161377\n",
      "2.894002676010132\n",
      "3.4684901237487793\n",
      "2.860306739807129\n",
      "3.1902129650115967\n",
      "3.292083263397217\n",
      "3.203188419342041\n",
      "3.716935396194458\n",
      "3.615701675415039\n",
      "2.6470630168914795\n",
      "3.631279945373535\n",
      "2.2438549995422363\n",
      "3.642435073852539\n",
      "3.2309131622314453\n",
      "3.486057996749878\n",
      "3.6094768047332764\n",
      "3.0025599002838135\n",
      "2.84965181350708\n",
      "3.575157880783081\n",
      "3.072809934616089\n",
      "2.5972023010253906\n",
      "2.767733573913574\n",
      "2.8612899780273438\n",
      "3.3787739276885986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6862330436706543\n",
      "3.1476902961730957\n",
      "3.467764377593994\n",
      "2.9137260913848877\n",
      "3.0745041370391846\n",
      "3.258908987045288\n",
      "3.7118468284606934\n",
      "3.9353678226470947\n",
      "3.4210104942321777\n",
      "3.7308743000030518\n",
      "3.4356606006622314\n",
      "3.2950263023376465\n",
      "2.4697277545928955\n",
      "2.902852773666382\n",
      "2.4023189544677734\n",
      "3.664562702178955\n",
      "3.5348854064941406\n",
      "2.952111005783081\n",
      "3.539364814758301\n",
      "3.1439855098724365\n",
      "2.478858232498169\n",
      "3.0458147525787354\n",
      "4.138321876525879\n",
      "2.7830443382263184\n",
      "3.480351209640503\n",
      "3.5566201210021973\n",
      "3.4810802936553955\n",
      "3.128493070602417\n",
      "3.3950178623199463\n",
      "3.1206002235412598\n",
      "2.999472141265869\n",
      "3.3236405849456787\n",
      "2.953434944152832\n",
      "3.1657564640045166\n",
      "2.9489996433258057\n",
      "3.100461959838867\n",
      "2.848396062850952\n",
      "2.4284260272979736\n",
      "2.7444775104522705\n",
      "3.1541550159454346\n",
      "3.2204439640045166\n",
      "3.1239688396453857\n",
      "2.7601072788238525\n",
      "2.97660231590271\n",
      "3.4574174880981445\n",
      "2.5511624813079834\n",
      "2.947700262069702\n",
      "3.648160696029663\n",
      "4.087888240814209\n",
      "3.0729048252105713\n",
      "4.278942584991455\n",
      "3.0430455207824707\n",
      "3.39845609664917\n",
      "3.587528944015503\n",
      "3.0314416885375977\n",
      "2.8381481170654297\n",
      "3.1228573322296143\n",
      "3.258627414703369\n",
      "2.6890010833740234\n",
      "3.0792665481567383\n",
      "2.942646026611328\n",
      "3.2998592853546143\n",
      "3.0220730304718018\n",
      "2.8553671836853027\n",
      "3.521942615509033\n",
      "3.2514636516571045\n",
      "2.755074977874756\n",
      "3.232072353363037\n",
      "2.9230496883392334\n",
      "3.0029304027557373\n",
      "3.214815139770508\n",
      "3.4948368072509766\n",
      "2.811886787414551\n",
      "2.271406412124634\n",
      "2.6497998237609863\n",
      "3.0545809268951416\n",
      "2.7887587547302246\n",
      "2.9966917037963867\n",
      "2.545858144760132\n",
      "2.788179397583008\n",
      "4.076517105102539\n",
      "2.7631030082702637\n",
      "2.537868022918701\n",
      "3.499293565750122\n",
      "2.5744524002075195\n",
      "3.06648850440979\n",
      "3.3315935134887695\n",
      "3.148681402206421\n",
      "2.561903715133667\n",
      "3.330695390701294\n",
      "3.1633150577545166\n",
      "2.75464129447937\n",
      "3.3765878677368164\n",
      "2.5350639820098877\n",
      "3.237638235092163\n",
      "2.7480666637420654\n",
      "2.8528378009796143\n",
      "2.5900111198425293\n",
      "3.067797899246216\n",
      "3.2788875102996826\n",
      "2.9371321201324463\n",
      "3.5860583782196045\n",
      "2.815465211868286\n",
      "3.542027235031128\n",
      "3.057513952255249\n",
      "3.2192881107330322\n",
      "2.651978015899658\n",
      "2.882429599761963\n",
      "3.476339340209961\n",
      "2.8600940704345703\n",
      "3.0353705883026123\n",
      "3.030515432357788\n",
      "2.578779458999634\n",
      "3.5070996284484863\n",
      "3.6129894256591797\n",
      "2.8386237621307373\n",
      "2.7221572399139404\n",
      "2.9778892993927\n",
      "2.836315631866455\n",
      "3.0959060192108154\n",
      "3.1782608032226562\n",
      "2.6247406005859375\n",
      "3.1870310306549072\n",
      "3.537740707397461\n",
      "3.276188850402832\n",
      "2.66025710105896\n",
      "3.231649160385132\n",
      "3.1523807048797607\n",
      "2.6188948154449463\n",
      "3.440734386444092\n",
      "3.2956182956695557\n",
      "2.6953980922698975\n",
      "3.3205301761627197\n",
      "3.268354654312134\n",
      "3.1245243549346924\n",
      "2.6992883682250977\n",
      "3.5931620597839355\n",
      "3.13010311126709\n",
      "3.4412147998809814\n",
      "3.2540206909179688\n",
      "2.4136009216308594\n",
      "3.017996311187744\n",
      "3.1192948818206787\n",
      "2.864691734313965\n",
      "2.7025558948516846\n",
      "3.25919508934021\n",
      "3.192664623260498\n",
      "2.985300064086914\n",
      "3.018310785293579\n",
      "2.781730890274048\n",
      "3.0353639125823975\n",
      "2.879373073577881\n",
      "2.5195860862731934\n",
      "2.6739323139190674\n",
      "3.1046979427337646\n",
      "2.8118298053741455\n",
      "2.8674261569976807\n",
      "3.1398792266845703\n",
      "3.388232707977295\n",
      "2.492863178253174\n",
      "2.935746192932129\n",
      "3.2403671741485596\n",
      "2.6386773586273193\n",
      "3.1133036613464355\n",
      "2.5442209243774414\n",
      "2.6445822715759277\n",
      "2.759019613265991\n",
      "2.906320810317993\n",
      "3.1742944717407227\n",
      "2.9591481685638428\n",
      "2.8989741802215576\n",
      "3.1716248989105225\n",
      "2.5008835792541504\n",
      "2.8948159217834473\n",
      "2.6957855224609375\n",
      "3.6848928928375244\n",
      "2.7891039848327637\n",
      "2.9779365062713623\n",
      "2.8844923973083496\n",
      "2.8880181312561035\n",
      "2.9055702686309814\n",
      "3.1096534729003906\n",
      "3.0012834072113037\n",
      "2.4646785259246826\n",
      "2.938086748123169\n",
      "3.1548664569854736\n",
      "3.021127462387085\n",
      "3.291025400161743\n",
      "3.002554416656494\n",
      "3.1969692707061768\n",
      "3.070080518722534\n",
      "2.7920749187469482\n",
      "2.831639528274536\n",
      "3.2360711097717285\n",
      "3.112088680267334\n",
      "2.761291980743408\n",
      "2.871135711669922\n",
      "2.426948308944702\n",
      "3.3436954021453857\n",
      "2.8108105659484863\n",
      "2.601328134536743\n",
      "2.9315874576568604\n",
      "3.1416163444519043\n",
      "3.2299699783325195\n",
      "2.9749512672424316\n",
      "2.893972635269165\n",
      "2.564284563064575\n",
      "3.0425684452056885\n",
      "3.173945188522339\n",
      "3.378716230392456\n",
      "3.148866891860962\n",
      "2.867586374282837\n",
      "3.1052486896514893\n",
      "3.0127902030944824\n",
      "2.809022903442383\n",
      "3.0804355144500732\n",
      "3.026031017303467\n",
      "3.477308511734009\n",
      "2.82867169380188\n",
      "4.152530193328857\n",
      "2.7778470516204834\n",
      "3.026573419570923\n",
      "2.7081313133239746\n",
      "2.8841025829315186\n",
      "3.082209587097168\n",
      "2.7125632762908936\n",
      "3.3638641834259033\n",
      "3.1374731063842773\n",
      "3.278082847595215\n",
      "3.102616548538208\n",
      "3.006938934326172\n",
      "3.0118775367736816\n",
      "3.0512163639068604\n",
      "2.9460625648498535\n",
      "3.718672752380371\n",
      "3.1623384952545166\n",
      "2.760782480239868\n",
      "2.7452499866485596\n",
      "3.0107760429382324\n",
      "3.1063573360443115\n",
      "2.674884796142578\n",
      "3.674734592437744\n",
      "2.835036277770996\n",
      "3.4153523445129395\n",
      "3.150102138519287\n",
      "3.2274675369262695\n",
      "2.633638381958008\n",
      "2.7572221755981445\n",
      "3.196795701980591\n",
      "2.896327018737793\n",
      "3.0801565647125244\n",
      "3.624020576477051\n",
      "3.388404369354248\n",
      "2.929717540740967\n",
      "3.1958723068237305\n",
      "2.8261802196502686\n",
      "3.2400715351104736\n",
      "2.6462786197662354\n",
      "2.569885015487671\n",
      "3.71557879447937\n",
      "3.375957489013672\n",
      "3.380641222000122\n",
      "3.5764975547790527\n",
      "3.076031446456909\n",
      "2.773210287094116\n",
      "3.7057747840881348\n",
      "3.455939292907715\n",
      "3.1715526580810547\n",
      "2.9724206924438477\n",
      "2.960047960281372\n",
      "3.5182573795318604\n",
      "2.581498622894287\n",
      "3.1089510917663574\n",
      "3.1368494033813477\n",
      "3.0699734687805176\n",
      "3.2849953174591064\n",
      "2.8711588382720947\n",
      "2.735945701599121\n",
      "3.9700450897216797\n",
      "3.2993266582489014\n",
      "3.4952569007873535\n",
      "3.1236398220062256\n",
      "3.0965240001678467\n",
      "3.778931140899658\n",
      "3.2730844020843506\n",
      "4.1243462562561035\n",
      "3.279606580734253\n",
      "3.08197283744812\n",
      "3.8606224060058594\n",
      "3.13041090965271\n",
      "3.0569040775299072\n",
      "2.781985282897949\n",
      "2.987622022628784\n",
      "2.7840702533721924\n",
      "2.8842601776123047\n",
      "2.9502265453338623\n",
      "3.469529151916504\n",
      "4.32503604888916\n",
      "3.8412513732910156\n",
      "3.718146324157715\n",
      "3.562704086303711\n",
      "3.309058904647827\n",
      "3.4767539501190186\n",
      "3.0004913806915283\n",
      "3.5426573753356934\n",
      "3.288201332092285\n",
      "2.8929407596588135\n",
      "3.590160369873047\n",
      "3.8150904178619385\n",
      "2.9073357582092285\n",
      "2.8484675884246826\n",
      "3.5639424324035645\n",
      "3.125706434249878\n",
      "4.618301868438721\n",
      "4.841714859008789\n",
      "4.505471706390381\n",
      "3.178619623184204\n",
      "4.290528297424316\n",
      "3.4065747261047363\n",
      "3.5290374755859375\n",
      "3.9683821201324463\n",
      "2.6798157691955566\n",
      "3.33420991897583\n",
      "3.0205509662628174\n",
      "2.623187780380249\n",
      "3.0040838718414307\n",
      "4.213490009307861\n",
      "3.6795294284820557\n",
      "3.973050594329834\n",
      "3.771912097930908\n",
      "3.5578906536102295\n",
      "3.5432870388031006\n",
      "3.4348721504211426\n",
      "3.5791079998016357\n",
      "3.262646436691284\n",
      "3.614107131958008\n",
      "3.6043126583099365\n",
      "3.9279885292053223\n",
      "4.054028034210205\n",
      "3.084162950515747\n",
      "3.1400504112243652\n",
      "4.051314830780029\n",
      "3.6988723278045654\n",
      "5.331782817840576\n",
      "4.630373477935791\n",
      "3.603404998779297\n",
      "3.218550682067871\n",
      "3.327008008956909\n",
      "3.913133382797241\n",
      "4.077112197875977\n",
      "2.9961698055267334\n",
      "3.4328970909118652\n",
      "3.9585866928100586\n",
      "3.6462645530700684\n",
      "3.4368784427642822\n",
      "3.6798486709594727\n",
      "4.408226013183594\n",
      "4.570468902587891\n",
      "4.5464863777160645\n",
      "3.3069710731506348\n",
      "4.573141574859619\n",
      "3.718374252319336\n",
      "4.088209629058838\n",
      "3.5434682369232178\n",
      "3.9202182292938232\n",
      "4.487661838531494\n",
      "5.053940296173096\n",
      "4.166270732879639\n",
      "4.342210292816162\n",
      "4.587041854858398\n",
      "4.466854572296143\n",
      "4.469021797180176\n",
      "4.216336250305176\n",
      "3.3703877925872803\n",
      "2.8465638160705566\n",
      "3.8648757934570312\n",
      "3.7956604957580566\n",
      "3.779909133911133\n",
      "3.8715193271636963\n",
      "4.003833770751953\n",
      "5.807869911193848\n",
      "3.8937454223632812\n",
      "4.50300407409668\n",
      "4.257734775543213\n",
      "3.863804340362549\n",
      "4.586611270904541\n",
      "3.8009254932403564\n",
      "3.3838319778442383\n",
      "4.2310285568237305\n",
      "7.998926162719727\n",
      "5.8168487548828125\n",
      "6.660499095916748\n",
      "4.778482437133789\n",
      "5.485812187194824\n",
      "3.6799540519714355\n",
      "4.210209369659424\n",
      "3.7837324142456055\n",
      "4.632872104644775\n",
      "3.09767746925354\n",
      "4.001021385192871\n",
      "3.5437867641448975\n",
      "4.897819995880127\n",
      "3.3342535495758057\n",
      "5.143523216247559\n",
      "3.612933874130249\n",
      "4.099184989929199\n",
      "5.382430553436279\n",
      "4.862123012542725\n",
      "5.364605903625488\n",
      "4.754778861999512\n",
      "6.374260902404785\n",
      "4.042466640472412\n",
      "5.034127235412598\n",
      "5.099289417266846\n",
      "4.157521724700928\n",
      "5.20457124710083\n",
      "7.330483913421631\n",
      "5.304389476776123\n",
      "6.008729934692383\n",
      "5.794632434844971\n",
      "5.887872219085693\n",
      "4.667130470275879\n",
      "5.373446941375732\n",
      "4.175260543823242\n",
      "5.770834445953369\n",
      "6.600052356719971\n",
      "5.608867168426514\n",
      "5.594350337982178\n",
      "4.817663669586182\n",
      "4.28800106048584\n",
      "5.7508864402771\n",
      "5.753495216369629\n",
      "4.6181745529174805\n",
      "4.010953903198242\n",
      "4.940386772155762\n",
      "4.724496364593506\n",
      "5.012787342071533\n",
      "4.275187015533447\n",
      "4.944446563720703\n",
      "6.2093071937561035\n",
      "6.743629455566406\n",
      "6.692795753479004\n",
      "5.446030616760254\n",
      "4.590074062347412\n",
      "5.838624954223633\n",
      "7.07719087600708\n",
      "7.441557884216309\n",
      "5.887662887573242\n",
      "5.538500785827637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.746973991394043\n",
      "6.858906269073486\n",
      "4.262596607208252\n",
      "4.995443344116211\n",
      "5.181946277618408\n",
      "5.490933418273926\n",
      "6.718283653259277\n",
      "8.590028762817383\n",
      "5.925363063812256\n",
      "6.416497707366943\n",
      "8.451143264770508\n",
      "7.039004325866699\n",
      "6.884308338165283\n",
      "6.345677852630615\n",
      "7.307611465454102\n",
      "7.146516799926758\n",
      "7.942399978637695\n",
      "6.118298530578613\n",
      "5.915438652038574\n",
      "8.392385482788086\n",
      "6.630154609680176\n",
      "7.028544902801514\n",
      "5.942543983459473\n",
      "8.684577941894531\n",
      "7.755612373352051\n",
      "11.531898498535156\n",
      "6.585060119628906\n",
      "7.37111759185791\n",
      "7.8743767738342285\n",
      "6.330507278442383\n",
      "6.952282905578613\n",
      "5.363361358642578\n",
      "7.424020290374756\n",
      "7.544375896453857\n",
      "6.742396831512451\n",
      "6.994511604309082\n",
      "5.893688678741455\n",
      "6.284351825714111\n",
      "8.373236656188965\n",
      "8.63415241241455\n",
      "7.863779067993164\n",
      "4.596696853637695\n",
      "6.714995861053467\n",
      "6.567584037780762\n",
      "6.5582780838012695\n",
      "7.22022819519043\n",
      "6.475634574890137\n",
      "8.71664047241211\n",
      "6.984683990478516\n",
      "8.30903148651123\n",
      "8.494425773620605\n"
     ]
    }
   ],
   "source": [
    "lri = []\n",
    "lossi = []\n",
    "for i in range(1000):\n",
    "    #mini-batch\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # we always take random 32 3 grams\n",
    "    #forward pass\n",
    "    emb = C[X[ix]] # 32, 3, 2 #only for those 32 trigrams we use embeding \n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) #32, 100\n",
    "    logits = h @ W2 + b2 # 32, 27\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    print(loss.item())\n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    lr = lrs[i] \n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    #tracking stats\n",
    "    lri.append(lr)\n",
    "    lossi.append(loss.item())\n",
    "#print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "249d98c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(lri, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0bb2f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from plot we can identify the best learning rate where loss is minimal\n",
    "#turns out 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0b9699b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0756852626800537\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    #mini-batch\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # we always take random 32 3 grams\n",
    "    #forward pass\n",
    "    emb = C[X[ix]] # 32, 3, 2 #only for those 32 trigrams we use embeding \n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) #32, 100\n",
    "    logits = h @ W2 + b2 # 32, 27\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    #print(loss.item())\n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    lr = 0.01 \n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    #tracking stats\n",
    "    #lri.append(lr)\n",
    "    #lossi.append(loss.item())\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "070c235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#much lower loss when compared with makeover1 basic neural net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "84d60c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training with entire model will result in overfiting, model will remember everything instead generalizing\n",
    "#hence we go for train, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "05fb43d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training split, dev/validation split, test split\n",
    "#80%, 10%, 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "38f2c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3 #defines no of characters do we take to predict the next one? i.e. context length \n",
    "X, Y = [], [] # X are inputs to neural net & y is output\n",
    "\n",
    "for w in words: #take 1st 5 words and do it, later change to all words in vocab\n",
    "    #print(w)\n",
    "    context = [0] * block_size #fist list is [...] \n",
    "    for ch in w + '.': #append . at the end of each char to identify end of word\n",
    "        ix = stoi[ch] #current character\n",
    "        X.append(context) #current 3 char bigram\n",
    "        Y.append(ix) #append corresponding output\n",
    "        #print(''.join(itos[i] for i in context), '------>', itos[ix])\n",
    "        context = context[1:] + [ix] # remove 1st and add new char at end to get next set of 3 gram\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4545d1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a5bf7053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words):\n",
    "    block_size = 3\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        #print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words) #random shuffling\n",
    "n1 = int(0.8*len(words)) \n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1]) #80% words in to train dataset\n",
    "Xdev, Ydev = build_dataset(words[n1:n2]) #10% into validation\n",
    "Xte, Yte = build_dataset(words[n2:]) #rest 10% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "57f0903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train from train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "018e9fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7d200fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a994fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bdc56813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.513019323348999\n"
     ]
    }
   ],
   "source": [
    "for i in range(2000):\n",
    "    #mini-batch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32,)) # we always take random 32 3 grams\n",
    "    #forward pass\n",
    "    emb = C[Xtr[ix]] # 32, 3, 2 #only for those 32 trigrams we use embeding \n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) #32, 100\n",
    "    logits = h @ W2 + b2 # 32, 27\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    #print(loss.item())\n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    lr = 0.1 \n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    #tracking stats\n",
    "    #lri.append(lr)\n",
    "    #lossi.append(loss.item())\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bafd9df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6139, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trained only from train dataset\n",
    "#lets evaluate using dev dataset\n",
    "emb = C[Xdev] # 32, 3, 2\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) #32, 100\n",
    "logits = h @ W2 + b2 # 32, 27\n",
    "#counts = logits.exp()\n",
    "#prob = counts / counts.sum(1, keepdims = True)\n",
    "#loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d256c54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6241, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xtr] # 32, 3, 2\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) #32, 100\n",
    "logits = h @ W2 + b2 # 32, 27\n",
    "#counts = logits.exp()\n",
    "#prob = counts / counts.sum(1, keepdims = True)\n",
    "#loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "54373572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#both train & dev loss is almost same, so no overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "82cf47ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets scale the neural net with multiple hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "92d18258",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 300), generator=g) #increased from 100 to 300 hidden neurons\n",
    "b1 = torch.randn(300, generator=g)\n",
    "W2 = torch.randn((300, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dda4b8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10281"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6f970e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d989aa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2968626022338867\n"
     ]
    }
   ],
   "source": [
    "lri = []\n",
    "lossi = []\n",
    "stepi = []\n",
    "for i in range(30000):\n",
    "    #mini-batch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32,)) # we always take random 32 3 grams\n",
    "    #forward pass\n",
    "    emb = C[Xtr[ix]] # 32, 3, 2 #only for those 32 trigrams we use embeding \n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) #32, 100\n",
    "    logits = h @ W2 + b2 # 32, 27\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    #print(loss.item())\n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    lr = 0.05 #little higher learing rate \n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    #tracking stats\n",
    "    #lri.append(lr)\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.item())\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f21db213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d0c024b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4255, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev] # 32, 3, 2\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) #32, 100\n",
    "logits = h @ W2 + b2 # 32, 27\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "749f019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try increasing batch size ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a3e90101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#or\n",
    "#right now embedding feature vector is just 2 dimension\n",
    "# may be increasing the featur dim will have much lower loss ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "153cbdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting embeding layer\n",
    "#plt.figure(figsize=(8,8))\n",
    "#plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "#for i in range(C.shape[0]):\n",
    "#    plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha=\"center\", va=\"center\", color=\"white\")\n",
    "#plt.grid('minor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e224057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets change embedding layer size and check if reduction in loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "494e5f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 10), generator=g) #10 embedding layer\n",
    "W1 = torch.randn((30, 200), generator=g) #3 * 10 = 30 parameters & hidden neurons = 200\n",
    "b1 = torch.randn(200, generator=g)\n",
    "W2 = torch.randn((200, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cd3c9010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11897"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cf72e8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c36fb556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.141078472137451\n"
     ]
    }
   ],
   "source": [
    "lri = []\n",
    "lossi = []\n",
    "stepi = []\n",
    "\n",
    "for i in range(30000):\n",
    "    #mini-batch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32,)) # we always take random 32 3 grams\n",
    "    #forward pass\n",
    "    emb = C[Xtr[ix]] # 32, 3, 2 #only for those 32 trigrams we use embeding \n",
    "    h = torch.tanh(emb.view(-1, 30) @ W1 + b1) #32, 100\n",
    "    logits = h @ W2 + b2 # 32, 27\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    #print(loss.item())\n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    #tracking stats\n",
    "    #lri.append(lr)\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.item())\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "89718ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2049, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xtr] # 32, 3, 2\n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) #32, 100\n",
    "logits = h @ W2 + b2 # 32, 27\n",
    "#counts = logits.exp()\n",
    "#prob = counts / counts.sum(1, keepdims = True)\n",
    "#loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0fd285de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2236, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev] # 32, 3, 2\n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) #32, 100\n",
    "logits = h @ W2 + b2 # 32, 27\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4e7ff635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try your best to tweak the hyper parameters and get much lower loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "30c5b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating samples from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8a181b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mria.\n",
      "kayanniee.\n",
      "med.\n",
      "rylla.\n",
      "emmrettagrael.\n",
      "aderedieliigh.\n",
      "parelle.\n",
      "elisan.\n",
      "aar.\n",
      "katelor.\n",
      "kamin.\n",
      "shebergiairiel.\n",
      "kende.\n",
      "jelionnton.\n",
      "fous.\n",
      "kacder.\n",
      "yarleyel.\n",
      "ylia.\n",
      "maston.\n",
      "mahil.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647 +10)\n",
    "\n",
    "for _ in range(20): #generate 20 random words from model\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with 0 \n",
    "    while True: #till we reach ., continue the loop...\n",
    "        emb = C[torch.tensor([context])] #embed the current context using current emb vector (1, block_size, d)\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 +b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ccceb0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thats we generated words using Multi Layer Perceptron implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ed206ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Though output is not so accurate but much better than previous neural net outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c6ddabf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lot of scope to improve using RNN... Lets see in next session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf00306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
